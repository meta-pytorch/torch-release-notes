
# Release Notes worksheet skip

The main goal of this process is to rephrase all the commit messages below to make them **clear and easy to read** by the end user. You should follow the following instructions to do so:

* **Please clean up and format commit titles to be readable by the general PyTorch user.** Make sure you're [following the guidance here](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit)! Your resulting notes must be consistent and easy to read.
* Please sort commits into the following categories (you should not rename the categories!), I tried to pre-sort these to ease your work, feel free to move commits around if the current categorization is not good.
* Anything that is not public facing needs to be removed.
* If anything is miscategorized/belongs to another domain, move it to `miscategorized.md`.
* Please scan through `miscategorized.md` and handle any commits that belong within your domain according to these instructions.
* We place a lot of emphasis on the “BC-breaking” and “deprecation” sections. Those should be where the most effort goes in. The “improvements” and “bug fixes” for Python API should be nice as well.
* Once you are finished, move this very file from `todo/` to `done/` and submit a pull request.

The categories below are as follows:

* BC breaking: All commits that are BC-breaking. These are the most important commits. If any pre-sorted commit is actually BC-breaking, do move it to this section. Each commit should contain a paragraph explaining the rational behind the change as well as an example for how to update user code [BC-Guidelines](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit#heading=h.a9htwgvvec1m).
* Deprecations: All commits introducing deprecation. Each commit should include a small example explaining what should be done to update user code.
* new_features: All commits introducing a new feature (new functions, new submodule, new supported platform etc)
* improvements: All commits providing improvements to existing feature should be here (new backend for a function, new argument, better numerical stability)
* bug fixes: All commits that fix bugs and behaviors that do not match the documentation
* performance: All commits that are added mainly for performance (we separate this from improvements above to make it easier for users to look for it)
* documentation: All commits that add/update documentation
* Developers: All commits that are not end-user facing but still impact people that compile from source, develop into pytorch, extend pytorch, etc
* not user facing: All commits that are not public end-user facing and hence should be dropped from the release notes

## skip
### bc breaking
### deprecation
### new features
### improvements
### bug fixes
### performance
### docs
### devs
### Untopiced
- Revert "Reland: [inductor] Simplify grid handling (5ada4e6a535)
- [ROCm] Input vectorization in elementwise kernels for tensors with heterogeneous types ([#147527](https://github.com/pytorch/pytorch/pull/147527))
- Split up cub-RadixSortPairs.cu to parallelize compilation ([#148936](https://github.com/pytorch/pytorch/pull/148936))
- Revert "Split up cub-RadixSortPairs.cu to parallelize compilation (49570cb4024)
- Revert "[ROCm] Input vectorization in elementwise kernels for tensors with heterogeneous types (e6839819c81)
- Revert "[MPS] Add support for `i0e` in eager. (be4e6c1c8ee)
- Revert "[AOTInductor] [BE] Add swap_constant_buffer into pybind for tests. (71795f159e9)
- [ROCm] enable HIPMallocAsyncAllocator ([#149145](https://github.com/pytorch/pytorch/pull/149145))
- Fix `torch.nn.functional.hardswish` gradients corner case ([#148049](https://github.com/pytorch/pytorch/pull/148049))
- Revert "[RFC] First version of statically compiled launcher for triton compiled CUDA kernels (643aaea1332)
- Revert "[pytree] add APIs to determine a class is a namedtuple or PyStructSequence (f9b4856989c)
- [BE]: Apply ruff PERF403 to use dict comprehensions more often ([#149257](https://github.com/pytorch/pytorch/pull/149257))
- Revert "[ROCm] enable HIPMallocAsyncAllocator (9d37b501db3)
- [ROCm] Input vectorization in elementwise kernels for tensors with heterogeneous types ([#147527](https://github.com/pytorch/pytorch/pull/147527))
- Revert "[BE]: Apply ruff PERF403 to use dict comprehensions more often (24cfeec2c70)
- Revert "[dynamo][guards][serialization] Dont use ID_MATCH guard for bool and None (b52a8bef013)
- [BE]: Apply ruff PERF403 to use dict comprehensions more often ([#149257](https://github.com/pytorch/pytorch/pull/149257))
- Revert "[AOTI] Forward fix unit test failures (5ba437fb452)
- Revert "[AOTI] Update test runner to use the new APIs (405025778de)
- [ROCm] enable HIPMallocAsyncAllocator ([#149145](https://github.com/pytorch/pytorch/pull/149145))
- Revert "[ROCm] enable HIPMallocAsyncAllocator (e1d143cb7b3)
- [ROCm] enable HIPMallocAsyncAllocator ([#149145](https://github.com/pytorch/pytorch/pull/149145))
- ci: Remove mentions and usages of DESIRED_DEVTOOLSET ([#149443](https://github.com/pytorch/pytorch/pull/149443))
- Revert "ci: Remove mentions and usages of DESIRED_DEVTOOLSET (826e7906960)
- Supporting non-tensor-data write_size in planner write items. ([#149434](https://github.com/pytorch/pytorch/pull/149434))
- Revert "Supporting non-tensor-data write_size in planner write items. (90ef7a95618)
- ci: Remove mentions and usages of DESIRED_DEVTOOLSET and cxx11 ([#149443](https://github.com/pytorch/pytorch/pull/149443))
- Revert "[cond] don't trace fw and bw graph in autograd key (24176f6e32d)
- Extend vec backend with BF16 SVE intrinsics ([#143666](https://github.com/pytorch/pytorch/pull/143666))
- Revert "[BE][Ez]: Update CU126 to CUDNN 12.8 too (b238e36fd99)
- Revert "ci: Add sccache to manylinux images (453da423d44)
- [export] Save unflattened gm ([#149717](https://github.com/pytorch/pytorch/pull/149717))
- Revert "[export] Save unflattened gm (42e7bda53eb)
- Revert "Extend vec backend with BF16 SVE intrinsics (bada898f5ee)
- Revert "[BE] Replace XPU support packages installation to offline mode in Linux CI/CD (7b218ca874c)
- Revert "[ONNX] Clean up the diagnostics module (30e8be599f3)
- [ROCm] build magma rocm and upload tarball ([#149902](https://github.com/pytorch/pytorch/pull/149902))
- Revert "[ROCm] build magma rocm and upload tarball (d3b7cf7b7d7)
- [ROCm] build magma rocm and upload tarball ([#149902](https://github.com/pytorch/pytorch/pull/149902))
- Revert "cpp_wrapper: precompile a few more commonly used headers, and improve RAIIPyObject interface (91bf92597c0)
- Revert "cpp_wrapper: Fix even more tests (1b373f6cd4b)
- Revert "[inductor] Fix mm logging for `torch._scaled_.mm` (ab9ca6b31f9)
- Revert "[custom_ops][perf] Move expensive pytree traversals of tensors to C++ (d256b2dcb21)
- Revert "Improve subproc autotuning implementation (185aaaaf8e1)
- Introduce guard_or_true, guard_or_false ([#148430](https://github.com/pytorch/pytorch/pull/148430))
- [cuda] Add new faster gammabeta backward kernel ([#148605](https://github.com/pytorch/pytorch/pull/148605))
- Use source hashing to generate consistent symbolic ids ([#149665](https://github.com/pytorch/pytorch/pull/149665))
- Revert "Introduce guard_or_true, guard_or_false (e080bac5336)
- Introduce guard_or_true, guard_or_false ([#148430](https://github.com/pytorch/pytorch/pull/148430))
- Revert "Use source hashing to generate consistent symbolic ids (af7719a2fa0)
- Revert "[triton] Warp specialization support in torchinductor (efc975feb26)
- Revert "Store statically launchable CachingAutotuners inside CompiledFXGraph.triton_bundle (80aa88f9070)
- Revert "[fbcode]Removing `@NoIntBaseDeprecated` annotation in `caffe2.thrift` file (#149742) (1a3bd894ff0)
- Support torchbind in OSS proxy executor ([#149747](https://github.com/pytorch/pytorch/pull/149747))
- Add one_shot_all_reduce_copy to allow non-symm-mem allocated tensors to be reduced ([#150129](https://github.com/pytorch/pytorch/pull/150129))
- Revert "Support torchbind in OSS proxy executor (ddb1e978393)
- Use source hashing to generate consistent symbolic ids ([#149665](https://github.com/pytorch/pytorch/pull/149665))
- Revert "Store statically launchable CachingAutotuners inside CompiledFXGraph.triton_bundle (7c4e49750e3)
- Revert "cpp_wrapper: precompile a few more commonly used headers, and improve RAIIPyObject interface (e691fcae0eb)
- Revert "cpp_wrapper: Fix even more tests (cf7447ae992)
- enable out variant of 2-shot reduction ([#150153](https://github.com/pytorch/pytorch/pull/150153))
- Revert "[CI] Fix docker builds failing due to cmake update by setting CMAKE_POLICY_VERSION_MINIMUM (7ac0658757d)
- Revert "Move MacOS inductor tests to M2-15 runner (ccfde4dadfa)
- Revert "[Profiler] Give non-zero default values to start events (3b00ff88509)
- Revert "Add one_shot_all_reduce_copy to allow non-symm-mem allocated tensors to be reduced (e57fa18b40e)
- Revert "enable out variant of 2-shot reduction (57fa99c5c38)
- Revert "Enable TMA persistent GEMM Template by default (7c858066aed)
- Revert "Add a warning when a tensor with requires_grad=True is converted to a scalar (1526ff955e6)
- enable out variant of 2-shot reduction ([#150153](https://github.com/pytorch/pytorch/pull/150153))
- Add one_shot_all_reduce_copy to allow non-symm-mem allocated tensors to be reduced ([#150129](https://github.com/pytorch/pytorch/pull/150129))
- Revert "Merge Triton ScaledMM as epilogue to MM template (f04cf13bddd)
- Revert "[ROCm] use correct workspace for hipblaslt, silence warning (76e1b3ba4c8)
- Revert "if blaslt fails, fall back to blas (9458460211a)
- Revert "[cuBLAS][cuBLASLt] Unify `cuBLASLt` workspaces with `cuBLAS` workspaces (203a27e0cec)
- Revert "[Profiler] Fix Empty C Call Queue (532530be34b)
- [dynamo] Support Tensor subclass that has dynamic attributes or calls `Parameter.__torch_function__` ([#149482](https://github.com/pytorch/pytorch/pull/149482))
- [dynamo] Support `torch.Tensor._make_subclass` and tracing through tensor subclass `__new__` ([#149483](https://github.com/pytorch/pytorch/pull/149483))
- [dynamo] Support tensor subclass with overriden tensor methods and properties ([#149484](https://github.com/pytorch/pytorch/pull/149484))
- [dynamo] Always trace into tensor subclass `__torch_function__` ([#149792](https://github.com/pytorch/pytorch/pull/149792))
- Revert "[dynamo] Always trace into tensor subclass `__torch_function__` (e5455673408)
- Revert "[dynamo] Support tensor subclass with overriden tensor methods and properties (01411c739f1)
- Revert "[dynamo] Support `torch.Tensor._make_subclass` and tracing through tensor subclass `__new__` (18908c8cedd)
- Revert "[dynamo] Support Tensor subclass that has dynamic attributes or calls `Parameter.__torch_function__` (03c879d59bf)
- [dynamo] Support Tensor subclass that has dynamic attributes or calls `Parameter.__torch_function__` ([#149482](https://github.com/pytorch/pytorch/pull/149482))
- [dynamo] Support `torch.Tensor._make_subclass` and tracing through tensor subclass `__new__` ([#149483](https://github.com/pytorch/pytorch/pull/149483))
- [dynamo] Support tensor subclass with overriden tensor methods and properties ([#149484](https://github.com/pytorch/pytorch/pull/149484))
- [dynamo] Always trace into tensor subclass `__torch_function__` ([#149792](https://github.com/pytorch/pytorch/pull/149792))
- Revert "[cuda] Add new faster gammabeta backward kernel (61a1f09b5b1)
- Enable C++ dynamic shape guards by default ([#140756](https://github.com/pytorch/pytorch/pull/140756))
- Revert "Enable C++ dynamic shape guards by default (5a654deb408)
- [cuda] Add new faster gammabeta backward kernel (#148605) (Reapply with launch bounds) ([#150625](https://github.com/pytorch/pytorch/pull/150625))
- Split up cub-RadixSortPairs.cu to parallelize compilation ([#148936](https://github.com/pytorch/pytorch/pull/148936))
- Revert "add unit test for preferred_blas_library settings (b0e28f60df6)
- Revert "Add torch._scaled_mm for CPU (4854926aeb5)
- Revert "[cuda] Add new faster gammabeta backward kernel (#148605) (Reapply with launch bounds) (f443035f10d)
- Revert "bound sympy accuracy (c93e34d7b56)
- Revert "Fix conv2d strided prologue (caf8d9bc174)
- Revert "Generalize poison fork logic for each device backend (bf1132c1967)
- Fixing NCCL abort hang issue when a ProcessGroupNCCL manages multiple ncclComms ([#150690](https://github.com/pytorch/pytorch/pull/150690))
- [cuda] Add new faster gammabeta backward kernel (#148605) (Reapply with launch bounds) ([#150625](https://github.com/pytorch/pytorch/pull/150625))
- Fix the Problems About Defining Static Variable in Inline Function ([#147095](https://github.com/pytorch/pytorch/pull/147095))
- Revert "Fix the Problems About Defining Static Variable in Inline Function (4926bd60040)
- Revert "Fixing NCCL abort hang issue when a ProcessGroupNCCL manages multiple ncclComms (d9f47c75ded)
- Inductor respects exact strides on custom ops by default ([#150511](https://github.com/pytorch/pytorch/pull/150511))
- Revert "Inductor respects exact strides on custom ops by default (a0e796df03b)
- Revert "Refactor layout constraint selection logic (01568cb17a5)
- ProcessGroupGloo: support lazy_init ([#150801](https://github.com/pytorch/pytorch/pull/150801))
- c10d/Store: add clone feature ([#150966](https://github.com/pytorch/pytorch/pull/150966))
- Revert "ProcessGroupGloo: support lazy_init (73f3d6d9aaa)
- Fixing NCCL abort hang issue when a ProcessGroupNCCL manages multiple ncclComms ([#150690](https://github.com/pytorch/pytorch/pull/150690))
- [profiler] don't disable CUPTI_LAZY_REINIT for cuda >= 12.6 ([#150957](https://github.com/pytorch/pytorch/pull/150957))
- [Intel GPU] Allow XPU backend in Depthwise_conv2d&3d operators ([#149114](https://github.com/pytorch/pytorch/pull/149114))
- Revert "[AOTI] Remove typedef for half and bfloat16 (31162214d8a)
- Revert "c10d/Store: add clone feature (abe41c5c9c0)
- Revert "Generalize poison fork logic for each device backend (a0ab243c3a5)
- Revert "Support tuning of _scaled_grouped_mm (6a65f2c4feb)
- Revert "[inductor] Change minimum number of SMs to 60 to let Ada use Triton GEMM backend (e786b3bf543)
- Revert "[profiler] don't disable CUPTI_LAZY_REINIT for cuda >= 12.6 (44ed0c9fbbe)
- Revert "update benchamark result due to <1% regression (67d3053d4b2)
- Revert "[MPSInductor] Naive welford_reduce implementation (83f14c0b067)
- Revert "[MPSInductor] Naive welford_reduce implementation (77407b38a99)
- Revert "Make export._trace._WrapperModule work in strict mode (2f899f07aac)
- Revert "[MPSInductor] Fix larger-than-threadgroup Welford reductions (7762bddd87f)
- Revert "[Inductor] Refactor wrapper codegen to use Wrapper IR. (8157e76b793)
- Add inductor standalone_compile API ([#150670](https://github.com/pytorch/pytorch/pull/150670))
- Revert "Add inductor standalone_compile API (24b3ab92550)
- Revert "[map] make proxy mode re-dispatch to fake key (6a77a0a50c1)
- Revert "[map] always turn on dynamo for map (4a47dd9b3f5)
- Add inductor standalone_compile API ([#150670](https://github.com/pytorch/pytorch/pull/150670))
- Revert "Add inductor standalone_compile API (74f6bc28a72)
- Revert "Fix setUpClass() / tearDownClass() for device-specific tests (98b1e82ba84)
- Add inductor standalone_compile API ([#150670](https://github.com/pytorch/pytorch/pull/150670))
- [dynamo] context manager/decorator for dynamo config patching during tracing ([#150586](https://github.com/pytorch/pytorch/pull/150586))
- Revert "[dynamo] context manager/decorator for dynamo config patching during tracing (6a3a6d22dce)
- Revert "[ez] Make relaxed constraint error message more user friendly (a582f046084)
- Revert "[Openreg][PrivateUse1] Fix releasing tensor issue when using pin_memory (e0535e823fe)
- Revert "[Openreg][PrivateUse1] Enable CI for openreg (f252f9df5e0)
- Revert "[DCP] Add logging for _stateful_to_state_dict(), stage_state_dict(), and synchronize_staging() (17ea9d14786)
- Revert "[MPS] Make fused rms_norm traceable (e4fe67f6239)
- Revert "[Reopen] [Intel GPU] Set higher tolerance for some models only on XPU Device (41b82611ee1)
- Revert "[Easy] Add more check for elapsedTime of torch.xxx.Event and torch.Event (1ce7969e818)
- faster gather implementation ([#151490](https://github.com/pytorch/pytorch/pull/151490))
- [export] allow partially specifying keys for dynamic shapes dict spec ([#151597](https://github.com/pytorch/pytorch/pull/151597))
- [dynamic shapes] guard_or_false for _reshape_view_helper, utils._infer_size for wildcard dims ([#150127](https://github.com/pytorch/pytorch/pull/150127))
- Revert "[Easy] Fix the compilation warning of BlasKernel. (28974a1ec3b)
- Revert "stage 2 of depreate silent fallback of tuning gemm (b7807759ded)
- Revert "[inductor][test] Skip triton tests for MPS as well, also change reason for skipping SM89 to not IS_BIG_GPU (e434a9152e1)
- Revert "[export] allow partially specifying keys for dynamic shapes dict spec (1b267a58a16)
- Revert "[dynamic shapes] guard_or_false for _reshape_view_helper, utils._infer_size for wildcard dims (97d97aef24b)
- Revert "inductor.config.descriptive_names = False is not actually supported (#145523) (#146051) (6261db7719a)
- Revert "Cache the value of torch_key in subproc (92d0c40c492)
- [Easy][torch.Event] Fix and improve the docs of torch.Event ([#151411](https://github.com/pytorch/pytorch/pull/151411))
- Revert "[Testing] Make test_add_complex3 run on different devices (68f748a9921)
- Revert "[Easy][torch.Event] Fix and improve the docs of torch.Event (c4482565cce)
- Revert "[Easy] Fix the function signature of torch.Event (48761e97376)
- Revert "[Easy] The event_id of torch.cuda.Event and torch.xpu.Event always is 0 (33808f0ebdc)
- Revert "[Easy] Add more check for elapsedTime of torch.xxx.Event and torch.Event (9374064483e)
- Revert "[aot autograd][logging] Profile large missing gaps in compile time tracing (fd04c79878e)
- reroute index to fast implementation for indexing on 0th dimension ([#151753](https://github.com/pytorch/pytorch/pull/151753))
- [dynamic shapes] guard_or_false for _reshape_view_helper, utils._infer_size for wildcard dims ([#150127](https://github.com/pytorch/pytorch/pull/150127))
- Revert "[Intel GPU] Allow XPU backend in Depthwise_conv2d&3d operators (40cf49d4607)
- Revert "reroute index to fast implementation for indexing on 0th dimension (0ff302e8e05)
- Revert "[dynamic shapes] guard_or_false for _reshape_view_helper, utils._infer_size for wildcard dims (e76c0b159a8)
- Do not generate long log messaged for suppressed data dependent errors. ([#151023](https://github.com/pytorch/pytorch/pull/151023))
- Revert "Non-deterministic alert in histc_cuda for floating types only (ed0d2ebaa09)
- Revert "faster gather implementation (f072bf27a75)
- Revert "[ez] Make relaxed constraint error message more user friendly (4504910843a)
- Revert "[Inductor] Add Additional Configs for persistent+TMA version of Triton mm and addmm (3804aed32e0)
- [Optimus][Observability] Improve tlparse logging ([#151635](https://github.com/pytorch/pytorch/pull/151635))
- Revert "[compile][compile time traces] Add more dynamo traces (0bb9b89fb73)
- Revert "Log information about suppressed data dependent errors (aaf71a481b6)
- Revert "Do not log exception when recording is disabled or already recording (459c62ee1db)
- Revert "Do not generate long log messaged for suppressed data dependent errors. (bc6c0bc344e)
- Revert "[Optimus][Observability] Improve tlparse logging (835413baed0)
- Revert "[inductor] Change minimum number of SMs to 60 to let Ada use Triton GEMM backend (72f711e2008)
- [Cutlass] Implement EVT example tensor creation ([#150904](https://github.com/pytorch/pytorch/pull/150904))
- [dynamic shapes] guard_or_false for _reshape_view_helper, utils._infer_size for wildcard dims ([#150127](https://github.com/pytorch/pytorch/pull/150127))
- [dynamo] context manager/decorator for dynamo config patching during tracing ([#150586](https://github.com/pytorch/pytorch/pull/150586))
- Revert "[FlexAttention] Fix device test instantation (7310049c425)
- Revert "[invoke_subgraph][fake tensor] Add finalizer on subgraph instead of the functionalize ctx wrapper (348272e67e6)
- Revert "[fake tensor cache] Support index with non bool/int8 indices (9344da8bd10)
- Revert "[cutlass backend] delay construction of cutlass presets to when called (aa285e6512b)
- Revert "[MPS] Fix test_neg_index_mps (98c53d8b397)
- Revert "Turn on static cuda launcher in OSS (562328501e1)
- Revert "Update torch-xpu-ops commit pin (81723970254)
- [Environment Variable][7/N] Use thread-safe getenv functions ([#140211](https://github.com/pytorch/pytorch/pull/140211))
- Revert "[MPS] Adjust test_sum_dtypes so it can run on MPS. (43f1b60dedf)
- Revert "[Cutlass] Implement EVT example tensor creation (3a170a8ce68)
- Revert "[dynamo] Add guard serialization for tensor matches. (b1d055fd6a0)
- Revert "Add OIDC permissions to bazel workflow (8313bc27f2e)
- [Cutlass] Implement EVT example tensor creation ([#150904](https://github.com/pytorch/pytorch/pull/150904))
- Revert "[Easy] Add more check for elapsedTime of torch.xxx.Event and torch.Event (67f75244ea4)
- Revert "[Inductor] Record Triton’s Base32 Cache Key in .best_config for Debugging (c03359de2dc)
- Revert "[BE] Do not allow PyTorch codebase to use `c10::optional` (0f765773e30)
- Rewrite the guts of torch::jit::Lexer to speed it up ([#151850](https://github.com/pytorch/pytorch/pull/151850))
- Revert "Rewrite the guts of torch::jit::Lexer to speed it up (fa1b4ef6499)
- [Easy][torch.Event] Fix and improve the docs of torch.Event ([#151411](https://github.com/pytorch/pytorch/pull/151411))
- Revert "Update OpenBLAS commit  (c02edba8638)
- Do not generate long log messages for suppressed data dependent errors. ([#151023](https://github.com/pytorch/pytorch/pull/151023))
- Revert "[Inductor UT] Generalize device-bias code in `test_flex_attention.py` (9c864f9b0f1)
- Extend vec backend with BF16 SVE intrinsics ([#143666](https://github.com/pytorch/pytorch/pull/143666))
- Reapply "Rewrite the guts of torch::jit::Lexer to speed it up (#151850)" ([#152250](https://github.com/pytorch/pytorch/pull/152250))
- Revert "Reapply "Rewrite the guts of torch::jit::Lexer to speed it up (#151850)" (e7c19f4f693)
- [Relandx2] Rewrite the guts of torch::jit::Lexer to speed it up   ([#152372](https://github.com/pytorch/pytorch/pull/152372))
- Revert "[Relandx2] Rewrite the guts of torch::jit::Lexer to speed it up   (46419c78997)
- Revert "[OpenReg] Add _lazy_init and rng_state support for OpenReg (3962b8f1e0c)
- Add detailed triton kernel logging to tlparse ([#152197](https://github.com/pytorch/pytorch/pull/152197))
- Revert "[AOTI][reland] Remove typedef for half and bfloat16 (471025c4898)
- Revert "Add detailed triton kernel logging to tlparse (fecaa60c3c3)
- Revert "[MPS][BE] Delete unused lerp functors (e35e31697eb)
- Revert "fix tests broken after #152450 (cc7346bf19c)
- Revert "fix tests broken after #152450 (424e21ae82c)
- Revert "[PT2] Port replace_lce_with_matmul / replace_first_lce_with_fused_matmul_lce to PT2 pre_grad passes (7a9d0d24519)
- Revert "Fix flaky test in test_custom_ops (371999782a0)
- Revert "Change test/inductor/test_standalone_compile to test/inductor/test_compile (702264dad4e)
- Revert "[inductor][BE] Add more debug logs for why fx graph cache doesn't happen (49a72011cc6)
- Revert "[CUDAGraph Trees] support memory allocation on side stream (56039b57782)
- Revert "[inductor][invoke_subgraph] Free the buffers before the subgraph call (2fa39e60ed2)
- Revert "[invoke_subgraph] Simplify output code for subgraph output node (2f1800bc3d5)
- Revert "[inductor][subgraph] Simplify the resulting output code for subgraph (f7b60456cc4)
- Revert "[inductor][invoke_subgraph] Remove assertion checks for outputs of invoke_subgraph (4c8dee7986d)
- Use swap_tensors path in nn.Module.to for all subclasses that override __torch_dispatch__ ([#152539](https://github.com/pytorch/pytorch/pull/152539))
- Revert "[torchgen] Refactor `torchgen.utils.FileManager` to accept `pathlib.Path` (1c04ea4e590)
- [aot][ca] save bw_module in AOTAutogradCache ([#151860](https://github.com/pytorch/pytorch/pull/151860))
- [ca] mark scalar int sizes as dynamic via tensor wrapping ([#151731](https://github.com/pytorch/pytorch/pull/151731))
- [ca] hide unused scalar int sizes from dynamo ([#151962](https://github.com/pytorch/pytorch/pull/151962))
- [dynamo][ca] support dynamic annotations on tensors in ListVariables/TupleVariables ([#152119](https://github.com/pytorch/pytorch/pull/152119))
- [Environment Variable][Rebase] Use thread-safe getenv functions ([#140200](https://github.com/pytorch/pytorch/pull/140200))
- Revert "[BE] Update numba versions (e6989ceea96)
- Revert "[Inductor] Add decomposeK as an autotuning choice for mm (7c3e679ddd5)
- Revert "Use swap_tensors path in nn.Module.to for all subclasses that override __torch_dispatch__ (4f9f1abd6d5)
- Revert "Enable -Wunused on torch targets (6dadfc44570)
- Revert "[cutlass backend] Minor lru_cache to slightly speed up filtering ops (50d4698ac8c)
- [inductor] Realize bucketize/searchsorted output ([#152644](https://github.com/pytorch/pytorch/pull/152644))
- Inductor respects exact strides on custom ops by default ([#150511](https://github.com/pytorch/pytorch/pull/150511))
- Revert "[ROCm] Upgrade ROCm CI to ROCm6.4 (cc28b439504)
- Revert "[inductor] Realize bucketize/searchsorted output (8faa2256951)
- Revert "Avoid triggering ignored requires_grad warning in our code (8dbe1ff34b8)
- Log aot and idx waitcounters. ([#152444](https://github.com/pytorch/pytorch/pull/152444))
- Revert "Log aot and idx waitcounters. (172a7c942ed)
- Revert "[float16]: Fast path for torch.dot with float16/bfloat16 (fdadda21b6c)
- [Inductor] FX backend via Wrapper IR ([#146942](https://github.com/pytorch/pytorch/pull/146942))
- Revert "[Inductor] FX backend via Wrapper IR (99dac7005f8)
- Revert "[c10d] Fix extra CUDA context created by barrier (cc954848d4a)
- Revert "Add infra to run CPython tests under Dynamo (103fe856e1f)
- Revert "[CI] Use cmake from pip instead of conda in CI docker images (d197228d433)
- cleanup, refactor and add missing  self._dde_suppressed checks ([#152657](https://github.com/pytorch/pytorch/pull/152657))
- Revert "Make device check error message more descriptive (451d652873c)
- Revert "cleanup, refactor and add missing  self._dde_suppressed checks (0e2b9482560)
- [Inductor] FX backend via Wrapper IR ([#146942](https://github.com/pytorch/pytorch/pull/146942))
- Revert "[dynamo] Recursively realize the stack_values (fcd5e491382)
- Log aot and idx waitcounters. ([#152444](https://github.com/pytorch/pytorch/pull/152444))
- Revert "[dynamo][ca] support dynamic annotations on tensors in ListVariables/TupleVariables (64bbf58fb4a)
- Revert "[ca] hide unused scalar int sizes from dynamo (8f208dc75a0)
- Revert "[ca] mark scalar int sizes as dynamic via tensor wrapping (f6db749e601)
- Revert "[aot][ca] save bw_module in AOTAutogradCache (a28dcdba2c7)
- Revert "[BE] Update numba versions (61dd2a0cc38)
- Add runtime asserts to AOTI ([#152125](https://github.com/pytorch/pytorch/pull/152125))
- Revert "[CI] Use cmake from pip instead of conda in CI docker images (a7ea115494a)
- [ca] mark scalar int sizes as dynamic via tensor wrapping ([#151731](https://github.com/pytorch/pytorch/pull/151731))
- [ca] hide unused scalar int sizes from dynamo ([#151962](https://github.com/pytorch/pytorch/pull/151962))
- [dynamo][ca] support dynamic annotations on tensors in ListVariables/TupleVariables ([#152119](https://github.com/pytorch/pytorch/pull/152119))
- Revert "Add runtime asserts to AOTI (05326b7e496)
- Revert "[inductor][dynamo] Include operator name in size/stride/alignment assertion (7b806a8cb1e)
- Revert "[dynamo] Support `delattr` on result of `torch.compile(module)` (34d424d8136)
- Revert "[dynamo] Avoid running `torch.nn.Module.__call__` twice under `torch.compile(mod)` (d36261d2e65)
- [ROCm] Maxpool forward NHWC Perf Improvement targeting Resnet scenarios ([#151727](https://github.com/pytorch/pytorch/pull/151727))
- Revert "[BE]: Add PEP621 project section to pyproject.toml (0203f89cc19)
- Revert "[CI] Add opt-in h100 tests (34196301d58)
- Fix fake tensor caching when output has unbacked ([#153034](https://github.com/pytorch/pytorch/pull/153034))
- Revert "Fix fake tensor caching when output has unbacked (e6dccb036e1)
- [Hierarchical Compilation] Track node mutations ([#152389](https://github.com/pytorch/pytorch/pull/152389))
- [Hierarchical Compilation] Use universal flatten APIs ([#152505](https://github.com/pytorch/pytorch/pull/152505))
- [Hierarchical Compile] Add mutation dependencies to topological sorting ([#152410](https://github.com/pytorch/pytorch/pull/152410))
- [Hierarchical Compile] Take into account mutation deps in cycle detection ([#152506](https://github.com/pytorch/pytorch/pull/152506))
- [Hierarchical Compile] Replace tracing alias and mutation check with dynamo impl ([#152570](https://github.com/pytorch/pytorch/pull/152570))
- [Dynamo] Optimize dedupe region ancestor tracking ([#152589](https://github.com/pytorch/pytorch/pull/152589))
- refine fp32 precision api ([#125888](https://github.com/pytorch/pytorch/pull/125888))
- Revert "`has_triton`: Use the device interface for detecting Triton availability (01bb249978a)
- Revert "refine fp32 precision api (fdc387ec7c9)
- Forward fix #151727 ([#153306](https://github.com/pytorch/pytorch/pull/153306))
- Revert "[Dynamo] Optimize dedupe region ancestor tracking (aa7fe6af416)
- Revert "[Dynamo] Fix typing in graph_deduplication.py (0071fdab9ee)
- Revert "[Hierarchical Compile] Replace tracing alias and mutation check with dynamo impl (53ebcabb527)
- Revert "[Hierarchical Compile] Take into account mutation deps in cycle detection (0e36887209b)
- Revert "[Hierarchical Compile] Add mutation dependencies to topological sorting (47df195065d)
- Revert "Forward fix #151727 (8511d210819)
- Revert "[ROCm] Maxpool forward NHWC Perf Improvement targeting Resnet scenarios (daca611465c)
- Revert "[Hierarchical Compilation] Use universal flatten APIs (78d752e96a0)
- Revert "[Hierarchical Compilation] Track node mutations (5c3fddb9cca)
- Rewrite autograd producer consumer stream sync logic ([#151079](https://github.com/pytorch/pytorch/pull/151079))
- Revert "[export][cond] support merging constant ints as unbacked symint (641e4bee674)
- Enable accelerator to perform streaming backward ([#153412](https://github.com/pytorch/pytorch/pull/153412))
- Revert "[DSD] Don't pop tensors if they are on Meta device (8d7dec6e922)
- [Hierarchical Compilation] Track node mutations ([#152389](https://github.com/pytorch/pytorch/pull/152389))
- [Hierarchical Compilation] Use universal flatten APIs ([#152505](https://github.com/pytorch/pytorch/pull/152505))
- [Hierarchical Compile] Add mutation dependencies to topological sorting ([#152410](https://github.com/pytorch/pytorch/pull/152410))
- [Hierarchical Compile] Take into account mutation deps in cycle detection ([#152506](https://github.com/pytorch/pytorch/pull/152506))
- [Hierarchical Compile] Replace tracing alias and mutation check with dynamo impl ([#152570](https://github.com/pytorch/pytorch/pull/152570))
- [Dynamo] Optimize dedupe region ancestor tracking ([#152589](https://github.com/pytorch/pytorch/pull/152589))
- [ROCm] Maxpool forward NHWC Perf Improvement targeting Resnet scenarios ([#151727](https://github.com/pytorch/pytorch/pull/151727))
- Update the heuristic for AArch64 bmm/baddbmm ([#149122](https://github.com/pytorch/pytorch/pull/149122))
- Revert "Enable accelerator to perform streaming backward (a628efd1e8e)
- Revert "Rewrite autograd producer consumer stream sync logic (2c1912452d7)
- Revert "Fix skipIfXpu and skipIfHpu disables tests when used on class (2344eca5ebd)
- Revert "[ROCm] Maxpool forward NHWC Perf Improvement targeting Resnet scenarios (6ef1cbc1910)
- Revert "[CUDA][CUDNN] Dispatch to cuDNN for non-batch-splittable 64-bit NCHW convolutions (bf0fe4f8287)
- Revert "[cuDNN][SDPA] cuDNN SDPA refactor/cleanup, nested tensor backward, test priority bump for `sm90`, `sm100` (f363a3f51ab)
- [ROCm] Maxpool forward NHWC Perf Improvement targeting Resnet scenarios ([#151727](https://github.com/pytorch/pytorch/pull/151727))
- Revert "[FlexAttention] Enforce Q,K,V memory layouts for fp8 flex attention to avoid perf degradation (71027b13b23)
- [BE]: Enable RUFF TRY400 rule - log.exception ([#153473](https://github.com/pytorch/pytorch/pull/153473))
- Revert "[ca][dtensor] run real PG dtensor tests under CA (2327c9eedcc)
- Revert "[ca][dynamo] always run eager checkpoint region's recomputation in eager (236b08cbf83)
- Revert "Delete TorchScript based Android demo app and point to ExecuTorch (ae0e8f0c731)
- Fix fake tensor caching when output has unbacked ([#153034](https://github.com/pytorch/pytorch/pull/153034))
- Revert "[inductor][dynamo] Include operator name in size/stride/alignment assertion (4d073af58ca)
- Revert "[Ez][BE]: Remove accidental classvar (86c6f71ddbd)
- Revert "[BE]: Enable RUFF TRY400 rule - log.exception (3443627e078)
- [dynamo] Make `OptimizedModule` more robust in attribute reads and writes ([#153637](https://github.com/pytorch/pytorch/pull/153637))
- Rewrite autograd producer consumer stream sync logic ([#151079](https://github.com/pytorch/pytorch/pull/151079))
- Revert "[dynamo] Make `OptimizedModule` more robust in attribute reads and writes (c2dda47bc59)
- cleanup, refactor and add missing  self._dde_suppressed checks ([#152657](https://github.com/pytorch/pytorch/pull/152657))
- Revert "cleanup, refactor and add missing  self._dde_suppressed checks (1748fa529a9)
- [dynamo] Make `OptimizedModule` more robust in attribute reads and writes ([#153637](https://github.com/pytorch/pytorch/pull/153637))
- Revert "Reapply "Delete TorchScript based Android demo app and point to ExecuTorch (#153633)" (084c4aa6140)
- Revert "[CUDA][cuBLAS][cuBLASLt] avoid polluting prefer cuBLAS/Lt setting across tests (40339c1e997)
- Revert "[Dynamo] added warning message for tracing lru_cache wrapped functions (75eb2f3ff6c)
- Improve torch.ops typing ([#153558](https://github.com/pytorch/pytorch/pull/153558))
- Enable accelerator to perform streaming backward ([#153412](https://github.com/pytorch/pytorch/pull/153412))
- cleanup, refactor and add missing  self._dde_suppressed checks ([#152657](https://github.com/pytorch/pytorch/pull/152657))
- Revert "Recheck autotune cache on static cuda launcher load (b0e5402377c)
- [Distributed][CI] Rework continuous TestCase ([#153653](https://github.com/pytorch/pytorch/pull/153653))
- Revert "[Distributed][CI] Rework continuous TestCase (674a85cf263)
- Revert "[CI] Reuse old whl (1ccacc028db)
- Revert "[CI] Reuse old whl (8c40c9ffcb4)
- Revert "Improve torch.ops typing (d81217be2e4)
- Revert "Cache code generation during triton template expansion and enable it  for mm_template. (b15720118a8)
- [DDP] rebuilt bucket order when find_unused_parameters=true ([#153404](https://github.com/pytorch/pytorch/pull/153404))
- [AOTI] Add an option to specify custom op C shim ([#153851](https://github.com/pytorch/pytorch/pull/153851))
- Revert "FakeTensorMode dispatch shouldn't include bypass in exception context (9849c79fa21)
- Revert "Fix fake tensor caching when output has unbacked (1075bb37d34)
- Revert "[AOTI] Add an option to specify custom op C shim (3102ae67986)
- Revert "Fixed an issue with XPU skip so the test_decompose_mem_bound_mm.py suite can be ran correctly (500a710422f)
- Revert "[inductor][cutlass backend] Add 2 stage autotuning aka prescreening (7b7604fdb4e)
- [3/n][Optimus][Auto-AC] Support float8_e4m3fn quantization type and set scaling as the default ([#153802](https://github.com/pytorch/pytorch/pull/153802))
- Revert "[cuBLASLt] relax `addmm` cuBLASLt constraint (cf6e5d18818)
- Revert "[CI][CUDA] Move cu118 distributed pull jobs to cu126, move cu124-sm75 to cu126-sm75 (1478d0185c2)
- Revert "[cuBLAS][cuBLASLt] Use cuBLAS default workspace size in Lt (531d8f5fb6e)
- [BE]: Type previously untyped decorators ([#153726](https://github.com/pytorch/pytorch/pull/153726))
- Revert "[3/n][Optimus][Auto-AC] Support float8_e4m3fn quantization type and set scaling as the default (3eb8fa081a8)
- [aoti] Add MPS runner and shim ([#153964](https://github.com/pytorch/pytorch/pull/153964))
- [aoti] Initial Metal support ([#153959](https://github.com/pytorch/pytorch/pull/153959))
- Revert "cpp_wrapper: build non-performance-sensitive code at O1 (261897734a4)
- Revert "[aoti] Initial Metal support (47a01f3efb4)
- Revert "[aoti] Add MPS runner and shim (a82c8891d56)
- Revert "[BE]: Type previously untyped decorators (7d3dab6b901)
- Revert "[inductor][cutlass backend] Add 2 stage autotuning aka prescreening (025c5cc0483)
- Patch the _is_conv_node function ([#153749](https://github.com/pytorch/pytorch/pull/153749))
- Revert "[DDP] rebuilt bucket order when find_unused_parameters=true (59c5fff2aa2)
- [aoti] Add MPS runner and shim ([#153964](https://github.com/pytorch/pytorch/pull/153964))
- [aoti] Initial Metal support ([#153959](https://github.com/pytorch/pytorch/pull/153959))
- Revert "Update the heuristic for AArch64 bmm/baddbmm (866142ff166)
- Fix fake tensor caching when output has unbacked ([#153034](https://github.com/pytorch/pytorch/pull/153034))
- [export] Move PT2ArchiveWriter/Reader to torch/export ([#153795](https://github.com/pytorch/pytorch/pull/153795))
- Revert "[export] Move PT2ArchiveWriter/Reader to torch/export (4ff19ecf665)
- Revert "Patch the _is_conv_node function (561a11aa68c)
- [export] Move PT2ArchiveWriter/Reader to torch/export ([#153795](https://github.com/pytorch/pytorch/pull/153795))
- Revert "[AOTI][cutlass backend] Do not remove the cutlass kernel .o file after packaging (90855835ffc)
- Revert "[c10d] Add support for testing SIGABRT return (28af44285bf)
- [Distributed][CI] Rework continuous TestCase ([#153653](https://github.com/pytorch/pytorch/pull/153653))
- Revert "[c10d] Add support for testing SIGABRT return (54932d865e5)
- Revert "Re-enable FakeTensor caching for SymInts (3f64502c984)
- Revert "[executorch hash update] update the pinned executorch hash (b643076e4e0)
- Revert "[executorch hash update] update the pinned executorch hash (ef6306e1c6c)
- introduce definitely_contiguous  and use it for reshape and tensor meta data computation.  ([#153432](https://github.com/pytorch/pytorch/pull/153432))
- Revert "introduce definitely_contiguous  and use it for reshape and tensor meta data computation.  (11a51a11afb)
- [ROCm] Improve vectorized elementwise kernel performance in MI300X ([#153634](https://github.com/pytorch/pytorch/pull/153634))
- Revert "[ROCm] Improve vectorized elementwise kernel performance in MI300X (8c0f07f9449)
- [Inductor] Improve typing, and prepare for ABI-compatible AOTI C-shim dispatching ([#154371](https://github.com/pytorch/pytorch/pull/154371))
- Revert "Move inductor workflows focal (ubuntu 20.04) -> jammy (ubuntu 22.04) (fa6ca59079c)
- Revert "[Inductor] Improve typing, and prepare for ABI-compatible AOTI C-shim dispatching (555fc058680)
- [ROCm] Improve vectorized elementwise kernel performance in MI300X ([#153634](https://github.com/pytorch/pytorch/pull/153634))
- Revert "Remove outdated CUDA 11 conditions (241f8dc84df)
- [AOTI] Support multi-arch when using package_cpp_only ([#154414](https://github.com/pytorch/pytorch/pull/154414))
- Fix the Problems About Defining Static Variable in Inline Function ([#147095](https://github.com/pytorch/pytorch/pull/147095))
- introduce definitely_contiguous  and use it for reshape and tensor meta data computation.  ([#153432](https://github.com/pytorch/pytorch/pull/153432))
- Revert "[AOTI] Support multi-arch when using package_cpp_only (fdc339003b7)
- Revert "[dynamo, nested graph breaks] add skip_frame debugging function (e86439ed5be)
- Revert "[dynamo, nested graph breaks] remove block stack graph break in output_graph (5fd7004dc9a)
- Revert "[dynamo, nested graph breaks] refactor codegen to minimize NULL codegen'ing (9603d6382d6)
- Revert "[dynamo, nested graph breaks] small fixes to resume function generation (a75e3a02be0)
- Remove MemPoolContext  ([#154042](https://github.com/pytorch/pytorch/pull/154042))
- Revert "Fix the Problems About Defining Static Variable in Inline Function (d4ab8e74f36)
- [Inductor] Improve typing, and prepare for ABI-compatible AOTI C-shim dispatching ([#154371](https://github.com/pytorch/pytorch/pull/154371))
- Use 3.27 as the minimum CMake version ([#153153](https://github.com/pytorch/pytorch/pull/153153))
- Revert "Use 3.27 as the minimum CMake version (53b0f6f5436)
- [c10d] Separate monitoring thread into a class in PGNCCL ([#153977](https://github.com/pytorch/pytorch/pull/153977))
- Enable C++ dynamic shape guards by default ([#140756](https://github.com/pytorch/pytorch/pull/140756))
- convert inductor codecache to use getArtifactLogger ([#153766](https://github.com/pytorch/pytorch/pull/153766))
- inductor codecache: include private inductor configs in cache key ([#153672](https://github.com/pytorch/pytorch/pull/153672))
- Revert "[c10d] Separate monitoring thread into a class in PGNCCL (852b99eba08)
- Revert "Enable C++ dynamic shape guards by default (ba51f4876d8)
- [forward fix] add support for MemoryFormat after type tightening ([#154658](https://github.com/pytorch/pytorch/pull/154658))
- Revert "Remove MemPoolContext  (d173ba5a756)
- Revert "[Inductor] Add NaN assert to returned values from generated code (639f459cb6b)
- Use 3.27 as the minimum CMake version ([#153153](https://github.com/pytorch/pytorch/pull/153153))
- Revert "Use 3.27 as the minimum CMake version (7e8532077f8)
- Revert "inductor codecache: include private inductor configs in cache key (31f95b5d2e8)
- Revert "[internal] Expose additional metadata to compilation callbacks (35fc5c49b47)
- Revert "[Inductor] Add NaN assert to returned values from generated code (fb67fa99680)
- [draft export] avoid storing intermediate real tensors in proxies ([#154630](https://github.com/pytorch/pytorch/pull/154630))
- Revert "convert inductor codecache to use getArtifactLogger (1193bf08557)
- Use 3.27 as the minimum CMake version ([#153153](https://github.com/pytorch/pytorch/pull/153153))
- Revert "[draft export] avoid storing intermediate real tensors in proxies (0fab32290a4)
- Revert "Use 3.27 as the minimum CMake version (108422ac268)
- Revert "Aten vector default constructors set to 0, add fnmadd and fnmsub (3e71016459f)
- Use 3.27 as the minimum CMake version ([#153153](https://github.com/pytorch/pytorch/pull/153153))
- Revert "Use 3.27 as the minimum CMake version (bd10ea4e6cb)
- [BE][Ez]: Fully type nn.utils.clip_grad ([#154801](https://github.com/pytorch/pytorch/pull/154801))
- Revert "[BE] Cleanup old ExecuTorch codegen and runtime code (67067512a19)
- Revert "[inductor] Add kernel_hash_key to ChoiceCaller (69e22301da7)
- Revert "Add CPython exception tests (e3af628b0d3)
- Revert "[Inductor UT] Reuse test_fused_attention.py for Intel GPU. (ac65e94f450)
- Revert "[Inductor] Add attention pattern for model DistilBert in transformers==4.44.2. (37eb909c94a)
- Revert "Remove AttributeError constructor (ef92653022f)
- Revert "[dynamo][guards] Flush cache to more accurately measure guard overhead (b86aaaae0bd)
- Revert "[dynamo] Record the pre-graph bytecode using fast record function event (a7e496a8968)
- [Cutlass] fp8 dynamic shapes test ([#154829](https://github.com/pytorch/pytorch/pull/154829))
- [Cutlass] EVT dynamic shapes support ([#154835](https://github.com/pytorch/pytorch/pull/154835))
- Revert "[BE][Ez]: Fully type nn.utils.clip_grad (50de6ae2536)
- [c10d] Separate monitoring thread into a class in PGNCCL ([#153977](https://github.com/pytorch/pytorch/pull/153977))
- Revert "[dynamo][dynamic] Recompilation hint for nn module integer attributes (a0f25445028)
- Revert "[dynamo] Mark a vt unspecialized nn module variable source earlier (a99a01a677f)
- Revert "[Cutlass] EVT dynamic shapes support (3fa3dbdb1fb)
- Revert "[Cutlass] fp8 dynamic shapes test (6f93ce3c868)
- Revert "Add __main__ guards to jit tests (20912673a6c)
- Revert "Always set CPU affinity for benchmark jobs (4405dc14874)
- Revert "Inductor logging + analysis of torch.profile (5e034334430)
- Revert "Add randint_like tensor overload for high (5130ac64f4f)
- Revert "[forward fix] add support for MemoryFormat after type tightening (93012d2290e)
- Revert "[Intel GPU] Make SDPA output has the same stride as Query. (d3c8f36ba0d)
- Revert "Inductor unit tests: cuda 12.6 -> 12.8 (f60b2712dd8)
- Revert "[test][dynamo] skip test_deopt_from_append_list on python>=3.13.3 (523b637cbeb)
- Revert "SDPA support gfx950 (3c72b9fd8fe)
- Revert "[reland][dynamo] Record the pre-graph bytecode using fast record function event (e01fde82131)
- Add dont constant fold flag ([#154945](https://github.com/pytorch/pytorch/pull/154945))
- Revert "Add CPython generator/contextlib tests (a1057cda31f)
- Add pinned numpy and fix build ([#155129](https://github.com/pytorch/pytorch/pull/155129))
- Revert "Add dont constant fold flag (05dd638ee98)
- [Cutlass] fp8 dynamic shapes test ([#154829](https://github.com/pytorch/pytorch/pull/154829))
- [Cutlass] EVT dynamic shapes support ([#154835](https://github.com/pytorch/pytorch/pull/154835))
- Revert "Add Intel GPU info collection to the collect env script (0db3e0cf296)
- Revert "Add pinned numpy and fix build (d3d64c6db09)
- Custom FX pass for inductor's backend registration ([#154841](https://github.com/pytorch/pytorch/pull/154841))
- Revert "[BE] Update cudnn to 9.10.1.4 (9656251bb1b)
- Revert "Turn on new tiling by default (b0fbbef1361)
- Revert "[inductor] Add typing to _inductor/ir.py (7e4c097b075)
- Revert "[inductor] use int64 for large index (27df0c56b7c)
- Revert "[Inductor] Improve typing, and prepare for ABI-compatible AOTI C-shim dispatching (95448b2ce61)
- Revert "Add Intel GPU info collection to the collect env script (6fb62931593)
- Revert "Custom FX pass for inductor's backend registration (79bdafe5b61)
- Add stack_trace on make_fx ([#155155](https://github.com/pytorch/pytorch/pull/155155))
- Revert "Add stack_trace on make_fx (620415e018c)
- Fix docs build ([#155129](https://github.com/pytorch/pytorch/pull/155129))
- Revert "[Graph Partition] move cpu scalar tensor to gpu (2c1a93a0ae7)
- Revert "Update auto-tuning support for _scaled_grouped_mm (e12597090c4)
- Revert "[cuBLASLt][cuBLAS] Support 2D bias and `beta != 1.0` in cuBLASLt (7a48cc69900)
- Revert "Move non inductor workflows cuda 12.6->cuda 12.8 (660695f11dd)
- [inductor][triton pin] TMA shim refactor & mm, mm_scaled_grouped support ([#155182](https://github.com/pytorch/pytorch/pull/155182))
- Revert "[Testing] Add more models to MPSInductor tests (805297981ae)
- Add dont constant fold flag ([#154945](https://github.com/pytorch/pytorch/pull/154945))
- Revert "Inductor logging + analysis of torch.profile (eb152ab1dd9)
- Revert "Make open device registration tests standalone (8347268edcf)
- Revert "[BE] Update cudnn to 9.10.1.4 (40fefe2871a)
- Revert "[inductor][triton pin] TMA shim refactor & mm, mm_scaled_grouped support (3b7c5e6fa5c)
- inductor codecache: include private inductor configs in cache key ([#153672](https://github.com/pytorch/pytorch/pull/153672))
- Revert "Add Intel GPU info collection to the collect env script (45c5a232373)
- Revert "[cuBLASLt][cuBLAS] Support 2D bias and `beta != 1.0` in cuBLASLt (3a43dba21ff)
- Revert "[dynamo] added github_cli to detect unimplemented_v2 calls (f80a61adf57)
- Revert "[BE]: Update cudnn to 9.10.2.21 (f59c76b5494)
- [draft export] avoid storing intermediate real tensors in proxies ([#154630](https://github.com/pytorch/pytorch/pull/154630))
- Revert "[PT2][partitioners] Add aten.split to view_ops list (8372d0986a4)
- Revert "[CI] Use `setup-python` from for Mac tests (2a3b41cbd07)
- Use swap_tensors path in nn.Module.to for FakeTensor ([#152539](https://github.com/pytorch/pytorch/pull/152539))
- Revert "[BE] Raise `NotImplementedError` (ce6e0523f99)
- Revert "Add view_simple as meta function for view,  and avoid calling reshape_view_helper.  (06408dae49d)
- Revert "Enable manywheel build and smoke test on main branch for ROCm (d7e3c9ce828)
- Revert "[BE]: Sync cusparselt 12.9 with static build and other cuda 12 (4574b39aa45)
- [Quant][CPU] fix fake_quantize_per_tensor_affine of inf values ([#155109](https://github.com/pytorch/pytorch/pull/155109))
- Custom FX pass for inductor's backend registration ([#154841](https://github.com/pytorch/pytorch/pull/154841))
- [Cutlass] Fix buffer missing issues ([#155897](https://github.com/pytorch/pytorch/pull/155897))
- Revert "[cuBLASLt][cuBLAS] Support 2D bias and `beta != 1.0` in cuBLASLt (b8d96c3f78a)
- Revert "Unify dynamic shapes APIs naming 2 (expect_true and check) (503362d019b)
- Revert "[C10][CUDA] Eagerly create context on torch.cuda.set_device(device) call (365ce465f39)
- Revert "[Quant][CPU] fix fake_quantize_per_tensor_affine of inf values (e9fdaf8701b)
- [Inductor] Delay codegen for fallback arguments and improve typing ([#154371](https://github.com/pytorch/pytorch/pull/154371))
- Revert "Implement guard collectives (61b271e0f3f)
- Revert "[MPS][Testing][BE] Fix samples for full_like (03488d820c2)
- Revert "Implement guard collectives (190f76fa313)
- Revert "[Cutlass] Fix buffer missing issues (35ecd7c2d44)
- [Draft][CUDA] Use runtime driver API for cuStreamWriteValue32 ([#156097](https://github.com/pytorch/pytorch/pull/156097))
- Revert "[inductor][cutlass] binary remote cache (ec08eb8ba22)
- Revert "[Docs] Convert to markdown to fix 155032 (fa4f07b5b80)
- [dynamo] add set_fullgraph decorator/context manager ([#154289](https://github.com/pytorch/pytorch/pull/154289))
- Revert "[dynamo] raise hard error if error is encountered while tracing resume function prologue (8f02161d101)
- Revert "[dynamo] handle fullgraph toggle using nested torch.compile (6201981f48a)
- Revert "[dynamo] fix set_fullgraph for nested calls (408d9884b07)
- Revert "[dynamo] add set_fullgraph decorator/context manager (c5d3e7a4ff4)
- Revert "[dynamo] control one_graph behavior additionally through config (ce3406817d5)
- Revert "[PT2][partitioners] raise getitems in partitioners to allow earlier release of buffers (94f8679019e)
- [cuDNN][64-bit indexing] update conv depthwise 64bit indexing dispatch condition to match native kernel ([#156140](https://github.com/pytorch/pytorch/pull/156140))
- Revert "[PT2]load dense delta by trimming prefixes (728cf6721e2)
- Revert "[Draft][CUDA] Use runtime driver API for cuStreamWriteValue32 (bfccfa0b312)
- [aot][ca] save bw_module in AOTAutogradCache ([#151860](https://github.com/pytorch/pytorch/pull/151860))
- Revert "[build] Create target for flash attention (a8fe9829932)
- Revert "Refine alignment check along dynamic dimension for grouped MMs (0b62465b99b)
- Revert "[cuDNN][64-bit indexing] update conv depthwise 64bit indexing dispatch condition to match native kernel (317af4c87b7)
- [dynamo] add set_fullgraph decorator/context manager ([#154289](https://github.com/pytorch/pytorch/pull/154289))
- Revert "[Precompile] Hook up backend="inductor"  (edd45f3a020)
- Revert "Upgrade to DLPack 1.0. (b4442f42a93)
- Revert "[ROCm] Bump AOTriton to 0.10b (1036f6d114b)
- Revert "[InductorBench] Fix accuracy validation logic for MPS (96d082d06bd)
- Revert "[BE][MPS] Refactor core matmul logic into matmul_core (d309cd1d502)
- Revert "[BE] Make Eigen an optional dependency (208ec60e72a)
- Revert "[dynamo] raise hard error if error is encountered while tracing resume function prologue (754c04aa062)
- [Draft][CUDA] Use runtime driver API for cuStreamWriteValue32 ([#156097](https://github.com/pytorch/pytorch/pull/156097))
- Revert "[nativert] move layout planner algorithms to libtorch (d846e213553)
- [BE][3/16] fix typos in torch/ (torch/_inductor/) ([#156313](https://github.com/pytorch/pytorch/pull/156313))
- [BE][5/16] fix typos in torch/ (torch/distributed/) ([#156315](https://github.com/pytorch/pytorch/pull/156315))
- [BE][6/16] fix typos in torch/ ([#156316](https://github.com/pytorch/pytorch/pull/156316))
- Revert "[BE][11/16] fix typos in torch/ (torch/csrc/distributed/) (4b55871e06d)
- Revert "[BE][9/16] fix typos in torch/ (torch/csrc/) (1d3bca40ed1)
- Revert "[BE][7/16] fix typos in torch/ (torch/csrc/) (035a68d25aa)
- Revert "[BE][6/16] fix typos in torch/ (3f44fdc03d6)
- Revert "[BE][5/16] fix typos in torch/ (torch/distributed/) (145d4cdc119)
- Revert "[BE][4/16] fix typos in torch/ (torch/_dynamo/) (5b427c92a88)
- Revert "[BE][3/16] fix typos in torch/ (torch/_inductor/) (f1331f3f1b4)
- Revert "[dynamo] handle fullgraph toggle using nested torch.compile (ee3d9969cc2)
- Revert "[dynamo] fix set_fullgraph for nested calls (c10eeb5bad7)
- Revert "[dynamo] add set_fullgraph decorator/context manager (5e56db59d46)
- Revert "[dynamo] control one_graph behavior additionally through config (b5c8b8d09f0)
- [BE][3/16] fix typos in torch/ (torch/_inductor/) ([#156313](https://github.com/pytorch/pytorch/pull/156313))
- [BE][5/16] fix typos in torch/ (torch/distributed/) ([#156315](https://github.com/pytorch/pytorch/pull/156315))
- [BE][6/16] fix typos in torch/ ([#156316](https://github.com/pytorch/pytorch/pull/156316))
- Remove remaining CUDA 12.4 CI code ([#155412](https://github.com/pytorch/pytorch/pull/155412))
- [Cutlass] Fix buffer missing issues ([#155897](https://github.com/pytorch/pytorch/pull/155897))
- Revert "Use CMake wholearchive group (4f70fbbd16d)
- Revert "Enable Leak Sanitizer (f5e1b24945c)
- Revert "[Draft][CUDA] Use runtime driver API for cuStreamWriteValue32 (e583b888194)
- Revert "[dynamo] fixes to lru_cache message and adding user stack trace in debug mode (55ef7b15e0b)
- Revert "[invoke_subgraph] make collect_meta_analysis fake prop cachable (35d03398e51)
- Revert "[invoke_subgraph] make same subgraph share get_attr target (d061a02e6ec)
- Revert "Use official CUDAToolkit module in CMake (b1d62febd03)
- Revert "[aotd] Support mutations of the same input in fw and bw (e600e044a77)
- Add unified memory APIs for torch.accelerator ([#152932](https://github.com/pytorch/pytorch/pull/152932))
### not user facing
- [AMD] Various fixes for mem efficient attention on CK backend ([#148986](https://github.com/pytorch/pytorch/pull/148986))
- Enable misc-use-internal-linkage check and apply fixes ([#148948](https://github.com/pytorch/pytorch/pull/148948))
- [TD] test_cpp_extensions_aot_ninja corresponds to things in test/cpp_extensions  ([#148992](https://github.com/pytorch/pytorch/pull/148992))
- Reland: [inductor] Simplify grid handling ([#148305](https://github.com/pytorch/pytorch/pull/148305))
- [cutlass backend] try make cutlass backend benchmark more robust ([#149015](https://github.com/pytorch/pytorch/pull/149015))
- [CUDA] try to abate some flakiness in `test_stream_event_nogil` ([#148796](https://github.com/pytorch/pytorch/pull/148796))
- [ROCm][TunableOp] hipblaslt tf32 support ([#145946](https://github.com/pytorch/pytorch/pull/145946))
- Update nightly PyTorch version to 2.8.0 ([#149038](https://github.com/pytorch/pytorch/pull/149038))
- [cutlass backend] switch layout for cutlass backend benchmark ([#149009](https://github.com/pytorch/pytorch/pull/149009))
- [logging] Set compile_id in the CachingAutotuner during compilation so we have it for dynamo_timed logging ([#148693](https://github.com/pytorch/pytorch/pull/148693))
- [Inductor UT] Enable PYTORCH_TESTING_DEVICE_ONLY_FOR test case filter for test_torchinductor.py ([#149023](https://github.com/pytorch/pytorch/pull/149023))
- Consolidate torchbind fake class registration ([#149063](https://github.com/pytorch/pytorch/pull/149063))
- [docs] fix autograd description on convex function case ([#148658](https://github.com/pytorch/pytorch/pull/148658))
- skip test_torch_dynamo_codegen_pow if CPU backend is not cpp ([#146595](https://github.com/pytorch/pytorch/pull/146595))
- [AOTI][refactor] Split MiniArrayRef into a separate header ([#149073](https://github.com/pytorch/pytorch/pull/149073))
- Update Kineto Submodule ([#149089](https://github.com/pytorch/pytorch/pull/149089))
- [ROCm] Use generated CK config.h rather than system ([#147993](https://github.com/pytorch/pytorch/pull/147993))
- Add back fake class registration to test_torchbind ([#149137](https://github.com/pytorch/pytorch/pull/149137))
- Use std::string_view in get_fully_qualified_type_name ([#145197](https://github.com/pytorch/pytorch/pull/145197))
- [AOTInductor] Activate CPU test for update_constant_buffer ([#149162](https://github.com/pytorch/pytorch/pull/149162))
- [AOTInductor] [BE] Add swap_constant_buffer into pybind for tests. ([#149167](https://github.com/pytorch/pytorch/pull/149167))
- Clean up grid in execution trace ([#149159](https://github.com/pytorch/pytorch/pull/149159))
- [pytree] add APIs to determine a class is a namedtuple or PyStructSequence ([#113257](https://github.com/pytorch/pytorch/pull/113257))
- Remove test decorations on MacOS 12 ([#148942](https://github.com/pytorch/pytorch/pull/148942))
- [RFC] First version of statically compiled launcher for triton compiled CUDA kernels ([#148561](https://github.com/pytorch/pytorch/pull/148561))
- [EZ] Fix typo in UnaryOps.mm ([#149217](https://github.com/pytorch/pytorch/pull/149217))
- Enable oneDNN dispatch for gemm bf16bf16->bf16  ([#148197](https://github.com/pytorch/pytorch/pull/148197))
- [ATen-CPU] Add `math.h` for Gelu ([#149164](https://github.com/pytorch/pytorch/pull/149164))
- [BE] Parametrize `TestMPS.test_binops_dtype_precedence` ([#149234](https://github.com/pytorch/pytorch/pull/149234))
- [MPS/metal] Add missing `inline` to function definitions. ([#149265](https://github.com/pytorch/pytorch/pull/149265))
- [dynamo][guards][serialization] Dont use ID_MATCH guard for bool and None ([#149228](https://github.com/pytorch/pytorch/pull/149228))
- [ARM64][CUDA] skip string pattern matching in `test_workspace_allocation_error` ([#149236](https://github.com/pytorch/pytorch/pull/149236))
- Skip some tests not using gradcheck on slowgradcheck ([#149220](https://github.com/pytorch/pytorch/pull/149220))
- optimize the decomposition of aten.native_group_norm ([#144733](https://github.com/pytorch/pytorch/pull/144733))
- Update slow tests ([#149300](https://github.com/pytorch/pytorch/pull/149300))
- [ROCm] testing: enable MEFF/FA unittests for gfx1100 ([#148911](https://github.com/pytorch/pytorch/pull/148911))
- [MTIA] Add _mtia_exchangeDevice to MTIA module ([#149322](https://github.com/pytorch/pytorch/pull/149322))
- [ROCm] Unskip flex attention UTs after triton 3.3 bump ([#148327](https://github.com/pytorch/pytorch/pull/148327))
- allow extra args for parameterization of tests in inductor ([#149154](https://github.com/pytorch/pytorch/pull/149154))
- [AOTInductor] [BE] Add macro for loading symbols in aoti runner ([#149249](https://github.com/pytorch/pytorch/pull/149249))
- [AOTI] Update test runner to use the new APIs ([#147105](https://github.com/pytorch/pytorch/pull/147105))
- [MPS/BE] @parametrize generation of pointwise_ops. ([#149363](https://github.com/pytorch/pytorch/pull/149363))
- [MPS/BE] Remove decorator that skipped test on macOS 12. ([#149365](https://github.com/pytorch/pytorch/pull/149365))
- [dynamo][guards][serialization] Dont use ID_MATCH guard for bool and None ([#149228](https://github.com/pytorch/pytorch/pull/149228))
- Replace c10::is_pod with std::is_trivial ([#149286](https://github.com/pytorch/pytorch/pull/149286))
- [BE] Add STABLE_LIBRARY test for multiple returns ([#149230](https://github.com/pytorch/pytorch/pull/149230))
- [MAIA] [Autocast] Enable autocast on MAIA device ([#148511](https://github.com/pytorch/pytorch/pull/148511))
- Fix torchbind schema str generation ([#149239](https://github.com/pytorch/pytorch/pull/149239))
- refresh benchmarks results. ([#149347](https://github.com/pytorch/pytorch/pull/149347))
- [AOTI] Forward fix unit test failures ([#149401](https://github.com/pytorch/pytorch/pull/149401))
- [MTIA] Add _mtia_maybeExchangeDevice to MTIA module ([#149340](https://github.com/pytorch/pytorch/pull/149340))
- [ROCm][TunableOp] Minor fix to BLAS logging for ScaledGEMM with no bias vector. ([#149357](https://github.com/pytorch/pytorch/pull/149357))
- [ROCm] enable CK backend for bf16/fp16 on gfx11 ([#143971](https://github.com/pytorch/pytorch/pull/143971))
- Release.md readability improvements ([#149402](https://github.com/pytorch/pytorch/pull/149402))
- Avoid unnecessary clone in torch.cuda.set_rng_state ([#149283](https://github.com/pytorch/pytorch/pull/149283))
- [ROCm][Windows] Enable torchvision build with ROCm on Windows ([#147382](https://github.com/pytorch/pytorch/pull/147382))
- [xnnpack] Expose subgraph symbols ([#149397](https://github.com/pytorch/pytorch/pull/149397))
- Fix format string in ck_gemm_template.h for int64_t variables ([#149438](https://github.com/pytorch/pytorch/pull/149438))
- Cache the get_device_module result ([#149207](https://github.com/pytorch/pytorch/pull/149207))
- [MTIA] Add _mtia_getCurrentRawStream to MTIA module ([#149436](https://github.com/pytorch/pytorch/pull/149436))
- Add Missing Communication collectives ([#147379](https://github.com/pytorch/pytorch/pull/147379))
- [ROCm][Windows] Disable hipSPARSE and CK declarations and remove references for Windows ([#149195](https://github.com/pytorch/pytorch/pull/149195))
- Fix the invalid link for FX ([#149289](https://github.com/pytorch/pytorch/pull/149289))
- [custom_ops][perf] Move expensive pytree traversals of tensors to C++ ([#148555](https://github.com/pytorch/pytorch/pull/148555))
- [codemod] Fix clang-tidy command line doc comments ([#149524](https://github.com/pytorch/pytorch/pull/149524))
- [MTIA] Ensure correct stream behavior for input_buffer add autograd on MTIA ([#149433](https://github.com/pytorch/pytorch/pull/149433))
- add grad_output shape check for adaptive_avg_pool2d_backward ([#145241](https://github.com/pytorch/pytorch/pull/145241))
- [aotd] Guess tangents stride as output strides ([#144579](https://github.com/pytorch/pytorch/pull/144579))
- [BE] simplify test_cpp_extensions_aot and .gitignore ([#149231](https://github.com/pytorch/pytorch/pull/149231))
- [cond] don't trace fw and bw graph in autograd key ([#148930](https://github.com/pytorch/pytorch/pull/148930))
- Fix broken dynamo_timed test due to python_version field ([#149659](https://github.com/pytorch/pytorch/pull/149659))
- add python root bin to windows load path. ([#146573](https://github.com/pytorch/pytorch/pull/146573))
- ci: Add sccache to manylinux images ([#148419](https://github.com/pytorch/pytorch/pull/148419))
- [aot] always lower the backward with a deepcopy ([#149229](https://github.com/pytorch/pytorch/pull/149229))
- Fix clang-tidy errors ([#149581](https://github.com/pytorch/pytorch/pull/149581))
- [aoti] follow up to use new api in `test_provenance_tracing.py` ([#149387](https://github.com/pytorch/pytorch/pull/149387))
- Add elu as core ATen ([#149684](https://github.com/pytorch/pytorch/pull/149684))
- [BE][Ez]: Update CU126 to CUDNN 12.8 too ([#149254](https://github.com/pytorch/pytorch/pull/149254))
- Skip test if torchvision is not available ([#149494](https://github.com/pytorch/pytorch/pull/149494))
- [BE] format `test/inductor/s429861_repro.py` ([#148554](https://github.com/pytorch/pytorch/pull/148554))
- [cuBLAS][cuBLASLt] Unify `cuBLASLt` workspaces with `cuBLAS` workspaces ([#145130](https://github.com/pytorch/pytorch/pull/145130))
- elif is not a cmake keyword ([#149655](https://github.com/pytorch/pytorch/pull/149655))
- Update slow tests ([#149844](https://github.com/pytorch/pytorch/pull/149844))
- A bunch of typos ([#149404](https://github.com/pytorch/pytorch/pull/149404))
- [cond] don't trace fw and bw graph in autograd key ([#148930](https://github.com/pytorch/pytorch/pull/148930))
- [ca] use torch.compile ca API for benchmarks ([#149647](https://github.com/pytorch/pytorch/pull/149647))
- avoid allocation when tensor_new from storage ([#149797](https://github.com/pytorch/pytorch/pull/149797))
- [AOTAutogradCache] Allow Custom Autograd functions behind a flag ([#149751](https://github.com/pytorch/pytorch/pull/149751))
- [sigmoid] Support _operator.neg/truediv ([#149754](https://github.com/pytorch/pytorch/pull/149754))
- Rename README.txt to README.md ([#149811](https://github.com/pytorch/pytorch/pull/149811))
- Add regression tests for 3 missing PR-time benchmarks ([#149423](https://github.com/pytorch/pytorch/pull/149423))
- add bobren and laithsakka as ds owners ([#149873](https://github.com/pytorch/pytorch/pull/149873))
- [sigmoid] Fix scalar resolution for Scalar_mode aten ops. ([#149755](https://github.com/pytorch/pytorch/pull/149755))
- update torch.nn.RelicationPad{1,2,3}d deternimistic documentation ([#148633](https://github.com/pytorch/pytorch/pull/148633))
- [BE] Replace XPU support packages installation to offline mode in Linux CI/CD ([#149843](https://github.com/pytorch/pytorch/pull/149843))
- [BE] Add Mac ARM64 actionlint binary ([#149917](https://github.com/pytorch/pytorch/pull/149917))
- cpp_wrapper: Fix even more tests ([#147225](https://github.com/pytorch/pytorch/pull/147225))
- cpp_wrapper: precompile a few more commonly used headers, and improve RAIIPyObject interface ([#149350](https://github.com/pytorch/pytorch/pull/149350))
- Improve subproc autotuning implementation ([#149700](https://github.com/pytorch/pytorch/pull/149700))
- [AOTInductor] Bug fix for freeing buffers when freeing multiple times ([#149810](https://github.com/pytorch/pytorch/pull/149810))
- test/test_cuda.py: rework TEST_PYNVML logic to make more sense, add not IS_JETSON condition ([#149578](https://github.com/pytorch/pytorch/pull/149578))
- add some extra test oom skips for jetson due to lacking nvml support ([#149587](https://github.com/pytorch/pytorch/pull/149587))
- refresh results of benchmarks ([#149936](https://github.com/pytorch/pytorch/pull/149936))
- [AOTInductor] Refine error message for dlopen in AOTInductor ([#149812](https://github.com/pytorch/pytorch/pull/149812))
- [Submodule] [cpuinfo]  cpuinfo update ([#149305](https://github.com/pytorch/pytorch/pull/149305))
- [cuDNN][SDPA] cuDNN SDPA supports `head_dim <= 256` on `sm90` and `sm100` as of `9.5.1+` ([#149904](https://github.com/pytorch/pytorch/pull/149904))
- [ROCm] fix uninitialized warning in BFloat16.h ([#149868](https://github.com/pytorch/pytorch/pull/149868))
- [inductor] Fix mm logging for `torch._scaled_.mm` ([#149967](https://github.com/pytorch/pytorch/pull/149967))
- add loop mm benchmark ([#149932](https://github.com/pytorch/pytorch/pull/149932))
- Add inductor test for torchbind symint ([#149980](https://github.com/pytorch/pytorch/pull/149980))
- gloo: update to latest version ([#149985](https://github.com/pytorch/pytorch/pull/149985))
- [ROCm] Change LoadHIP to use find_file for rocm_version.h ([#149983](https://github.com/pytorch/pytorch/pull/149983))
- [targets2buck] Remove tombstone messages proactively ([#147897](https://github.com/pytorch/pytorch/pull/147897))
- [MPSInductor] Run chebyshev_polynomial_t tests ([#150042](https://github.com/pytorch/pytorch/pull/150042))
- Refactor row-wise scaled MM ([#149978](https://github.com/pytorch/pytorch/pull/149978))
- [invoke_subgraph] Fake tensor prop caching ([#149087](https://github.com/pytorch/pytorch/pull/149087))
- [invoke_subgraph][fake tensor cache] Add a finalizer for id hashed objects ([#149667](https://github.com/pytorch/pytorch/pull/149667))
- [easy] Use config patch to toggle capture_scalar_output ([#150036](https://github.com/pytorch/pytorch/pull/150036))
- [BE] do not retain/release tensor ([#150075](https://github.com/pytorch/pytorch/pull/150075))
- [AOTInductor] Free folded constants that's managed by AOTInductor ([#149825](https://github.com/pytorch/pytorch/pull/149825))
- added fake tensor support for foreach_copy ([#149127](https://github.com/pytorch/pytorch/pull/149127))
- Make codegen dynamic shapes more device agnostic ([#146830](https://github.com/pytorch/pytorch/pull/146830))
- [triton] Warp specialization support in torchinductor ([#148503](https://github.com/pytorch/pytorch/pull/148503))
- Store statically launchable CachingAutotuners inside CompiledFXGraph.triton_bundle ([#149054](https://github.com/pytorch/pytorch/pull/149054))
- wire torch._scaled_mm with fp4 operands to the cublas nvfp4 kernel ([#148792](https://github.com/pytorch/pytorch/pull/148792))
- cpp_wrapper: Fix even more tests ([#147225](https://github.com/pytorch/pytorch/pull/147225))
- [BE] Suppress user_warnings while running opinfo tests ([#150115](https://github.com/pytorch/pytorch/pull/150115))
- cpp_wrapper: precompile a few more commonly used headers, and improve RAIIPyObject interface ([#149350](https://github.com/pytorch/pytorch/pull/149350))
- Improve subproc autotuning implementation ([#149700](https://github.com/pytorch/pytorch/pull/149700))
- Do not fetch NCCL when system NCCL is used ([#149607](https://github.com/pytorch/pytorch/pull/149607))
- Store statically launchable CachingAutotuners inside CompiledFXGraph.triton_bundle ([#149054](https://github.com/pytorch/pytorch/pull/149054))
- [aotd] Config to guess_tangents_stride ([#150035](https://github.com/pytorch/pytorch/pull/150035))
- Fix test failures on non-x86 Linux ([#148445](https://github.com/pytorch/pytorch/pull/148445))
- [TD] Enable TD on distributed cpu ([#150028](https://github.com/pytorch/pytorch/pull/150028))
- [CI] Fix docker builds failing due to cmake update by setting CMAKE_POLICY_VERSION_MINIMUM ([#150220](https://github.com/pytorch/pytorch/pull/150220))
- Mention the cherry-picker bot in the release docs ([#150210](https://github.com/pytorch/pytorch/pull/150210))
- Update referenced PRs for ecosystem library branch cut ([#150211](https://github.com/pytorch/pytorch/pull/150211))
- Update reference for binary_build workflows ([#150213](https://github.com/pytorch/pytorch/pull/150213))
- Explicitly state that a test-infra branch cut is required ([#150214](https://github.com/pytorch/pytorch/pull/150214))
- Move MacOS inductor tests to M2-15 runner ([#150228](https://github.com/pytorch/pytorch/pull/150228))
- if blaslt fails, fall back to blas ([#150147](https://github.com/pytorch/pytorch/pull/150147))
- Move MacOS inductor tests to M2-15 runner ([#150228](https://github.com/pytorch/pytorch/pull/150228))
- Super tiny fix typo ([#149190](https://github.com/pytorch/pytorch/pull/149190))
- Add .editorconfig ([#149193](https://github.com/pytorch/pytorch/pull/149193))
- [cuDNN][SDPA] abide by `enable_gqa` convention in cuDNN ([#149976](https://github.com/pytorch/pytorch/pull/149976))
- Remove unnecessary "special linking" for `BLAS_LIBRARIES` ([#145487](https://github.com/pytorch/pytorch/pull/145487))
- [Profiler] Give non-zero default values to start events ([#149757](https://github.com/pytorch/pytorch/pull/149757))
- Enable TMA persistent GEMM Template by default ([#149427](https://github.com/pytorch/pytorch/pull/149427))
- Update type of `create_block_mask` to more accurately reflect things ([#150244](https://github.com/pytorch/pytorch/pull/150244))
- Store statically launchable CachingAutotuners inside CompiledFXGraph.triton_bundle ([#149054](https://github.com/pytorch/pytorch/pull/149054))
- Update docstring to match code. ([#148455](https://github.com/pytorch/pytorch/pull/148455))
- [PrivateUse1] Impl `isBuilt()` and `isAvailable()` ([#149594](https://github.com/pytorch/pytorch/pull/149594))
- Fix documentation build errors caused by unsupported section titles ([#150205](https://github.com/pytorch/pytorch/pull/150205))
- test 0-dim squeeze in basic.TestSqueeze ([#147928](https://github.com/pytorch/pytorch/pull/147928))
- Fix mtia_extension.cpp setDevice() to correctly set current_device ([#149398](https://github.com/pytorch/pytorch/pull/149398))
- [ROCm] use correct workspace for hipblaslt, silence warning ([#150227](https://github.com/pytorch/pytorch/pull/150227))
- Update torch-xpu-ops commit pin to 3ee2bd2 ([#150300](https://github.com/pytorch/pytorch/pull/150300))
- Make PyTorch buildable by CMake-4.x on s390x ([#150294](https://github.com/pytorch/pytorch/pull/150294))
- Adjust TestInductorOpInfo to depend on backend, not device  ([#146911](https://github.com/pytorch/pytorch/pull/146911))
- Update gloo submodule ([#150320](https://github.com/pytorch/pytorch/pull/150320))
- Merge Triton ScaledMM as epilogue to MM template ([#150045](https://github.com/pytorch/pytorch/pull/150045))
- [cachinghostallocator] remove the check on cudaHostRegister path ([#150070](https://github.com/pytorch/pytorch/pull/150070))
- [AOTInductor] Free tensors in test ([#150274](https://github.com/pytorch/pytorch/pull/150274))
- [ROCm] Enable more inductor UTs ([#149513](https://github.com/pytorch/pytorch/pull/149513))
- [CI] Skip test_copy_large_tensor on M2-15 runners ([#150377](https://github.com/pytorch/pytorch/pull/150377))
- [fr] Added protection against missing stack frames in fr cont. ([#150133](https://github.com/pytorch/pytorch/pull/150133))
- [pytree] add APIs to determine a class is a namedtuple or PyStructSequence ([#113257](https://github.com/pytorch/pytorch/pull/113257))
- Enable SVE ACLE implementation for tanH Aten op for FP32 dType. ([#143741](https://github.com/pytorch/pytorch/pull/143741))
- [AOTI] Skip test_buffer_mutation_and_force_mmap_weights for fbcode ([#150340](https://github.com/pytorch/pytorch/pull/150340))
- Set proper `LD_LIBRARY_PATH` on Linux in nightly venv in nightly pull tool ([#143262](https://github.com/pytorch/pytorch/pull/143262))
- [custom_ops][perf] Move expensive pytree traversals of tensors to C++ ([#148555](https://github.com/pytorch/pytorch/pull/148555))
- [ROCm][Windows] Fix torchvision build with ROCm 6.4 on windows ([#150180](https://github.com/pytorch/pytorch/pull/150180))
- [invoke_subgraph] Support None in the fwd output ([#150082](https://github.com/pytorch/pytorch/pull/150082))
- [invoke_subgraph] Do not cache fake tensors for AOTDispatcher first pass ([#150450](https://github.com/pytorch/pytorch/pull/150450))
- [AOTInductor] Modify test for Memory tracking for memory-related ([#150269](https://github.com/pytorch/pytorch/pull/150269))
- Compare device name of profiler dynamically ([#150396](https://github.com/pytorch/pytorch/pull/150396))
- cpp_wrapper: precompile a few more commonly used headers, and improve RAIIPyObject interface ([#149350](https://github.com/pytorch/pytorch/pull/149350))
- [invoke_subgraph] Filter out grad_out where fw_out requires_grad is False ([#150486](https://github.com/pytorch/pytorch/pull/150486))
- expect fail scan test in sigmoid ([#150475](https://github.com/pytorch/pytorch/pull/150475))
- [invoke_subgraph][min-cut partitioner] Fix bug to use the correct root module ([#150556](https://github.com/pytorch/pytorch/pull/150556))
- Update torch-xpu-ops commit pin to 98c808d ([#150554](https://github.com/pytorch/pytorch/pull/150554))
- [invoke_subgraph] Force grad_outs to be contiguous at tracing time ([#150561](https://github.com/pytorch/pytorch/pull/150561))
- [FlexAttention] Remove dead code ([#150575](https://github.com/pytorch/pytorch/pull/150575))
- Ensure cuda_dlink_post_cflags are quoted as well ([#150151](https://github.com/pytorch/pytorch/pull/150151))
- [ROCm] code cleanup of architecture checks ([#150473](https://github.com/pytorch/pytorch/pull/150473))
- ROCm: Add trailing comma for consistency in gfx architecture list ([#150250](https://github.com/pytorch/pytorch/pull/150250))
- [AOTI][dashboard] Update how peak memory is measured ([#150534](https://github.com/pytorch/pytorch/pull/150534))
- [fbcode]Removing `@NoIntBaseDeprecated` annotation in `evaluation.thrift` file ([#150271](https://github.com/pytorch/pytorch/pull/150271))
- add unit test for preferred_blas_library settings ([#150581](https://github.com/pytorch/pytorch/pull/150581))
- Add new dependences for gen_pyi.py ([#150391](https://github.com/pytorch/pytorch/pull/150391))
- if blaslt fails, fall back to blas ([#150147](https://github.com/pytorch/pytorch/pull/150147))
- CUDA CachingHostAllocator tracks registrations to call correct free ([#146520](https://github.com/pytorch/pytorch/pull/146520))
- Update commitlist.py instructions for the GitHub repo regime ([#149535](https://github.com/pytorch/pytorch/pull/149535))
- bound sympy accuracy ([#150383](https://github.com/pytorch/pytorch/pull/150383))
- clang-format aten/src/ATen/cpu/vec/*.h ([#150426](https://github.com/pytorch/pytorch/pull/150426))
- Adapt test_misc.py for HPUs ([#149499](https://github.com/pytorch/pytorch/pull/149499))
- [aoti] Split ConstantType definition out of model.h ([#150545](https://github.com/pytorch/pytorch/pull/150545))
- Expose symbols on macos in the xplat pytorch stack ([#150487](https://github.com/pytorch/pytorch/pull/150487))
- Fix conv2d strided prologue ([#150697](https://github.com/pytorch/pytorch/pull/150697))
- Refresh expected results. ([#150264](https://github.com/pytorch/pytorch/pull/150264))
- [AOTInductor] Introduce MaybeOwningAtenTensorHandle for ConstantMap ([#150275](https://github.com/pytorch/pytorch/pull/150275))
- add unit test for preferred_blas_library settings ([#150581](https://github.com/pytorch/pytorch/pull/150581))
- [AOTI][dashboard] Fix mis-calculated memory compression ratio ([#150695](https://github.com/pytorch/pytorch/pull/150695))
- [cuDNN][SDPA] Loosen constraints for GQA for cuDNN Attention ([#150337](https://github.com/pytorch/pytorch/pull/150337))
- Add type hints to `_tensor_docs.add_docstr_all` ([#150715](https://github.com/pytorch/pytorch/pull/150715))
- Suppress `-Wunused-function` for DSA ([#150735](https://github.com/pytorch/pytorch/pull/150735))
- Generalize poison fork logic for each device backend ([#144664](https://github.com/pytorch/pytorch/pull/144664))
- Fix conv2d strided prologue ([#150697](https://github.com/pytorch/pytorch/pull/150697))
- [precompile] Serialization for GlobalStateGuard ([#150636](https://github.com/pytorch/pytorch/pull/150636))
- Add check in `test_cow_input` to ensure COW data is never changed ([#150723](https://github.com/pytorch/pytorch/pull/150723))
- Update slow tests ([#150283](https://github.com/pytorch/pytorch/pull/150283))
- AOTI fallback ops: sort alphabetically ([#150672](https://github.com/pytorch/pytorch/pull/150672))
- cpp_wrapper: Fix even more tests ([#147225](https://github.com/pytorch/pytorch/pull/147225))
- [ROCm] Expand workspace size for gfx95 ([#150632](https://github.com/pytorch/pytorch/pull/150632))
- [BE] Fix Amp.metal compilation warning ([#150783](https://github.com/pytorch/pytorch/pull/150783))
- [invoke_subgraph] Lazy backward ([#150666](https://github.com/pytorch/pytorch/pull/150666))
- [ROCm] Build Pytorch extensions with amdclang++ ([#150451](https://github.com/pytorch/pytorch/pull/150451))
- README: anaconda license violation / no longer recommend anaconda since it's no longer free to use ([#150619](https://github.com/pytorch/pytorch/pull/150619))
- [Inductor XPU] Refine `test_mkldnn_pattern_matcher.py` to be reusable for XPU. ([#150286](https://github.com/pytorch/pytorch/pull/150286))
- [XPU] Fix XPU unit test on Windows ([#150520](https://github.com/pytorch/pytorch/pull/150520))
- Update CPython tests for ctx manager to use unittest ([#146501](https://github.com/pytorch/pytorch/pull/146501))
- Remove redundant code in cuda/__init__.py ([#150529](https://github.com/pytorch/pytorch/pull/150529))
- Remove torch functions that do not support device arguments from _device_constructor ([#150290](https://github.com/pytorch/pytorch/pull/150290))
- [Intel GPU] int4 WOQ gemm XPU Support ([#137566](https://github.com/pytorch/pytorch/pull/137566))
- [Accelerator][Chore] Use existing `acc` when raising an error ([#150829](https://github.com/pytorch/pytorch/pull/150829))
- [ROCm][Windows] Include AOTriton dependent sources in Windows build ([#150521](https://github.com/pytorch/pytorch/pull/150521))
- [invoke_subgraph] Preserve node meta ([#150782](https://github.com/pytorch/pytorch/pull/150782))
- Code Clean: Remove python3.8 specific code because PyTorch now need Python3.9 and later ([#150834](https://github.com/pytorch/pytorch/pull/150834))
- [docs] remove --recursive flag from readme ([#150785](https://github.com/pytorch/pytorch/pull/150785))
- Fix _del_library ([#150495](https://github.com/pytorch/pytorch/pull/150495))
- Added _fused_sdp_choice_stub dispatcher support for HPU device ([#149512](https://github.com/pytorch/pytorch/pull/149512))
- [c10d][fr] Refactor analysis script for modularization and reusing for coalesce collectives ([#150881](https://github.com/pytorch/pytorch/pull/150881))
- [aoti] Use generate_fake_kernels_from_real_mismatches config for draft exported programs ([#150651](https://github.com/pytorch/pytorch/pull/150651))
- update benchamark result due to <1% regression ([#150937](https://github.com/pytorch/pytorch/pull/150937))
- [AOTI] Remove typedef for half and bfloat16 ([#150657](https://github.com/pytorch/pytorch/pull/150657))
- Add dynamic version for mm_loop benchmark ([#150865](https://github.com/pytorch/pytorch/pull/150865))
- [5/N] Remove unnecessary once flag usage  ([#147445](https://github.com/pytorch/pytorch/pull/147445))
- Remove unneeded CUDA logic from _create_build_env ([#145822](https://github.com/pytorch/pytorch/pull/145822))
- Generalize poison fork logic for each device backend ([#144664](https://github.com/pytorch/pytorch/pull/144664))
- Docs: Add missing whitespace in the cmake warning message ([#150929](https://github.com/pytorch/pytorch/pull/150929))
- Expose is_available API for torch.backends.mkldnn ([#147432](https://github.com/pytorch/pytorch/pull/147432))
- [pytorch] Remove numpy dependency from Knapsack Evaluator ([#150825](https://github.com/pytorch/pytorch/pull/150825))
- [inductor] Change minimum number of SMs to 60 to let Ada use Triton GEMM backend ([#150888](https://github.com/pytorch/pytorch/pull/150888))
- Support tuning of _scaled_grouped_mm ([#150421](https://github.com/pytorch/pytorch/pull/150421))
- [inductor] Change minimum number of SMs to 60 to let Ada use Triton GEMM backend ([#150888](https://github.com/pytorch/pytorch/pull/150888))
- [Inductor UT][Break XPU] Apply CUDA tolerances changes on XPU that introduced by #144579. ([#149862](https://github.com/pytorch/pytorch/pull/149862))
- [Inductor UT][Break XPU] Fix UTs for XPU broken by community. ([#150830](https://github.com/pytorch/pytorch/pull/150830))
- [aotinductor] fix std::{min.max} compilation error for sympy expr with multiple args ([#150894](https://github.com/pytorch/pytorch/pull/150894))
- Add mempool to allocator's trace events ([#150683](https://github.com/pytorch/pytorch/pull/150683))
- Cache the value of torch_key in subproc ([#151057](https://github.com/pytorch/pytorch/pull/151057))
- Allow OpaqueTensorImpl to be used for views ([#151028](https://github.com/pytorch/pytorch/pull/151028))
- Update ninja missing error message ([#147698](https://github.com/pytorch/pytorch/pull/147698))
- Support tuning of _scaled_grouped_mm ([#150421](https://github.com/pytorch/pytorch/pull/150421))
- [CUDA][TF32] Account for TF32 in `test_alexnet_prefix` ([#150970](https://github.com/pytorch/pytorch/pull/150970))
- [MPSInductor] Naive welford_reduce implementation ([#150824](https://github.com/pytorch/pytorch/pull/150824))
- [map] always turn on dynamo for map ([#150962](https://github.com/pytorch/pytorch/pull/150962))
- [map] make proxy mode re-dispatch to fake key ([#151034](https://github.com/pytorch/pytorch/pull/151034))
- [MPSInductor] Naive welford_reduce implementation ([#150824](https://github.com/pytorch/pytorch/pull/150824))
- [Inductor] Refactor wrapper codegen to use Wrapper IR. ([#150458](https://github.com/pytorch/pytorch/pull/150458))
- [CI][CUDA] xfail grouped gemm unit tests on blackwell ([#150982](https://github.com/pytorch/pytorch/pull/150982))
- Add some autograd producer consumer stream sync tests ([#150952](https://github.com/pytorch/pytorch/pull/150952))
- [MPSInductor] Naive welford_reduce implementation ([#150824](https://github.com/pytorch/pytorch/pull/150824))
- Make export._trace._WrapperModule work in strict mode ([#146919](https://github.com/pytorch/pytorch/pull/146919))
- [OpenReg][PrivateUse1] add device context for OpenReg Module ([#150997](https://github.com/pytorch/pytorch/pull/150997))
- [Openreg][PrivateUse1] Improve openreg module capabilities ([#151000](https://github.com/pytorch/pytorch/pull/151000))
- [Openreg][PrivateUse1] Refactor csrc files of Pytorch_openreg ([#151004](https://github.com/pytorch/pytorch/pull/151004))
- [AMD] Block mem efficient attention for FP32 in CK backend ([#151132](https://github.com/pytorch/pytorch/pull/151132))
- [easy] Add cache bypass traceback information to cache_info on autograd_cache_bypass ([#151025](https://github.com/pytorch/pytorch/pull/151025))
- [Testing] Skip `test_unspec_inputs_float64_mps` ([#151167](https://github.com/pytorch/pytorch/pull/151167))
- [Intel GPU] skip a cuda api call in amp to save some host overhead on xpu ([#151111](https://github.com/pytorch/pytorch/pull/151111))
- Generalize poison fork logic for each device backend ([#144664](https://github.com/pytorch/pytorch/pull/144664))
- Docs: Fix typos in the Symbolic Numbers docstrings ([#151181](https://github.com/pytorch/pytorch/pull/151181))
- [HPU] Add HPU as a supported device for NestedTensor ([#148659](https://github.com/pytorch/pytorch/pull/148659))
- [Attention] Always pad in preprocess_mask to avoid recompilations ([#150403](https://github.com/pytorch/pytorch/pull/150403))
- [ez] move GuardsContext code comment to the right place ([#150755](https://github.com/pytorch/pytorch/pull/150755))
- Super tiny fix typo ([#151212](https://github.com/pytorch/pytorch/pull/151212))
- [scan] Autograd with partial gradient support ([#146285](https://github.com/pytorch/pytorch/pull/146285))
- Fix assert_tensor_meta ([#150808](https://github.com/pytorch/pytorch/pull/150808))
- [ROCm] Improve behavior of get_torch_rocm_version helper function on non-ROCm systems. ([#151040](https://github.com/pytorch/pytorch/pull/151040))
- [cutlass backend] Add more logs for cutlass backend benchmark ([#150639](https://github.com/pytorch/pytorch/pull/150639))
- make einsum unbacked friendly ([#151032](https://github.com/pytorch/pytorch/pull/151032))
- [Intel GPU] Avoid using fp32 in sdp math path when benchmark performance. ([#150996](https://github.com/pytorch/pytorch/pull/150996))
- [Intel GPU][PT2E] Register qconv impls to general qconv_pointwise schema ([#151092](https://github.com/pytorch/pytorch/pull/151092))
- [AOTInductor] Add Python interface for user managed buffer. ([#151141](https://github.com/pytorch/pytorch/pull/151141))
- [fbgemm_gpu] Incorporate Torch DSA ([#151148](https://github.com/pytorch/pytorch/pull/151148))
- [Inductor UT] Refactor FlexAttention UT and add CPU tests ([#144953](https://github.com/pytorch/pytorch/pull/144953))
- don't return logits for benchmark script ([#151075](https://github.com/pytorch/pytorch/pull/151075))
- [Inductor] Refactor wrapper codegen to use Wrapper IR. ([#150458](https://github.com/pytorch/pytorch/pull/150458))
- update expected results for comptime benchmark ([#151319](https://github.com/pytorch/pytorch/pull/151319))
- [HOP] Reworked DispatchKey.Autograd ([#151107](https://github.com/pytorch/pytorch/pull/151107))
- Warn user of existing lock file to avoid infinite waiting ([#149382](https://github.com/pytorch/pytorch/pull/149382))
- [BE] Fix extra-semi warning in attention.cpp ([#151367](https://github.com/pytorch/pytorch/pull/151367))
- [1/N] Use std::string_view in torchgen   ([#146403](https://github.com/pytorch/pytorch/pull/146403))
- Disable -Werror for s390x test module compilation ([#150413](https://github.com/pytorch/pytorch/pull/150413))
- [CI][NoOp] Update skip reason for argmin_with_nan ([#151374](https://github.com/pytorch/pytorch/pull/151374))
- fix test_einsum: use initialized values ([#151363](https://github.com/pytorch/pytorch/pull/151363))
- [custom ops]  Fix destroy function ([#151299](https://github.com/pytorch/pytorch/pull/151299))
- [OpenReg][PrivateUse1] Refactoring the csrc files of pytorch_openreg ([#151005](https://github.com/pytorch/pytorch/pull/151005))
- [Openreg][PrivateUse1] Enable CI for openreg ([#151007](https://github.com/pytorch/pytorch/pull/151007))
- [Openreg][PrivateUse1] Fix releasing tensor issue when using pin_memory ([#151091](https://github.com/pytorch/pytorch/pull/151091))
- Add ccode for CeilToInt and IntTrueDiv ([#151375](https://github.com/pytorch/pytorch/pull/151375))
- [cuSPARSE][B200] Bump tolerances for test_sparse_csr matvec ([#148721](https://github.com/pytorch/pytorch/pull/148721))
- Allow to run flex_attention on HPU ([#148656](https://github.com/pytorch/pytorch/pull/148656))
- [compile][compile time traces] Add more dynamo traces ([#151357](https://github.com/pytorch/pytorch/pull/151357))
- [Reopen] [Intel GPU] Set higher tolerance for some models only on XPU Device ([#144756](https://github.com/pytorch/pytorch/pull/144756))
- [Easy] Fix the compilation warning of BlasKernel. ([#151302](https://github.com/pytorch/pytorch/pull/151302))
- [Easy] Add more check for elapsedTime of torch.xxx.Event and torch.Event ([#151404](https://github.com/pytorch/pytorch/pull/151404))
- [BE] Remove outdated script to check namespace BC ([#151453](https://github.com/pytorch/pytorch/pull/151453))
- [AOTInductor] Add states for constant folding process ([#151273](https://github.com/pytorch/pytorch/pull/151273))
- [MegaCache] Encode key in base64 ([#151472](https://github.com/pytorch/pytorch/pull/151472))
- [BE][Easy]: Dedupe a TypeAlias in PrimsCommon ([#151565](https://github.com/pytorch/pytorch/pull/151565))
- [c10] helpers for runtime c10::alias re-use ([#151361](https://github.com/pytorch/pytorch/pull/151361))
- [fake tensor cache] Support index with non bool/int8 indices ([#151477](https://github.com/pytorch/pytorch/pull/151477))
- Update PyTorchStreamReader API to take cpu allocator override ([#150439](https://github.com/pytorch/pytorch/pull/150439))
- refresh benchmark results ([#151622](https://github.com/pytorch/pytorch/pull/151622))
- [Openreg][PrivateUse1] Enable CI for openreg ([#151007](https://github.com/pytorch/pytorch/pull/151007))
- [Openreg][PrivateUse1] Fix releasing tensor issue when using pin_memory ([#151091](https://github.com/pytorch/pytorch/pull/151091))
- [ez] fix code owners typo ([#151499](https://github.com/pytorch/pytorch/pull/151499))
- [XPU] skip a subprocess UT for Windows ([#150999](https://github.com/pytorch/pytorch/pull/150999))
- Update torch-xpu-ops commit pin ([#150827](https://github.com/pytorch/pytorch/pull/150827))
- [Easy] Add more check for elapsedTime of torch.xxx.Event and torch.Event ([#151404](https://github.com/pytorch/pytorch/pull/151404))
- [inductor][test] Skip triton tests for MPS as well, also change reason for skipping SM89 to not IS_BIG_GPU ([#151506](https://github.com/pytorch/pytorch/pull/151506))
- stage 2 of depreate silent fallback of tuning gemm ([#148622](https://github.com/pytorch/pytorch/pull/148622))
- [easy] Update test/dynamo/test_utils.py ([#151599](https://github.com/pytorch/pytorch/pull/151599))
- [AMD] Remove fbcode limit for uuid ([#151652](https://github.com/pytorch/pytorch/pull/151652))
- [c10d][fr] Fix a bug when first rank is not zero in the script ([#151683](https://github.com/pytorch/pytorch/pull/151683))
- [invoke_subgraph][fake tensor] Add finalizer on subgraph instead of the functionalize ctx wrapper ([#151633](https://github.com/pytorch/pytorch/pull/151633))
- [easy] Update test/dynamo/test_structured_trace.py ([#151606](https://github.com/pytorch/pytorch/pull/151606))
- Overload Library::def rather than templating it ([#151626](https://github.com/pytorch/pytorch/pull/151626))
- Reserve vectors in FunctionSchema::cloneWithRealTypes ([#151627](https://github.com/pytorch/pytorch/pull/151627))
- Use fmt::format for debug strings in Library init ([#151629](https://github.com/pytorch/pytorch/pull/151629))
- [Testing] Make test_add_complex3 run on different devices ([#151732](https://github.com/pytorch/pytorch/pull/151732))
- [Easy] The event_id of torch.cuda.Event and torch.xpu.Event always is 0 ([#151226](https://github.com/pytorch/pytorch/pull/151226))
- [Easy] Fix the function signature of torch.Event ([#151221](https://github.com/pytorch/pytorch/pull/151221))
- [Testing] Make test_add_complex3 run on different devices ([#151732](https://github.com/pytorch/pytorch/pull/151732))
- Use more efficient row/col computation ([#151474](https://github.com/pytorch/pytorch/pull/151474))
- consolidate ATen/test/dispatch_key_set_test.cpp with rest of DispatchKeySet tests ([#151697](https://github.com/pytorch/pytorch/pull/151697))
- [Easy] Fix the compilation warning of BlasKernel. ([#151736](https://github.com/pytorch/pytorch/pull/151736))
- [Testing] Unskip expm1 log1p for MPS ([#151790](https://github.com/pytorch/pytorch/pull/151790))
- Run standalone compile tests on cpu/gpu ([#151768](https://github.com/pytorch/pytorch/pull/151768))
- [cutlass] Define GELU_taylor<float> only if CUTLASS version is <= 380 ([#151702](https://github.com/pytorch/pytorch/pull/151702))
- [torch] Expose PCI info from CUDA device ([#151672](https://github.com/pytorch/pytorch/pull/151672))
- [inductor][test] Skip triton tests for MPS as well, also change reason for skipping SM89 to not IS_BIG_GPU ([#151506](https://github.com/pytorch/pytorch/pull/151506))
- stage 2 of depreate silent fallback of tuning gemm ([#148622](https://github.com/pytorch/pytorch/pull/148622))
- [BE]: Better cleanup optimized code from #151474 ([#151794](https://github.com/pytorch/pytorch/pull/151794))
- FlexAttention add decorator for large test cases ([#151459](https://github.com/pytorch/pytorch/pull/151459))
- [Inductor] Add Additional Configs for persistent+TMA version of Triton mm and addmm ([#150587](https://github.com/pytorch/pytorch/pull/150587))
- [c10] add #pragma once to leftright ([#151710](https://github.com/pytorch/pytorch/pull/151710))
- [easy] Fix test_dynamo_timed ([#151816](https://github.com/pytorch/pytorch/pull/151816))
- Deprecate host allocator legacy APIs ([#151437](https://github.com/pytorch/pytorch/pull/151437))
- Non-deterministic alert in histc_cuda for floating types only ([#151701](https://github.com/pytorch/pytorch/pull/151701))
- Add OIDC permissions to bazel workflow ([#151456](https://github.com/pytorch/pytorch/pull/151456))
- [EZ/Profiler] Update Submodule ([#151843](https://github.com/pytorch/pytorch/pull/151843))
- Fix doc requirements install error ([#151787](https://github.com/pytorch/pytorch/pull/151787))
- Cache the value of torch_key in subproc ([#151057](https://github.com/pytorch/pytorch/pull/151057))
- Use more efficient mask to index computation ([#151372](https://github.com/pytorch/pytorch/pull/151372))
- [cutlass backend] delay construction of cutlass presets to when called ([#151875](https://github.com/pytorch/pytorch/pull/151875))
- [associative_scan] Fixes for assoc_scan testcases ([#149988](https://github.com/pytorch/pytorch/pull/149988))
- [ROCm] opportunistic fastatomics for ReduceAdd operations for MI300 GPUs ([#146264](https://github.com/pytorch/pytorch/pull/146264))
- [Testing] Enable `test_mutations_loop_fusion_mps` ([#151872](https://github.com/pytorch/pytorch/pull/151872))
- [autodeps2] Replace third-party/pyyaml with third-party/pypi/pyyaml ([#151668](https://github.com/pytorch/pytorch/pull/151668))
- Speed up OperatorEntry construction by avoiding updateDispatchTableFull_ ([#151682](https://github.com/pytorch/pytorch/pull/151682))
- [MegaCache] Return None on no compilation ([#151921](https://github.com/pytorch/pytorch/pull/151921))
- [FlexAttention] Fix device test instantation ([#151846](https://github.com/pytorch/pytorch/pull/151846))
- [BE] Replace `std::runtime_error` with `TORCH_CHECK` [1/N] ([#151880](https://github.com/pytorch/pytorch/pull/151880))
- [Easy] Remove redundant code ([#151883](https://github.com/pytorch/pytorch/pull/151883))
- [CUDA][cuBLAS][cuBLASLt] Opt-in unified cuBLAS + cuBLASLt workspaces ([#151163](https://github.com/pytorch/pytorch/pull/151163))
- Turn on static cuda launcher in OSS ([#151691](https://github.com/pytorch/pytorch/pull/151691))
- Update description for torch.random.fork_rng ([#151881](https://github.com/pytorch/pytorch/pull/151881))
- [MPS] Fix test_neg_index_mps ([#151966](https://github.com/pytorch/pytorch/pull/151966))
- Make `torch.jit.Error` inherit from Exception ([#151947](https://github.com/pytorch/pytorch/pull/151947))
- [torchbind] fix error message when attr is a real tensor. ([#151944](https://github.com/pytorch/pytorch/pull/151944))
- [Inductor] Add Additional Configs for persistent+TMA version of Triton mm and addmm ([#150587](https://github.com/pytorch/pytorch/pull/150587))
- move find_hop_schema into _higher_order_ops/schema.py ([#151147](https://github.com/pytorch/pytorch/pull/151147))
- [map] defer importing AOTConfig and create_joint dependency ([#151479](https://github.com/pytorch/pytorch/pull/151479))
- [pytorch] use a mutex in initialize_torch_libraries ([#151938](https://github.com/pytorch/pytorch/pull/151938))
- [CI] Update sleef submodule to v3.8 ([#151955](https://github.com/pytorch/pytorch/pull/151955))
- [FlexAttention] Fix device test instantation ([#151846](https://github.com/pytorch/pytorch/pull/151846))
- Fix typos in meta.rst ([#151979](https://github.com/pytorch/pytorch/pull/151979))
- [Easy] Add more check for elapsedTime of torch.xxx.Event and torch.Event ([#151404](https://github.com/pytorch/pytorch/pull/151404))
- [sigmoid] memory planner C10 deps ([#151275](https://github.com/pytorch/pytorch/pull/151275))
- [MPS] Adjust test_sum_dtypes so it can run on MPS. ([#152064](https://github.com/pytorch/pytorch/pull/152064))
- [MPS] Fix test_neg_index_mps ([#151966](https://github.com/pytorch/pytorch/pull/151966))
- Don't copy DynamicType argument to DynamicType::create ([#151801](https://github.com/pytorch/pytorch/pull/151801))
- Create and use DynamicTypes for check in DispatchKeyExtractor::makeBitsetForDispatchArgs ([#151802](https://github.com/pytorch/pytorch/pull/151802))
- Fix return type of TypeFactoryBase<c10::DynamicType>::get ([#151803](https://github.com/pytorch/pytorch/pull/151803))
- [MPS] Adjust test_sum_dtypes so it can run on MPS. ([#152064](https://github.com/pytorch/pytorch/pull/152064))
- [invoke_subgraph] Compile time traces ([#151409](https://github.com/pytorch/pytorch/pull/151409))
- [invoke_subgraph][fake tensor] Add finalizer on subgraph instead of the functionalize ctx wrapper ([#151633](https://github.com/pytorch/pytorch/pull/151633))
- [fake tensor cache] Support index with non bool/int8 indices ([#151477](https://github.com/pytorch/pytorch/pull/151477))
- [invoke_subgraph] Cache fake tensor if no unbacked symint in the output ([#151957](https://github.com/pytorch/pytorch/pull/151957))
- [fake tensor] Cache None, integer and SymInts in the output ([#151961](https://github.com/pytorch/pytorch/pull/151961))
- [dynamo] Add guard serialization for tensor matches. ([#151318](https://github.com/pytorch/pytorch/pull/151318))
- Update docs dependencies for local build ([#151796](https://github.com/pytorch/pytorch/pull/151796))
- add Out Notes ([#151306](https://github.com/pytorch/pytorch/pull/151306))
- [CUDA][TF32] Account for TF32 in `test_corrcoef` ([#151830](https://github.com/pytorch/pytorch/pull/151830))
- Non-deterministic alert in histc_cuda for floating types only ([#151701](https://github.com/pytorch/pytorch/pull/151701))
- [Inductor] Record Triton’s Base32 Cache Key in .best_config for Debugging ([#148981](https://github.com/pytorch/pytorch/pull/148981))
- [xpu] set aot device flags in cpp_extension ([#149459](https://github.com/pytorch/pytorch/pull/149459))
- [Break XPU] generalize newly introduced device bias code in Inductor UT. ([#151926](https://github.com/pytorch/pytorch/pull/151926))
- Relax tolerance on test_aot_autograd_exhaustive_matmul_cpu_float32 without MKL ([#152106](https://github.com/pytorch/pytorch/pull/152106))
- [dynamo] Add guard serialization for tensor matches. ([#151318](https://github.com/pytorch/pytorch/pull/151318))
- Turn on static cuda launcher in OSS ([#151691](https://github.com/pytorch/pytorch/pull/151691))
- [aarch64] Fixes to build with ArmPL's cblas.h ([#151126](https://github.com/pytorch/pytorch/pull/151126))
- BM FM FlashAttention Test ([#151974](https://github.com/pytorch/pytorch/pull/151974))
- [Easy] Add more check for elapsedTime of torch.xxx.Event and torch.Event ([#151404](https://github.com/pytorch/pytorch/pull/151404))
- [ROCm] Implemented dropout usage for RNN with MIOpen backend ([#144572](https://github.com/pytorch/pytorch/pull/144572))
- [BE] Do not allow PyTorch codebase to use `c10::optional` ([#150464](https://github.com/pytorch/pytorch/pull/150464))
- Add simple direct C++ tests for torch::jit::Lexer ([#151849](https://github.com/pytorch/pytorch/pull/151849))
- [BE] Do not allow PyTorch codebase to use `c10::optional` ([#150464](https://github.com/pytorch/pytorch/pull/150464))
- [BE] Switch `TestConsistency` to MPS device ([#147893](https://github.com/pytorch/pytorch/pull/147893))
- [ROCm][Windows] Fix HIP Caffe2 Tests ([#152014](https://github.com/pytorch/pytorch/pull/152014))
- Fix inductor test_linear_with_in_out_buffer ([#151548](https://github.com/pytorch/pytorch/pull/151548))
- Remove useless options for third-party ONNX build ([#147616](https://github.com/pytorch/pytorch/pull/147616))
- [Easy] Fix the function signature of torch.Event ([#151221](https://github.com/pytorch/pytorch/pull/151221))
- [MPS/inductor] Adjust test_to_dtype_mps so that it works on the backend. ([#152230](https://github.com/pytorch/pytorch/pull/152230))
- [Easy] The event_id of torch.cuda.Event and torch.xpu.Event always is 0 ([#151226](https://github.com/pytorch/pytorch/pull/151226))
- Correctly handle duplicated arguments when merging input views. ([#146275](https://github.com/pytorch/pytorch/pull/146275))
- Update OpenBLAS commit  ([#151547](https://github.com/pytorch/pytorch/pull/151547))
- [Kineto] Enable OOM observer ([#152160](https://github.com/pytorch/pytorch/pull/152160))
- Fix initGdsBindings declaration ([#152277](https://github.com/pytorch/pytorch/pull/152277))
- [Inductor UT] Generalize device-bias code in `test_flex_attention.py` ([#151937](https://github.com/pytorch/pytorch/pull/151937))
- Add check for 2-dim mask to COO mask computation ([#151940](https://github.com/pytorch/pytorch/pull/151940))
- Enable max autotune for AOTInductor benchmark ([#149309](https://github.com/pytorch/pytorch/pull/149309))
- test(Conv3d): use correct class for `test_Conv3d_module_same_padding` ([#152187](https://github.com/pytorch/pytorch/pull/152187))
- Skip test requiring MKL ([#152322](https://github.com/pytorch/pytorch/pull/152322))
- [cutlass backend] add addmm and bmm for cutlass backend benchmark ([#152163](https://github.com/pytorch/pytorch/pull/152163))
- [Security] Advise against loading untrusted TorchScripts ([#152336](https://github.com/pytorch/pytorch/pull/152336))
- Provide list of files to link linters if desired ([#152352](https://github.com/pytorch/pytorch/pull/152352))
- [torch-xpu-ops] Update torch-xpu-ops commit pin. ([#152321](https://github.com/pytorch/pytorch/pull/152321))
- Don't run NCCL/gloo distributed test without GPUs ([#150764](https://github.com/pytorch/pytorch/pull/150764))
- [OpenReg] Add _lazy_init and rng_state support for OpenReg ([#151914](https://github.com/pytorch/pytorch/pull/151914))
- [BE] Remove dangling # in contributing.md ([#152259](https://github.com/pytorch/pytorch/pull/152259))
- Fix shadow local variables ([#152429](https://github.com/pytorch/pytorch/pull/152429))
- [easy] Fix test_dynamo_timed ([#152387](https://github.com/pytorch/pytorch/pull/152387))
- Add latex settings ([#152350](https://github.com/pytorch/pytorch/pull/152350))
- submodules: point gloo to new home in pytorch/ ([#152438](https://github.com/pytorch/pytorch/pull/152438))
- Add private config to broadcast rank0 decision from the partitioner to all ranks ([#152264](https://github.com/pytorch/pytorch/pull/152264))
- Avoid linking multiple OMP runtimes in libtorch_cpu.so if BLAS used is  OpenBLAS.  ([#147725](https://github.com/pytorch/pytorch/pull/147725))
- [PT2] Port replace_lce_with_matmul / replace_first_lce_with_fused_matmul_lce to PT2 pre_grad passes ([#152450](https://github.com/pytorch/pytorch/pull/152450))
- add xfail for distributed tests on Jetson ([#152224](https://github.com/pytorch/pytorch/pull/152224))
- [CUDA][MXFP8] bump tolerances for `test_blockwise_mxfp8_nvfp4_numerics` ([#151811](https://github.com/pytorch/pytorch/pull/151811))
- fix tests broken after #152450 ([#152493](https://github.com/pytorch/pytorch/pull/152493))
- Fix instantiate_device_type_tests() for 3rd-party devices ([#152177](https://github.com/pytorch/pytorch/pull/152177))
- Fix flaky test in test_custom_ops ([#152484](https://github.com/pytorch/pytorch/pull/152484))
- fix tests broken after #152450 ([#152493](https://github.com/pytorch/pytorch/pull/152493))
- [CUDA][TF32] Account for TF32 in `compile_kernel_advanced` ([#152468](https://github.com/pytorch/pytorch/pull/152468))
- [CUDA] Add new architectures ([#152414](https://github.com/pytorch/pytorch/pull/152414))
- [invoke_subgraph] Use backward identifier for min-cut parititioning ([#152207](https://github.com/pytorch/pytorch/pull/152207))
- Cast to unsigned char to avoid UB ([#152360](https://github.com/pytorch/pytorch/pull/152360))
- Change test/inductor/test_standalone_compile to test/inductor/test_compile ([#152103](https://github.com/pytorch/pytorch/pull/152103))
- Add two missing JIT tests to CMake ([#152440](https://github.com/pytorch/pytorch/pull/152440))
- Remove unnecessary condition compilation macro ([#152512](https://github.com/pytorch/pytorch/pull/152512))
- [CUDA] Fix `test_multi_device_context_manager` on CUDA ([#152474](https://github.com/pytorch/pytorch/pull/152474))
- [AOTAutogradCache] Allow `torch.Tensor` and a non-torch op from einops ([#152369](https://github.com/pytorch/pytorch/pull/152369))
- [CUDAGraph Trees] support memory allocation on side stream ([#152472](https://github.com/pytorch/pytorch/pull/152472))
- [Docs] Add Description of `validate_args` for torch.distributions ([#152173](https://github.com/pytorch/pytorch/pull/152173))
- Configurable logging for cpp_extensions.py ([#152260](https://github.com/pytorch/pytorch/pull/152260))
- [CUDA][SDPA] Bump python `fused_attention_vs_math_ref_grads` `fudge_factor` for `sm120` ([#152491](https://github.com/pytorch/pytorch/pull/152491))
- Update CODEOWNERS (torch/utils/data/) ([#152482](https://github.com/pytorch/pytorch/pull/152482))
- [inductor][BE] Add more debug logs for why fx graph cache doesn't happen ([#152487](https://github.com/pytorch/pytorch/pull/152487))
- [CUDA][SDPA] bump fudge factor in `test_sdpa` in `test_nestedtensor` ([#152235](https://github.com/pytorch/pytorch/pull/152235))
- [nativert] Add utility function to convert strings into numbers. ([#151467](https://github.com/pytorch/pytorch/pull/151467))
- Clean up conda usage in benchmark scripts ([#152552](https://github.com/pytorch/pytorch/pull/152552))
- [nativert] Add moodycamel/concurrentqueue as third-party dependency ([#152033](https://github.com/pytorch/pytorch/pull/152033))
- Remove redundant line in partitioner ([#152517](https://github.com/pytorch/pytorch/pull/152517))
- [invoke_subgraph] Cache on tangent metadata and retrace if needed ([#152357](https://github.com/pytorch/pytorch/pull/152357))
- Change test/inductor/test_standalone_compile to test/inductor/test_compile ([#152103](https://github.com/pytorch/pytorch/pull/152103))
- Decorate `test_host_memory_stats` with `@serialTest` ([#152454](https://github.com/pytorch/pytorch/pull/152454))
- [inductor][invoke_subgraph] Remove assertion checks for outputs of invoke_subgraph ([#152384](https://github.com/pytorch/pytorch/pull/152384))
- [inductor][subgraph] Simplify the resulting output code for subgraph ([#152383](https://github.com/pytorch/pytorch/pull/152383))
- [invoke_subgraph] Simplify output code for subgraph output node ([#152490](https://github.com/pytorch/pytorch/pull/152490))
- [inductor][invoke_subgraph] Free the buffers before the subgraph call ([#152494](https://github.com/pytorch/pytorch/pull/152494))
- [inductor][BE] Add more debug logs for why fx graph cache doesn't happen ([#152487](https://github.com/pytorch/pytorch/pull/152487))
- [torchgen] Refactor `torchgen.utils.FileManager` to accept `pathlib.Path` ([#150726](https://github.com/pytorch/pytorch/pull/150726))
- [BE] detect CXX pytree requirement with `TorchVersion` ([#151102](https://github.com/pytorch/pytorch/pull/151102))
- [nativert] port enumerate from folly to c10::utill ([#152481](https://github.com/pytorch/pytorch/pull/152481))
- [aot] mark dynamic activations as maybe dynamic ([#149707](https://github.com/pytorch/pytorch/pull/149707))
- Fix some inductor periodic benchmarks ([#152605](https://github.com/pytorch/pytorch/pull/152605))
- [Inductor] Add decomposeK as an autotuning choice for mm ([#150654](https://github.com/pytorch/pytorch/pull/150654))
- [BE] Update numba versions ([#152557](https://github.com/pytorch/pytorch/pull/152557))
- [hop][be] make check_input_alias_and_mutation_return_ouputs create new fake mode ([#152245](https://github.com/pytorch/pytorch/pull/152245))
- [hop][schema] allow adding kw_only info to schema argument ([#152246](https://github.com/pytorch/pytorch/pull/152246))
- [hop] make materialize_as_graph's include and exclude dispatch key set optional ([#152247](https://github.com/pytorch/pytorch/pull/152247))
- [refactor] refactor dense implementation of auto_functionalized_v2 for better clarity ([#152248](https://github.com/pytorch/pytorch/pull/152248))
- [cutlass backend] Minor lru_cache to slightly speed up filtering ops ([#152577](https://github.com/pytorch/pytorch/pull/152577))
- [CUDAGraph Trees] support memory allocation on side stream ([#152472](https://github.com/pytorch/pytorch/pull/152472))
- Remove unnecessary __STDC_FORMAT_MACROS macro ([#152513](https://github.com/pytorch/pytorch/pull/152513))
- [inductor][invoke_subgraph] Remove assertion checks for outputs of invoke_subgraph ([#152384](https://github.com/pytorch/pytorch/pull/152384))
- [inductor][subgraph] Simplify the resulting output code for subgraph ([#152383](https://github.com/pytorch/pytorch/pull/152383))
- Do not check out nccl when not building it ([#152533](https://github.com/pytorch/pytorch/pull/152533))
- [invoke_subgraph] Simplify output code for subgraph output node ([#152490](https://github.com/pytorch/pytorch/pull/152490))
- [ROCm] Upgrade ROCm CI to ROCm6.4 ([#151368](https://github.com/pytorch/pytorch/pull/151368))
- [StaticCudaLauncher] Ensure cuda context exists before launching kernels ([#152667](https://github.com/pytorch/pytorch/pull/152667))
- Skip search for MKL on ARM cpus ([#145850](https://github.com/pytorch/pytorch/pull/145850))
- [cuDNN][SDPA] Fix head-dim 256 condition for SM 10.0 ([#152076](https://github.com/pytorch/pytorch/pull/152076))
- [AOTAutogradCache][Easy] Move `"einops.einops.rearrange"` to `SAFE_NON_TORCH_FUNCTIONS` ([#152640](https://github.com/pytorch/pytorch/pull/152640))
- [ez] Disable failing test in periodic no gpu no avx  ([#152698](https://github.com/pytorch/pytorch/pull/152698))
- [CUDA][TF32] Account for TF32 in `test_conv2d_same_padding` ([#152618](https://github.com/pytorch/pytorch/pull/152618))
- Cleanup DeviceInterface in triton test ([#152409](https://github.com/pytorch/pytorch/pull/152409))
- [inductor][invoke_subgraph] Free the buffers before the subgraph call ([#152494](https://github.com/pytorch/pytorch/pull/152494))
- [Inductor UT] Generalize device-bias code in `test_flex_attention.py` ([#151937](https://github.com/pytorch/pytorch/pull/151937))
- [Inductor] Add decomposeK as an autotuning choice for mm ([#150654](https://github.com/pytorch/pytorch/pull/150654))
- Make DispatchKeySet serializable; add `__eq__` ([#152732](https://github.com/pytorch/pytorch/pull/152732))
- Make torch/csrc/utils.h to be device-agnostic ([#152521](https://github.com/pytorch/pytorch/pull/152521))
- Remove the unnecessary cuda/Tensor.cpp ([#152522](https://github.com/pytorch/pytorch/pull/152522))
- [OpenReg] Add _lazy_init and rng_state support for OpenReg ([#151914](https://github.com/pytorch/pytorch/pull/151914))
- Set CMake 3.5 as minimum version in pytorch_android ([#152769](https://github.com/pytorch/pytorch/pull/152769))
- Exempt overriding methods from docstring_linter (fix #151692) ([#151906](https://github.com/pytorch/pytorch/pull/151906))
- Add "#pragma once" to CachingHostAllocator.h ([#152800](https://github.com/pytorch/pytorch/pull/152800))
- [cutlass backend] Minor lru_cache to slightly speed up filtering ops ([#152577](https://github.com/pytorch/pytorch/pull/152577))
- [CI] Use cmake from pip instead of conda in CI docker images ([#152537](https://github.com/pytorch/pytorch/pull/152537))
- Add infra to run CPython tests under Dynamo ([#150787](https://github.com/pytorch/pytorch/pull/150787))
- Disable SLEEF implementation of vec::maximum in vec128_float_neon.h | Accelerate aten::hardtanh_ by 21x ([#152538](https://github.com/pytorch/pytorch/pull/152538))
- Generate test reports for pytest when option is given  ([#152170](https://github.com/pytorch/pytorch/pull/152170))
- [nativert] move intrusive list to c10/util ([#152754](https://github.com/pytorch/pytorch/pull/152754))
- [BE]: Update torch core lazy helpers with micropts ([#152778](https://github.com/pytorch/pytorch/pull/152778))
- Synchronize in foreach tests after profiling ([#152857](https://github.com/pytorch/pytorch/pull/152857))
- [submodule] Bump ITTAPI to 3.25.5 ([#150263](https://github.com/pytorch/pytorch/pull/150263))
- [nativert] Move TensorMeta to pytorch core ([#152475](https://github.com/pytorch/pytorch/pull/152475))
- Make device check error message more descriptive ([#150750](https://github.com/pytorch/pytorch/pull/150750))
- Fix typo on `test_multi_device_context_manager` for XPU ([#152812](https://github.com/pytorch/pytorch/pull/152812))
- Only do shallow clone when checkout nccl ([#152826](https://github.com/pytorch/pytorch/pull/152826))
- [dynamo] Recursively realize the stack_values ([#152853](https://github.com/pytorch/pytorch/pull/152853))
- [partitioner] Fix argument to _broadcast_on_rank0 ([#152846](https://github.com/pytorch/pytorch/pull/152846))
- [precompile] Refactor AOTAutogradCacheEntry to be generic ([#152836](https://github.com/pytorch/pytorch/pull/152836))
- Devcontainer: Fix context path and workspace mount ([#152880](https://github.com/pytorch/pytorch/pull/152880))
- Devcontainer: Replace conda with apt-based setup ([#152881](https://github.com/pytorch/pytorch/pull/152881))
- Devcontainer: Optimize apt-get commands to reduce Docker image size ([#152882](https://github.com/pytorch/pytorch/pull/152882))
- [dynamo] Avoid running `torch.nn.Module.__call__` twice under `torch.compile(mod)` ([#152740](https://github.com/pytorch/pytorch/pull/152740))
- [dynamo] Support `delattr` on result of `torch.compile(module)` ([#152741](https://github.com/pytorch/pytorch/pull/152741))
- [BE] Update numba versions ([#152557](https://github.com/pytorch/pytorch/pull/152557))
- [cutlass backend][BE][clean-up] refactor to remove use of autotune_fallback_to_aten=True in cutlass backend tests ([#152850](https://github.com/pytorch/pytorch/pull/152850))
- [dynamo] Recursively realize the stack_values ([#152853](https://github.com/pytorch/pytorch/pull/152853))
- [nativert] Port string join and split to c10/util ([#152873](https://github.com/pytorch/pytorch/pull/152873))
- Add infra to run CPython tests under Dynamo ([#150787](https://github.com/pytorch/pytorch/pull/150787))
- [Break XPU] Fix XPU UT failures introduced by community. ([#152945](https://github.com/pytorch/pytorch/pull/152945))
- Fix Codegen.cmake warning ([#153023](https://github.com/pytorch/pytorch/pull/153023))
- [Lint] Add install command for GHA step ([#153013](https://github.com/pytorch/pytorch/pull/153013))
- [CUDA] Rest peak memory stats before running `test_set_per_process_memory_fraction` ([#152540](https://github.com/pytorch/pytorch/pull/152540))
- [BE] Update ruamel to 0.18.10 ([#153057](https://github.com/pytorch/pytorch/pull/153057))
- [cutlass backend][test] re-enable test_cuda_compile_command for fbcode ([#153001](https://github.com/pytorch/pytorch/pull/153001))
- [Typing] Improve device typing for `torch.set_default_device()` ([#153028](https://github.com/pytorch/pytorch/pull/153028))
- Fixes detection of ArmPL on Linux platform ([#150031](https://github.com/pytorch/pytorch/pull/150031))
- [nativert] Move MPMCQueue to torch/nativert. ([#152837](https://github.com/pytorch/pytorch/pull/152837))
- Fix test/test_optim.py error message. ([#153076](https://github.com/pytorch/pytorch/pull/153076))
- Make device check error message more descriptive ([#150750](https://github.com/pytorch/pytorch/pull/150750))
- [inductor][dynamo] Include operator name in size/stride/alignment assertion ([#152353](https://github.com/pytorch/pytorch/pull/152353))
- Clean up of CUTLASS_VERSION ([#152947](https://github.com/pytorch/pytorch/pull/152947))
- [CI] Use cmake from pip instead of conda in CI docker images ([#152537](https://github.com/pytorch/pytorch/pull/152537))
- [ROCm] Upgrade ROCm CI to ROCm6.4 ([#151368](https://github.com/pytorch/pytorch/pull/151368))
- [CI] Use cmake from pip instead of conda in CI docker images ([#152537](https://github.com/pytorch/pytorch/pull/152537))
- [TEST][ATen][CUDA] Skip row-wise scaled matrix mmultiplication tests on sm_120+ ([#152814](https://github.com/pytorch/pytorch/pull/152814))
- [nativert] Address tooling setup for torch/nativert/ ([#153164](https://github.com/pytorch/pytorch/pull/153164))
- At least one of ROCM_HOME or CUDA_HOME must be None ([#152236](https://github.com/pytorch/pytorch/pull/152236))
- [Cutlass] Fix tests ([#153196](https://github.com/pytorch/pytorch/pull/153196))
- [fbgemm_gpu] Replace `C10_CUDA_KERNEL_LAUNCH_CHECK()` in the `KernelLauncher` ([#153178](https://github.com/pytorch/pytorch/pull/153178))
- Format all headers under ATen/cpu/vec, not just top-level ([#152364](https://github.com/pytorch/pytorch/pull/152364))
- [nativert] Improve MPMCQueue tests. ([#153154](https://github.com/pytorch/pytorch/pull/153154))
- allocate cuMem memory with rdma flag ([#153261](https://github.com/pytorch/pytorch/pull/153261))
- [pytorch] Expose `c10_retrieve_device_side_assertion_info()` for use by external code ([#153211](https://github.com/pytorch/pytorch/pull/153211))
- [Dynamo] Fix typing in graph_deduplication.py ([#152572](https://github.com/pytorch/pytorch/pull/152572))
- [export] Exporter API prototype. ([#153205](https://github.com/pytorch/pytorch/pull/153205))
- [Typing] Apply `torch.types.Device` in `torch/cuda/memory.py` ([#153027](https://github.com/pytorch/pytorch/pull/153027))
- Update slow tests ([#151207](https://github.com/pytorch/pytorch/pull/151207))
- [Inductor UT][Break XPU] Generalize newly added device-bias code in Inductor UT. ([#153355](https://github.com/pytorch/pytorch/pull/153355))
- [BE]: Use undocumented temp shim to restore setuptools compat ([#153052](https://github.com/pytorch/pytorch/pull/153052))
- [BE]: Enable ruff rule TC007 ([#153394](https://github.com/pytorch/pytorch/pull/153394))
- Clean up right nav ([#153090](https://github.com/pytorch/pytorch/pull/153090))
- convert guard_size_oblivious to runtime check in infer_size_impl ([#148872](https://github.com/pytorch/pytorch/pull/148872))
- [OpenReg] Move SDPA to OpenReg from open_registration_extension.cpp ([#153309](https://github.com/pytorch/pytorch/pull/153309))
- refresh expected results  ([#150166](https://github.com/pytorch/pytorch/pull/150166))
- Remove unused typing-extensions BUCK target ([#153229](https://github.com/pytorch/pytorch/pull/153229))
- [Dynamo] Fix typing in graph_deduplication.py ([#152572](https://github.com/pytorch/pytorch/pull/152572))
- Fix skipIfXpu and skipIfHpu disables tests when used on class ([#151315](https://github.com/pytorch/pytorch/pull/151315))
- [Intel GPU] Use user-friendly err msg in mm ([#151655](https://github.com/pytorch/pytorch/pull/151655))
- [MemoryZ] Sync changes to internal page ([#153166](https://github.com/pytorch/pytorch/pull/153166))
- [ROCm] unkip test_non_standard_bool except for failings ops ([#152956](https://github.com/pytorch/pytorch/pull/152956))
- [ca] run xfails up until their last passing backend ([#153279](https://github.com/pytorch/pytorch/pull/153279))
- [EZ/Profiler] Replace manual GIL calls with pybind GIL calls ([#153415](https://github.com/pytorch/pytorch/pull/153415))
- Fix misleadingly high AOT Inductor dashboard performance ([#153060](https://github.com/pytorch/pytorch/pull/153060))
- [nativert] move executor config to torch ([#153087](https://github.com/pytorch/pytorch/pull/153087))
- [cuDNN][SDPA] cuDNN SDPA refactor/cleanup, nested tensor backward, test priority bump for `sm90`, `sm100` ([#149282](https://github.com/pytorch/pytorch/pull/149282))
- [BE] remove outdated torch/README.md ([#153500](https://github.com/pytorch/pytorch/pull/153500))
- [MemoryZ] Show the current and max entries rendered ([#153446](https://github.com/pytorch/pytorch/pull/153446))
- Checks kv pair indexing in OrderedPreservingDictTest.test_range_insert ([#148136](https://github.com/pytorch/pytorch/pull/148136))
- [CUDA][CUDNN] Dispatch to cuDNN for non-batch-splittable 64-bit NCHW convolutions ([#153101](https://github.com/pytorch/pytorch/pull/153101))
- [nativert] Move Placement to pytorch core ([#152953](https://github.com/pytorch/pytorch/pull/152953))
- Update torch-xpu-ops commit pin ([#153445](https://github.com/pytorch/pytorch/pull/153445))
- [BE][Ez]: RUF200 - validate pyproject.toml metadata ([#153543](https://github.com/pytorch/pytorch/pull/153543))
- Update lint_urls.sh ([#153246](https://github.com/pytorch/pytorch/pull/153246))
- [dynamo] Emit warning on global module hooks when calling using output of `torch.compile(module)` ([#152740](https://github.com/pytorch/pytorch/pull/152740))
- [dynamo] Support `delattr` on result of `torch.compile(module)` ([#152741](https://github.com/pytorch/pytorch/pull/152741))
- torch.compile: Remove reference to the unused dynamo_config.dynamic_shapes from ([#153297](https://github.com/pytorch/pytorch/pull/153297))
- [Ez][BE]: Remove accidental classvar ([#153540](https://github.com/pytorch/pytorch/pull/153540))
- [inductor][dynamo] Include operator name in size/stride/alignment assertion ([#152353](https://github.com/pytorch/pytorch/pull/152353))
- [FlexAttention] Enforce Q,K,V memory layouts for fp8 flex attention to avoid perf degradation ([#153357](https://github.com/pytorch/pytorch/pull/153357))
- [torchgen] Refactor `torchgen.utils.FileManager` to accept `pathlib.Path` ([#150726](https://github.com/pytorch/pytorch/pull/150726))
- unbreak fb:operator_benchmark_test ([#152049](https://github.com/pytorch/pytorch/pull/152049))
- [BE] Improve the typing related to `model` input argument of `torch.compile()` ([#153559](https://github.com/pytorch/pytorch/pull/153559))
- Removed duplicate patterns from gitignore ([#153515](https://github.com/pytorch/pytorch/pull/153515))
- fix set_logs for a single child log file ([#153580](https://github.com/pytorch/pytorch/pull/153580))
- [Memento] On-demand mode using without torch api ([#153171](https://github.com/pytorch/pytorch/pull/153171))
- [ca][dynamo] always run eager checkpoint region's recomputation in eager ([#153300](https://github.com/pytorch/pytorch/pull/153300))
- [ca][dtensor] run real PG dtensor tests under CA ([#152689](https://github.com/pytorch/pytorch/pull/152689))
- [torchgen] Refactor and simplify `gen_pyi.py` to use Generic TypeAlias (PEP 585) and Union Type (PEP 604) ([#150727](https://github.com/pytorch/pytorch/pull/150727))
- [submodule] Update kleidiai to v1.8.0 ([#153592](https://github.com/pytorch/pytorch/pull/153592))
- cpu: enable gemm-bf16f32 for SDPA BF16  ([#140159](https://github.com/pytorch/pytorch/pull/140159))
- Delete TorchScript based Android demo app and point to ExecuTorch ([#153633](https://github.com/pytorch/pytorch/pull/153633))
- Refactor tests in test_max_autotune into a few separate test cases. ([#153486](https://github.com/pytorch/pytorch/pull/153486))
- [BE] Extend empty_gpu_cache to mps ([#153657](https://github.com/pytorch/pytorch/pull/153657))
- [ca][dynamo] always run eager checkpoint region's recomputation in eager ([#153300](https://github.com/pytorch/pytorch/pull/153300))
- [ca][dtensor] run real PG dtensor tests under CA ([#152689](https://github.com/pytorch/pytorch/pull/152689))
- Reapply "Delete TorchScript based Android demo app and point to ExecuTorch (#153633)" ([#153656](https://github.com/pytorch/pytorch/pull/153656))
- [FlexAttention] Enforce Q,K,V memory layouts for fp8 flex attention to avoid perf degradation ([#153357](https://github.com/pytorch/pytorch/pull/153357))
- [CUDA][cuBLAS] Remove `IS_ARM64` skip in `test_matmul_cuda.py` ([#153660](https://github.com/pytorch/pytorch/pull/153660))
- [Linter] Add linter to detect device-bias hard code in test cases. ([#152948](https://github.com/pytorch/pytorch/pull/152948))
- Split out second pass of LayerNorm for profiler attribution reasons ([#153578](https://github.com/pytorch/pytorch/pull/153578))
- [easy] Fix endif comments in functional_base.h ([#153696](https://github.com/pytorch/pytorch/pull/153696))
- [Break XPU] Skip newly added test case on XPU that failed because torch._C._scatter not implemented. ([#153685](https://github.com/pytorch/pytorch/pull/153685))
- [Set] Add CPython set tests ([#150792](https://github.com/pytorch/pytorch/pull/150792))
- [hipify] Replace cuda error cudaErrorContextIsDestroyed ([#153576](https://github.com/pytorch/pytorch/pull/153576))
- [BE]: Enable misc RUF rules and fix pyproject.toml indent ([#153624](https://github.com/pytorch/pytorch/pull/153624))
- [CUDA][cuBLAS][cuBLASLt] avoid polluting prefer cuBLAS/Lt setting across tests ([#153655](https://github.com/pytorch/pytorch/pull/153655))
- [submodule] Update google benchmark to v1.9.3 ([#153676](https://github.com/pytorch/pytorch/pull/153676))
- Add cudaLaunchKernel to cuda_to_hip_mappings ([#153690](https://github.com/pytorch/pytorch/pull/153690))
- [BE]: Update ruff linter to 0.11.10 ([#153625](https://github.com/pytorch/pytorch/pull/153625))
- [Dynamo] added warning message for tracing lru_cache wrapped functions ([#153744](https://github.com/pytorch/pytorch/pull/153744))
- [SDPA][EZ] Abate narrowing conversion warning spam in `flash_api.cpp` ([#153643](https://github.com/pytorch/pytorch/pull/153643))
- [BE] Ensure generated stub files by `gen_pyi` are properly formatted ([#150730](https://github.com/pytorch/pytorch/pull/150730))
- FakeTensorMode dispatch shouldn't include bypass in exception context ([#153780](https://github.com/pytorch/pytorch/pull/153780))
- [ATen-CPU] Use `math.h` for GeLU as well as `cmath` ([#153742](https://github.com/pytorch/pytorch/pull/153742))
- [XPU] [Windows] Auto turn on kineto XPU build when compiler version support. ([#153681](https://github.com/pytorch/pytorch/pull/153681))
- [amd] fix tunableop gemm ([#153764](https://github.com/pytorch/pytorch/pull/153764))
- [BE]: Remove redundant copy ([#153629](https://github.com/pytorch/pytorch/pull/153629))
- Update slow tests ([#153815](https://github.com/pytorch/pytorch/pull/153815))
- Recheck autotune cache on static cuda launcher load ([#153565](https://github.com/pytorch/pytorch/pull/153565))
- [AOTI] Skip a rocm test ([#153828](https://github.com/pytorch/pytorch/pull/153828))
- Enable ruff check for all ipynb files ([#153820](https://github.com/pytorch/pytorch/pull/153820))
- [pytorch] Delete TorchScript based Android demo app and point user to ExecuTorch ([#153767](https://github.com/pytorch/pytorch/pull/153767))
- [CI] Reuse old whl ([#153838](https://github.com/pytorch/pytorch/pull/153838))
- [BE][Ez]: Propogate some nodiscard in RNN ([#153836](https://github.com/pytorch/pytorch/pull/153836))
- [CUDA][cuBLASLt] Respect `allow[FP16/BF16]ReductionCuBLAS` in `cuBLASLt` ([#153095](https://github.com/pytorch/pytorch/pull/153095))
- Cache code generation during triton template expansion and enable it  for mm_template. ([#151773](https://github.com/pytorch/pytorch/pull/151773))
- [CI] Reuse old whl ([#153838](https://github.com/pytorch/pytorch/pull/153838))
- [CI] Reuse old whl ([#153838](https://github.com/pytorch/pytorch/pull/153838))
- [Docs] Mention `version.txt` change for patch releases ([#153860](https://github.com/pytorch/pytorch/pull/153860))
- [CachingHostAllocator] guard accesses to use_host_register by mutex ([#153845](https://github.com/pytorch/pytorch/pull/153845))
- [3.13] Remove all profiler related skips ([#153857](https://github.com/pytorch/pytorch/pull/153857))
- [Dynamo] added warning message for tracing lru_cache wrapped functions ([#153744](https://github.com/pytorch/pytorch/pull/153744))
- [Cutlass] EVT tests update ([#153926](https://github.com/pytorch/pytorch/pull/153926))
- Recheck autotune cache on static cuda launcher load ([#153565](https://github.com/pytorch/pytorch/pull/153565))
- [BE]: Update fmtlib submodule to 11.2.0 ([#153853](https://github.com/pytorch/pytorch/pull/153853))
- Fixed an issue with XPU skip so the test_decompose_mem_bound_mm.py suite can be ran correctly ([#153245](https://github.com/pytorch/pytorch/pull/153245))
- [CUDA][cuBLAS][cuBLASLt] avoid polluting prefer cuBLAS/Lt setting across tests ([#153655](https://github.com/pytorch/pytorch/pull/153655))
- Redirect mobile_optimizer.rst to executorch ([#153664](https://github.com/pytorch/pytorch/pull/153664))
- [cuBLASLt] relax `addmm` cuBLASLt constraint ([#153675](https://github.com/pytorch/pytorch/pull/153675))
- Support independent builds for cpp extension tests + apply to libtorch_agnostic tests ([#153264](https://github.com/pytorch/pytorch/pull/153264))
- Make python_agnostic cpp extension tests standalone ([#153274](https://github.com/pytorch/pytorch/pull/153274))
- set CUDA_MODULE_LOADING for older drivers only ([#152695](https://github.com/pytorch/pytorch/pull/152695))
- [CUDA][CUDNN] Dispatch to cuDNN for non-batch-splittable 64-bit NCHW convolutions ([#153101](https://github.com/pytorch/pytorch/pull/153101))
- [submodule] Update fbgemm pinned version ([#153950](https://github.com/pytorch/pytorch/pull/153950))
- Fix test_side_stream_backward_overlap flakiness ([#153963](https://github.com/pytorch/pytorch/pull/153963))
- [EZ] Update mps xfail reason ([#153971](https://github.com/pytorch/pytorch/pull/153971))
- [BE] light cleanups to linter logic ([#153965](https://github.com/pytorch/pytorch/pull/153965))
- [nativert] Move GraphSignature to pytorch core ([#152969](https://github.com/pytorch/pytorch/pull/152969))
- [cuBLAS][cuBLASLt] Use cuBLAS default workspace size in Lt ([#153556](https://github.com/pytorch/pytorch/pull/153556))
- [CI][CUDA] Move cu118 distributed pull jobs to cu126, move cu124-sm75 to cu126-sm75 ([#151594](https://github.com/pytorch/pytorch/pull/151594))
- [inductor][cutlass backend] Add 2 stage autotuning aka prescreening ([#153335](https://github.com/pytorch/pytorch/pull/153335))
- Add codeowner for merge rules ([#152354](https://github.com/pytorch/pytorch/pull/152354))
- Remove janky (though at times useful) dlclose test ([#153975](https://github.com/pytorch/pytorch/pull/153975))
- Add torch/header_only_apis.txt and enforce they're tested ([#153635](https://github.com/pytorch/pytorch/pull/153635))
- [MTIA ATen Backend] Migrate "_unsafe_view" and "view" ops from out-of-tree to pytorch in-tree ([#153670](https://github.com/pytorch/pytorch/pull/153670))
- [Intel GPU] scalar tensor case handling in addmm, baddmm ([#153051](https://github.com/pytorch/pytorch/pull/153051))
- Update torch-xpu-ops commit pin ([#153902](https://github.com/pytorch/pytorch/pull/153902))
- [inductor][cutlass backend] Add 2 stage autotuning aka prescreening ([#153335](https://github.com/pytorch/pytorch/pull/153335))
- [precompile] Add BundledAOTAutogradCacheEntry ([#152840](https://github.com/pytorch/pytorch/pull/152840))
- Cache code generation during triton template expansion and enable it  for mm_template. ([#151773](https://github.com/pytorch/pytorch/pull/151773))
- [hop_schema] add HopSchemaGenerator to make it easier to create hop schema ([#152974](https://github.com/pytorch/pytorch/pull/152974))
- [hop_schema] support gen_schema for invoke_subgraph ([#152984](https://github.com/pytorch/pytorch/pull/152984))
- [easy] Fix internal only test ([#154035](https://github.com/pytorch/pytorch/pull/154035))
- [ROCm][Windows] Run hipcc with compatibility flags. ([#153986](https://github.com/pytorch/pytorch/pull/153986))
- [map] make proxy mode re-dispatch to fake key ([#151034](https://github.com/pytorch/pytorch/pull/151034))
- remove TestCustomOp.test_impl_device_cpu from dynamo expected failures ([#154049](https://github.com/pytorch/pytorch/pull/154049))
- [aot] fix deepcopying of aot bwd containing real tensors ([#153999](https://github.com/pytorch/pytorch/pull/153999))
- [cond] support output the same unbacked symbol from two branches ([#148206](https://github.com/pytorch/pytorch/pull/148206))
- [CI][CUDA][Distributed] Move cuda 11.8 distributed pull jobs to cuda 12.6  ([#151594](https://github.com/pytorch/pytorch/pull/151594))
- Allow higher fp16 tolerance for phlippe_resnet on CUDA 12.8 ([#154109](https://github.com/pytorch/pytorch/pull/154109))
- Get rid of unused code in linters ([#154043](https://github.com/pytorch/pytorch/pull/154043))
- remove unused code. ([#153979](https://github.com/pytorch/pytorch/pull/153979))
- [Inductor] Add attention pattern for model DistilBert in transformers==4.44.2. ([#154091](https://github.com/pytorch/pytorch/pull/154091))
- [BE][Ez]: Enable PT014 check for duplicate parameterize test cases ([#154118](https://github.com/pytorch/pytorch/pull/154118))
- Don't upload compiler benchmark debug info to the benchmark database ([#153769](https://github.com/pytorch/pytorch/pull/153769))
- [AOTI][cutlass backend] Do not remove the cutlass kernel .o file after packaging ([#154155](https://github.com/pytorch/pytorch/pull/154155))
- [ROCm] Prefer hipblaslt for gfx1200, gfx1201 ([#153610](https://github.com/pytorch/pytorch/pull/153610))
- [inductor][cutlass backend] Add 2 stage autotuning aka prescreening ([#153335](https://github.com/pytorch/pytorch/pull/153335))
- [MTIA Aten Backend][2/n] Migrate clamp ops(clamp.out/clamp_min.out/clamp_max.out) from out-of-tree to in-tree ([#154015](https://github.com/pytorch/pytorch/pull/154015))
- [CI] Fix `TestDynamoTimed.test_ir_count` for 3.12 ([#154268](https://github.com/pytorch/pytorch/pull/154268))
- remove sleef_arm target ([#154166](https://github.com/pytorch/pytorch/pull/154166))
- Add Vectorized FP8 E4M3 ([#152417](https://github.com/pytorch/pytorch/pull/152417))
- Add Vectorized FP8 E5M2 ([#153364](https://github.com/pytorch/pytorch/pull/153364))
- Add a link to transformer_building_blocks tutorial ([#154281](https://github.com/pytorch/pytorch/pull/154281))
- [cuBLAS][cuBLASLt] Use cuBLAS default workspace size in Lt ([#153556](https://github.com/pytorch/pytorch/pull/153556))
- [Inductor UT] Reuse test_fused_attention.py for Intel GPU. ([#154110](https://github.com/pytorch/pytorch/pull/154110))
- Fix segfault on exit in CachingHostAllocator by signaling background thread to exit ([#154117](https://github.com/pytorch/pytorch/pull/154117))
- [BE][Ez]: Enable ruff rule PLW1507. os.environ is not copied ([#154120](https://github.com/pytorch/pytorch/pull/154120))
- Bump setuptools from 70.0.0 to 78.1.1 in /tools/build/bazel ([#154075](https://github.com/pytorch/pytorch/pull/154075))
- Re-enable FakeTensor caching for SymInts ([#152662](https://github.com/pytorch/pytorch/pull/152662))
- [executorch hash update] update the pinned executorch hash ([#153436](https://github.com/pytorch/pytorch/pull/153436))
- Fix s390x vectorization compilation in inductor ([#153946](https://github.com/pytorch/pytorch/pull/153946))
- Add missing docstring for `sym_ite` ([#154201](https://github.com/pytorch/pytorch/pull/154201))
- used guard_or_false instead of  guard_size_oblivious in is_int_or_symint ([#154167](https://github.com/pytorch/pytorch/pull/154167))
- used guard_or_false instead of guard_size_oblivious inside maybe_reduce ([#154172](https://github.com/pytorch/pytorch/pull/154172))
- [EASY] use guard_or_false instead of gso in Meta converter ([#154234](https://github.com/pytorch/pytorch/pull/154234))
- Remove outdated CUDA 11 conditions ([#154313](https://github.com/pytorch/pytorch/pull/154313))
- [nativert] Move file_util to pytorch core ([#153162](https://github.com/pytorch/pytorch/pull/153162))
- [executorch hash update] update the pinned executorch hash ([#153436](https://github.com/pytorch/pytorch/pull/153436))
- fix a compilation issue when TORCH_XPU_ARCH_LIST is an empty string ([#153604](https://github.com/pytorch/pytorch/pull/153604))
- [MTIA Aten Backend][1.2/n] Migrate as_strided to in-tree, and add unit tests ([#154336](https://github.com/pytorch/pytorch/pull/154336))
- Add pyrefly.toml ([#154144](https://github.com/pytorch/pytorch/pull/154144))
- Move inductor workflows focal (ubuntu 20.04) -> jammy (ubuntu 22.04) ([#154153](https://github.com/pytorch/pytorch/pull/154153))
- Add getDeviceProperties api to torch mtia device ([#153577](https://github.com/pytorch/pytorch/pull/153577))
- [Intel GPU] Enable mkdnn._linear_pointwise at XPU backend ([#140365](https://github.com/pytorch/pytorch/pull/140365))
- [ez] Rewrite comment to be more friendly to non haskellers ([#151421](https://github.com/pytorch/pytorch/pull/151421))
- [BE] Fix typos in SyntaxError description ([#154436](https://github.com/pytorch/pytorch/pull/154436))
- [Kineto x Insight] Update Kineto submodule ([#154426](https://github.com/pytorch/pytorch/pull/154426))
- [ROCm][Windows] Fix building torch 2.8 wheel with ROCm (added hipblasLt and rocblas directories) ([#153144](https://github.com/pytorch/pytorch/pull/153144))
- Add CPython tests for iter/sort ([#150797](https://github.com/pytorch/pytorch/pull/150797))
- Add CPython complex tests ([#152015](https://github.com/pytorch/pytorch/pull/152015))
- Add CPython tests for unittest ([#150788](https://github.com/pytorch/pytorch/pull/150788))
- [MTIA Aten Backend][1.3/n] Migrate remaining view ops, which all need explicit register in `native_functions.yaml` ([#154337](https://github.com/pytorch/pytorch/pull/154337))
- Fixed an issue with XPU skip so the test_decompose_mem_bound_mm.py suite can be ran correctly ([#153245](https://github.com/pytorch/pytorch/pull/153245))
- Move inductor workflows focal (ubuntu 20.04) -> jammy (ubuntu 22.04) ([#154153](https://github.com/pytorch/pytorch/pull/154153))
- [ez] Replace misleading implementations with NYI ([#154440](https://github.com/pytorch/pytorch/pull/154440))
- Don't CSE unbacked nodes ([#154387](https://github.com/pytorch/pytorch/pull/154387))
- [Intel GPU] convolution fusion at XPU backend ([#154202](https://github.com/pytorch/pytorch/pull/154202))
- Remove outdated CUDA 11 conditions ([#154313](https://github.com/pytorch/pytorch/pull/154313))
- [dynamo, nested graph breaks] small fixes to resume function generation ([#151056](https://github.com/pytorch/pytorch/pull/151056))
- [dynamo, nested graph breaks] refactor codegen to minimize NULL codegen'ing ([#153510](https://github.com/pytorch/pytorch/pull/153510))
- [dynamo, nested graph breaks] remove block stack graph break in output_graph ([#153772](https://github.com/pytorch/pytorch/pull/153772))
- [dynamo, nested graph breaks] add skip_frame debugging function ([#153773](https://github.com/pytorch/pytorch/pull/153773))
- [Typing] Refactor `torch.types.Device` in `torch/cuda/__init__.py` ([#153447](https://github.com/pytorch/pytorch/pull/153447))
- [ez] Make SymNodeImpl comments less misleading ([#154480](https://github.com/pytorch/pytorch/pull/154480))
- Remove empty files from execlude lint rule ([#154483](https://github.com/pytorch/pytorch/pull/154483))
- pymft lint torch/utils/weak.py ([#154484](https://github.com/pytorch/pytorch/pull/154484))
- [AOTI][cutlass backend] Do not remove the cutlass kernel .o file after packaging ([#154155](https://github.com/pytorch/pytorch/pull/154155))
- [nativert] port semaphore to c10 util ([#153504](https://github.com/pytorch/pytorch/pull/153504))
- [Kineto x Insight] Add device to activity type map in pytorch ([#154253](https://github.com/pytorch/pytorch/pull/154253))
- [ca] fix hop_db tests ([#154133](https://github.com/pytorch/pytorch/pull/154133))
- [ca] disable ca for functorch grad and run all HOO tests ([#154147](https://github.com/pytorch/pytorch/pull/154147))
- [dynamo, nested graph breaks] small fixes to resume function generation ([#151056](https://github.com/pytorch/pytorch/pull/151056))
- [dynamo, nested graph breaks] refactor codegen to minimize NULL codegen'ing ([#153510](https://github.com/pytorch/pytorch/pull/153510))
- [dynamo, nested graph breaks] remove block stack graph break in output_graph ([#153772](https://github.com/pytorch/pytorch/pull/153772))
- [dynamo, nested graph breaks] add skip_frame debugging function ([#153773](https://github.com/pytorch/pytorch/pull/153773))
- [BE][Ez]: Denote common types as TypeAlias ([#154527](https://github.com/pytorch/pytorch/pull/154527))
- [ROCm] Updated default workspace for gfx95 ([#153988](https://github.com/pytorch/pytorch/pull/153988))
- Skip test file that doesn't run gradcheck for slow gradcheck ([#154509](https://github.com/pytorch/pytorch/pull/154509))
- [rocm]add device guard when initialize single stream ([#154433](https://github.com/pytorch/pytorch/pull/154433))
- [BE] Remove unused release scripts. Add clarifications for the branch cut process ([#154649](https://github.com/pytorch/pytorch/pull/154649))
- Always set CPU affinity for benchmark jobs ([#154569](https://github.com/pytorch/pytorch/pull/154569))
- deprecate MTIA_WORKLOADD from pytorch ([#154627](https://github.com/pytorch/pytorch/pull/154627))
- [BE]: Improve aten formatter with fmtlib ([#152830](https://github.com/pytorch/pytorch/pull/152830))
- Prevent SAC cache from being kept alive by reference cycle ([#154651](https://github.com/pytorch/pytorch/pull/154651))
- Forward fix for test_frame_traced_hook in internal testing ([#154641](https://github.com/pytorch/pytorch/pull/154641))
- [cuDNN] Allow cudnn attention or flash attention in `test_export.py` regex  ([#154458](https://github.com/pytorch/pytorch/pull/154458))
- Enhance UT on elapsed_time for XPUEvent ([#154494](https://github.com/pytorch/pytorch/pull/154494))
- NativeRT readme ([#154581](https://github.com/pytorch/pytorch/pull/154581))
- [Inductor] Add NaN assert to returned values from generated code ([#154455](https://github.com/pytorch/pytorch/pull/154455))
- [internal] Expose additional metadata to compilation callbacks ([#153596](https://github.com/pytorch/pytorch/pull/153596))
- [Inductor] Add NaN assert to returned values from generated code ([#154455](https://github.com/pytorch/pytorch/pull/154455))
- Add CPython list/tuple tests ([#150790](https://github.com/pytorch/pytorch/pull/150790))
- Re-enable FakeTensor caching for SymInts ([#152662](https://github.com/pytorch/pytorch/pull/152662))
- [nativert] move layout planner settings to torch ([#154668](https://github.com/pytorch/pytorch/pull/154668))
- Add CPython dict tests ([#150791](https://github.com/pytorch/pytorch/pull/150791))
- Fix test_tensorboard when started w/o tensorboard package ([#154709](https://github.com/pytorch/pytorch/pull/154709))
- [BE][Ez]: Enable ClangFormat aten/src/core/Formatting.cpp ([#154719](https://github.com/pytorch/pytorch/pull/154719))
- [BE][Ez]: Update mimalloc submodule to 2.2.3 ([#154720](https://github.com/pytorch/pytorch/pull/154720))
- [Inductor] Add NaN assert to returned values from generated code ([#154455](https://github.com/pytorch/pytorch/pull/154455))
- remove another instance of mtia_workloadd from pytorch ([#154739](https://github.com/pytorch/pytorch/pull/154739))
- [refactor] move materialize_as_graph to _higher_order_ops/utils.py ([#154070](https://github.com/pytorch/pytorch/pull/154070))
- [nativert] move OpKernelKind enum to torch ([#154756](https://github.com/pytorch/pytorch/pull/154756))
- Aten vector default constructors set to 0, add fnmadd and fnmsub ([#154298](https://github.com/pytorch/pytorch/pull/154298))
- [inductor] Add kernel_hash_key to ChoiceCaller ([#154470](https://github.com/pytorch/pytorch/pull/154470))
- [BE] install_triton_wheel.sh update for internal dev ([#154637](https://github.com/pytorch/pytorch/pull/154637))
- Fix typo in vec256 interleave2 ([#154784](https://github.com/pytorch/pytorch/pull/154784))
- [BE]Enhance _get_clean_triton.py to auto-generate launch_params if missing ([#154666](https://github.com/pytorch/pytorch/pull/154666))
- Temporarily disable sparse tensor validation when loading from external storage. ([#154758](https://github.com/pytorch/pytorch/pull/154758))
- [BE][Ez]: Update NVTX submodule to 3.2.1 ([#154797](https://github.com/pytorch/pytorch/pull/154797))
- [BE][Ez]: Unskip conv1d MPS test ([#154795](https://github.com/pytorch/pytorch/pull/154795))
- test for 146431 ([#154786](https://github.com/pytorch/pytorch/pull/154786))
- [ez] delete code that died a long time ago ([#154802](https://github.com/pytorch/pytorch/pull/154802))
- [BE]: Replace a couple of call sites with fmtlib printf ([#154533](https://github.com/pytorch/pytorch/pull/154533))
- [BE]: Update cpp-httplib submodule to 0.20.1 ([#154825](https://github.com/pytorch/pytorch/pull/154825))
- [BE]: Update nlohmann submodule to 3.12.0 ([#154817](https://github.com/pytorch/pytorch/pull/154817))
- Fix flaky test in test_custom_ops ([#152484](https://github.com/pytorch/pytorch/pull/152484))
- [BE] Cleanup old ExecuTorch codegen and runtime code ([#154165](https://github.com/pytorch/pytorch/pull/154165))
- Update README.md - James has the wrong github link. ([#151473](https://github.com/pytorch/pytorch/pull/151473))
- Update slow tests ([#154347](https://github.com/pytorch/pytorch/pull/154347))
- Update lint_urls.sh ([#154838](https://github.com/pytorch/pytorch/pull/154838))
- Bump pocketfft submodule to the latest ([#154845](https://github.com/pytorch/pytorch/pull/154845))
- Symintify repeat_interleave ([#154660](https://github.com/pytorch/pytorch/pull/154660))
- [ez] add dynamic sources docs ([#154826](https://github.com/pytorch/pytorch/pull/154826))
- Add docs for how to mark as unbacked ([#154822](https://github.com/pytorch/pytorch/pull/154822))
- add reference to stances from dynamic shapes doc ([#154823](https://github.com/pytorch/pytorch/pull/154823))
- Vary batch size when running dynamic shapes benchmarks ([#154805](https://github.com/pytorch/pytorch/pull/154805))
- [ez] add docs for *eager_then_compile stances ([#154818](https://github.com/pytorch/pytorch/pull/154818))
- Add CPython exception tests ([#150789](https://github.com/pytorch/pytorch/pull/150789))
- Add CPython math/cmath tests ([#150794](https://github.com/pytorch/pytorch/pull/150794))
- [dynamo] Record the pre-graph bytecode using fast record function event ([#154769](https://github.com/pytorch/pytorch/pull/154769))
- [dynamo][guards] Flush cache to more accurately measure guard overhead ([#154764](https://github.com/pytorch/pytorch/pull/154764))
- [nativert] Free stale execution frames ([#154636](https://github.com/pytorch/pytorch/pull/154636))
- [triton pin][tests] update inductor/profiler launch_(enter|exit)_hooks tests ([#154894](https://github.com/pytorch/pytorch/pull/154894))
- [triton pin][test] relax codecache test checks for number of triton artifacts ([#154879](https://github.com/pytorch/pytorch/pull/154879))
- Remove AttributeError constructor ([#154808](https://github.com/pytorch/pytorch/pull/154808))
- Remove AttributeError constructor ([#154808](https://github.com/pytorch/pytorch/pull/154808))
- [inductor] Add kernel_hash_key to ChoiceCaller ([#154470](https://github.com/pytorch/pytorch/pull/154470))
- Improve error message for `torch.fft.ihfft2` when input's dtype is complex ([#149692](https://github.com/pytorch/pytorch/pull/149692))
- [dynamo][guards] Flush cache to more accurately measure guard overhead ([#154764](https://github.com/pytorch/pytorch/pull/154764))
- Removing per torch.compile audit. ([#154572](https://github.com/pytorch/pytorch/pull/154572))
- Add type annotation to orthogonal_ ([#154927](https://github.com/pytorch/pytorch/pull/154927))
- [typing] Add missing type annotations to torch.nn.init module ([#154504](https://github.com/pytorch/pytorch/pull/154504))
- [AOTDispatch] Use the proper meta function for `_amp_foreach_non_finite_check_and_unscale_` ([#154930](https://github.com/pytorch/pytorch/pull/154930))
- [dynamo] Mark a vt unspecialized nn module variable source earlier ([#154780](https://github.com/pytorch/pytorch/pull/154780))
- [inductor][dynamo] Include operator name in size/stride/alignment assertion ([#152353](https://github.com/pytorch/pytorch/pull/152353))
- Avoid index integer overflow in gemm_notrans_ ([#154809](https://github.com/pytorch/pytorch/pull/154809))
- [1/3] Add header file for Graph in nativert ([#154530](https://github.com/pytorch/pytorch/pull/154530))
- [2/3] Add source file for Graph in nativert ([#154531](https://github.com/pytorch/pytorch/pull/154531))
- [3/3] Add build rule and test for Graph in nativert ([#154532](https://github.com/pytorch/pytorch/pull/154532))
- [Inductor] Add attention pattern for model DistilBert in transformers==4.44.2. ([#154091](https://github.com/pytorch/pytorch/pull/154091))
- [Inductor UT] Reuse test_fused_attention.py for Intel GPU. ([#154110](https://github.com/pytorch/pytorch/pull/154110))
- [dynamo][dynamic] Recompilation hint for nn module integer attributes ([#154867](https://github.com/pytorch/pytorch/pull/154867))
- Add randint_like tensor overload for high ([#154899](https://github.com/pytorch/pytorch/pull/154899))
- [ROCm][Windows] Fix building tests for multiple architectures ([#154979](https://github.com/pytorch/pytorch/pull/154979))
- [Intel GPU] Make SDPA output has the same stride as Query. ([#154340](https://github.com/pytorch/pytorch/pull/154340))
- Skip another test file that doesn't run gradcheck for slow gradcheck ([#154852](https://github.com/pytorch/pytorch/pull/154852))
- Add __main__ guards to jit tests ([#154725](https://github.com/pytorch/pytorch/pull/154725))
- [ROCm] fix CI failures from inductor periodic ([#154896](https://github.com/pytorch/pytorch/pull/154896))
- [AOTInductor] Activate CPU test for package and update weights ([#155078](https://github.com/pytorch/pytorch/pull/155078))
- [export] Use patching in test ([#155132](https://github.com/pytorch/pytorch/pull/155132))
- [HOP] Added clone for outputs of create_bw_fn that are aliasing the inputs ([#153932](https://github.com/pytorch/pytorch/pull/153932))
- Add CPython generator/contextlib tests ([#150796](https://github.com/pytorch/pytorch/pull/150796))
- Add CPython int/float tests ([#150795](https://github.com/pytorch/pytorch/pull/150795))
- SDPA support gfx950 ([#155103](https://github.com/pytorch/pytorch/pull/155103))
- [test][dynamo] skip test_deopt_from_append_list on python>=3.13.3 ([#155167](https://github.com/pytorch/pytorch/pull/155167))
- Inductor unit tests: cuda 12.6 -> 12.8 ([#155056](https://github.com/pytorch/pytorch/pull/155056))
- [reland][dynamo] Record the pre-graph bytecode using fast record function event ([#154974](https://github.com/pytorch/pytorch/pull/154974))
- Fix docstring for `torch.UntypedStorage.from_file` ([#155067](https://github.com/pytorch/pytorch/pull/155067))
- Update docs build to specify <3.13 in CONTRIBUTING ([#155140](https://github.com/pytorch/pytorch/pull/155140))
- [BE] Update cudnn to 9.10.1.4 ([#155122](https://github.com/pytorch/pytorch/pull/155122))
- [dynamo][dynamic] Recompilation hint for nn module integer attributes ([#154867](https://github.com/pytorch/pytorch/pull/154867))
- Inductor unit tests: cuda 12.6 -> 12.8 ([#155056](https://github.com/pytorch/pytorch/pull/155056))
- [cutlass backend] add teraflops and increase rep for benchmark script ([#154944](https://github.com/pytorch/pytorch/pull/154944))
- [scan] disable functionalization key in backward tracing ([#154343](https://github.com/pytorch/pytorch/pull/154343))
- [ROCm][Windows] Fix offload gpu arch list in tests ([#155212](https://github.com/pytorch/pytorch/pull/155212))
- Turn on new tiling by default ([#154768](https://github.com/pytorch/pytorch/pull/154768))
- Add Intel GPU info collection to the collect env script ([#137846](https://github.com/pytorch/pytorch/pull/137846))
- Add Intel GPU info collection to the collect env script ([#137846](https://github.com/pytorch/pytorch/pull/137846))
- Make device check throw specific error ([#155085](https://github.com/pytorch/pytorch/pull/155085))
- [reland][dynamo] Record the pre-graph bytecode using fast record function event ([#154974](https://github.com/pytorch/pytorch/pull/154974))
- Add claude local md files ([#155299](https://github.com/pytorch/pytorch/pull/155299))
- Support detached checkout in tools/nightly.py ([#154314](https://github.com/pytorch/pytorch/pull/154314))
- Add randint_like tensor overload for high ([#154899](https://github.com/pytorch/pytorch/pull/154899))
- pass mempool arg through emptyCache ([#155315](https://github.com/pytorch/pytorch/pull/155315))
- DOC: Convert to markdown: torch.overrides.rst, type_info.rst, utils.rst, xpu.rst ([#155088](https://github.com/pytorch/pytorch/pull/155088))
- [easy][invoke_subgraph] Remove skip from already fixed test ([#155286](https://github.com/pytorch/pytorch/pull/155286))
- Turn on new tiling by default ([#154768](https://github.com/pytorch/pytorch/pull/154768))
- [Docs] Convert to markdown: torch.compiler_troubleshooting_old.rst, torch.compiler.rst ([#155348](https://github.com/pytorch/pytorch/pull/155348))
- [ROCm] Make optional features in LoadHIP better conditioned. ([#155305](https://github.com/pytorch/pytorch/pull/155305))
- [ROCm] Adds initialization support for PyTorch when built from ROCm wheels. ([#155285](https://github.com/pytorch/pytorch/pull/155285))
- SDPA support gfx950 ([#155103](https://github.com/pytorch/pytorch/pull/155103))
- [nativert] move function schema to torch ([#154948](https://github.com/pytorch/pytorch/pull/154948))
- [BE] Cleanup old ExecuTorch codegen and runtime code ([#154165](https://github.com/pytorch/pytorch/pull/154165))
- [Graph Partition] move cpu scalar tensor to gpu ([#154464](https://github.com/pytorch/pytorch/pull/154464))
- [Break XPU] Fix failed test cases which are introduced by community for XPU. ([#155317](https://github.com/pytorch/pytorch/pull/155317))
- [inductor] use int64 for large index ([#154575](https://github.com/pytorch/pytorch/pull/154575))
- Fix some incorrect reST markups in the document ([#154831](https://github.com/pytorch/pytorch/pull/154831))
- Update auto-tuning support for _scaled_grouped_mm ([#150944](https://github.com/pytorch/pytorch/pull/150944))
- [BE] Polish `Makefile` ([#155425](https://github.com/pytorch/pytorch/pull/155425))
- Fix/issue #155027 ([#155252](https://github.com/pytorch/pytorch/pull/155252))
- Document the default garbage_collection_threshold value and improve the organization of cuda docs ([#155341](https://github.com/pytorch/pytorch/pull/155341))
- [nativert] Move Weights to PyTorch core ([#155156](https://github.com/pytorch/pytorch/pull/155156))
- Generate unique id for tensor storage object by observing the week pointer of tensor storage object ([#154859](https://github.com/pytorch/pytorch/pull/154859))
- Update torch-xpu-ops commit pin ([#154962](https://github.com/pytorch/pytorch/pull/154962))
- [aotd] Support mutations in reordering_to_mimic_autograd_engine ([#155353](https://github.com/pytorch/pytorch/pull/155353))
- enable test ([#155342](https://github.com/pytorch/pytorch/pull/155342))
- [BE][Testing] Unskip `ones_like`/`zeros_like` testing on MPS ([#155476](https://github.com/pytorch/pytorch/pull/155476))
- [nativert] Move serialization to PyTorch core ([#155229](https://github.com/pytorch/pytorch/pull/155229))
- [cuBLASLt][cuBLAS] Support 2D bias and `beta != 1.0` in cuBLASLt ([#154170](https://github.com/pytorch/pytorch/pull/154170))
- Move non inductor workflows cuda 12.6->cuda 12.8 ([#155234](https://github.com/pytorch/pytorch/pull/155234))
- Add doc for missing functions for torch.special module ([#155074](https://github.com/pytorch/pytorch/pull/155074))
- [Docs] Convert to markdown: torch.compiler_troubleshooting.rst ([#155351](https://github.com/pytorch/pytorch/pull/155351))
- Convert compiler rst files to markdown ([#155335](https://github.com/pytorch/pytorch/pull/155335))
- Updates to README about CUDA install dir and conda not required ([#155458](https://github.com/pytorch/pytorch/pull/155458))
- Add a stub AGENTS.md for Codex ([#155459](https://github.com/pytorch/pytorch/pull/155459))
- [Easy] update pip sources for CUDA in nightly pull tool ([#149143](https://github.com/pytorch/pytorch/pull/149143))
- [Testing] Add more models to MPSInductor tests ([#155494](https://github.com/pytorch/pytorch/pull/155494))
- [Testing] Add more models to MPSInductor tests ([#155494](https://github.com/pytorch/pytorch/pull/155494))
- [Docs] Convert to markdown: torch.compiler_troubleshooting.rst  ([#155514](https://github.com/pytorch/pytorch/pull/155514))
- [precompile] Add low level C API to load precompiled dynamo code on functions. ([#155329](https://github.com/pytorch/pytorch/pull/155329))
- [BE] Update cudnn to 9.10.1.4 ([#155122](https://github.com/pytorch/pytorch/pull/155122))
- Make open device registration tests standalone ([#153855](https://github.com/pytorch/pytorch/pull/153855))
- [inductor] use int64 for large index ([#154575](https://github.com/pytorch/pytorch/pull/154575))
- Fixes OpInfo gradient checks for ctc_loss ([#154590](https://github.com/pytorch/pytorch/pull/154590))
- [TEST] Modernize test_sort_large ([#155546](https://github.com/pytorch/pytorch/pull/155546))
- Convert to markdown: testing.rst, threading_environment_variables.rst, torch_cuda_memory.rst, torch_environment_variables.rst, torch_nccl_environment_variables.rst  ([#155523](https://github.com/pytorch/pytorch/pull/155523))
- DOC: Convert to markdown: torch.compiler_best_practices_for_backends.rst, torch.compiler_cudagraph_trees.rst, torch.compiler_custom_backends.rst, torch.compiler_dynamic_shapes.rst, torch.compiler_dynamo_deepdive.rst ([#155137](https://github.com/pytorch/pytorch/pull/155137))
- Disable foreach tests that depend on profiler for CUDA 12.6 ([#155596](https://github.com/pytorch/pytorch/pull/155596))
- [nativert] move execution planner to torch ([#155374](https://github.com/pytorch/pytorch/pull/155374))
- [dynamo] added github_cli to detect unimplemented_v2 calls ([#155610](https://github.com/pytorch/pytorch/pull/155610))
- Add Intel GPU info collection to the collect env script ([#137846](https://github.com/pytorch/pytorch/pull/137846))
- Replace TORCH_INTERNAL_ASSERT with TORCH_CHECK in set_history ([#155453](https://github.com/pytorch/pytorch/pull/155453))
- [ez][AOTI] Add test for std::nullopt return in custom op ([#155636](https://github.com/pytorch/pytorch/pull/155636))
- remove redundent type_id ([#155539](https://github.com/pytorch/pytorch/pull/155539))
- [Graph Partition] move cpu scalar tensor to gpu ([#154464](https://github.com/pytorch/pytorch/pull/154464))
- [triton pin][tests] refactor test_triton_kernel.py tests to test new & old API ([#155510](https://github.com/pytorch/pytorch/pull/155510))
- [cuBLASLt][cuBLAS] Support 2D bias and `beta != 1.0` in cuBLASLt ([#154170](https://github.com/pytorch/pytorch/pull/154170))
- set_grad_enabled add str and repr for prints ([#155681](https://github.com/pytorch/pytorch/pull/155681))
- Convert fx.rst to fx.md ([#155482](https://github.com/pytorch/pytorch/pull/155482))
- [Docs] Convert to markdown: torch.compiler_transformations.rst, torch.compiler.config.rst ([#155347](https://github.com/pytorch/pytorch/pull/155347))
- Add types to torch/utils/_triton.py ([#155612](https://github.com/pytorch/pytorch/pull/155612))
- Update auto-tuning support for _scaled_grouped_mm ([#150944](https://github.com/pytorch/pytorch/pull/150944))
- Convert to markdown: export.programming_model.rst ([#155659](https://github.com/pytorch/pytorch/pull/155659))
- [BE]: Update cudnn to 9.10.2.21 ([#155576](https://github.com/pytorch/pytorch/pull/155576))
- remove float args benchmark ([#155674](https://github.com/pytorch/pytorch/pull/155674))
- remove single remaining gso from compute_stride ([#155635](https://github.com/pytorch/pytorch/pull/155635))
- [coreml-backend-tool] fix pytorch-backended  issue on new coremltools ([#155543](https://github.com/pytorch/pytorch/pull/155543))
- [PT2][partitioners] Add aten.split to view_ops list ([#155424](https://github.com/pytorch/pytorch/pull/155424))
- DOC: Convert to markdown: mobile_optimizer.rst, model_zoo.rst, module_tracker.rst, monitor.rst, mps_environment_variables.rst ([#155702](https://github.com/pytorch/pytorch/pull/155702))
- [refactor] make check input alias and mutation easier to use ([#155261](https://github.com/pytorch/pytorch/pull/155261))
- [hop] auto functionalize invoke_subgraph ([#154072](https://github.com/pytorch/pytorch/pull/154072))
- [hop schema] add schema.tree_spec to support pytree inputs ([#154191](https://github.com/pytorch/pytorch/pull/154191))
- [refactor] make do_auto_functionalize_v2 take HopInstance ([#154192](https://github.com/pytorch/pytorch/pull/154192))
- [easy] revert unintended changes from #152579 ([#155614](https://github.com/pytorch/pytorch/pull/155614))
- [cuDNN] Enabled dilation for deterministic convolutions in cuDNN ([#154292](https://github.com/pytorch/pytorch/pull/154292))
- Fix #155022 rst to markdown conversion ([#155540](https://github.com/pytorch/pytorch/pull/155540))
- [nativert] Move Pytree ([#155136](https://github.com/pytorch/pytorch/pull/155136))
- Fix set per proc memory fraction when running tests ([#155631](https://github.com/pytorch/pytorch/pull/155631))
- Move _storage_Use_Count to be gerneric ([#155451](https://github.com/pytorch/pytorch/pull/155451))
- [ca] Annotate AccumulateGrad branching and add polyfill tests ([#155289](https://github.com/pytorch/pytorch/pull/155289))
- [Docs] Convert to markdown: accelerator.rst, amp.rst, autograd.rst, backends.rst, benchmark_utils.rst ([#155762](https://github.com/pytorch/pytorch/pull/155762))
- [BE] Fix warning in open_registration_extension.cpp ([#155755](https://github.com/pytorch/pytorch/pull/155755))
- [nativert] Move DelegateExecutor to PyTorch core ([#155581](https://github.com/pytorch/pytorch/pull/155581))
- Add view_simple as meta function for view,  and avoid calling reshape_view_helper.  ([#154757](https://github.com/pytorch/pytorch/pull/154757))
- [Multiprocesing] Fix `_release_ipc_counter` missing in rebuilding cuda ipc tensor with UntypedStorage ([#155312](https://github.com/pytorch/pytorch/pull/155312))
- Move non inductor workflows cuda 12.6->cuda 12.8 ([#155234](https://github.com/pytorch/pytorch/pull/155234))
- [BE]: Update cudnn to 9.10.2.21 ([#155576](https://github.com/pytorch/pytorch/pull/155576))
- [XPU] Simplify XPU `make triton` by install from PyTorch source ([#155675](https://github.com/pytorch/pytorch/pull/155675))
- Remove C10_DEPRECATED references in c10 ([#151058](https://github.com/pytorch/pytorch/pull/151058))
- fix code indentation for fx.md ([#155764](https://github.com/pytorch/pytorch/pull/155764))
- Basic MTIA ATen CMake ([#155477](https://github.com/pytorch/pytorch/pull/155477))
- [scan] fix doc of scan and list the restrctions. ([#155577](https://github.com/pytorch/pytorch/pull/155577))
- [BE][Testing] Delete some unused code ([#155760](https://github.com/pytorch/pytorch/pull/155760))
- Fix #155020 - rst2markdown for export.rst (split PR) ([#155753](https://github.com/pytorch/pytorch/pull/155753))
- Add a Additional Example that showcases the usage of `torch.autograd.functional.jacobian` ([#155683](https://github.com/pytorch/pytorch/pull/155683))
- pyfmt lint   torch/_custom_op/* ([#155782](https://github.com/pytorch/pytorch/pull/155782))
- fix warning spam for list indexing ([#155815](https://github.com/pytorch/pytorch/pull/155815))
- Replace device check of TORCH_INTERNAL_ASSERT with TORCH_CHECK ([#155318](https://github.com/pytorch/pytorch/pull/155318))
- [Easy] Remove empty file ([#155796](https://github.com/pytorch/pytorch/pull/155796))
- Convert hub.rst to hub.md ([#155483](https://github.com/pytorch/pytorch/pull/155483))
- convert jit_language_reference.rst to jit_language_reference.md ([#155633](https://github.com/pytorch/pytorch/pull/155633))
- Convert rst to markdown - profiler.rst #155031 ([#155559](https://github.com/pytorch/pytorch/pull/155559))
- [cutlass backend] add fp8 to cutlass benchmark script ([#155507](https://github.com/pytorch/pytorch/pull/155507))
- Convert to .md: draft_export.rst, export.ir_spec.rst, fft.rst ([#155567](https://github.com/pytorch/pytorch/pull/155567))
- [symm_mem] Update CMakeList to reflect code moving a dedicated folder ([#155823](https://github.com/pytorch/pytorch/pull/155823))
- update get start xpu ([#151886](https://github.com/pytorch/pytorch/pull/151886))
- [MTIA Aten Backend] Migrate where.self and where.self_out ([#154589](https://github.com/pytorch/pytorch/pull/154589))
- [MTIA Aten Backend] Migrate bitwise_and.Tensor_out ([#154591](https://github.com/pytorch/pytorch/pull/154591))
- [pytorch Aten] Delete unused duplicate clamp_stub, to avoid compile error ([#154631](https://github.com/pytorch/pytorch/pull/154631))
- Move glslc to cas to enable remote execution ([#155832](https://github.com/pytorch/pytorch/pull/155832))
- [nativert] Move graph_passes to nativert ([#155411](https://github.com/pytorch/pytorch/pull/155411))
- [ATen][CUDA][cuSOLVER] Add cusolverDnXsyevBatched for torch.linalg.eigh ([#155695](https://github.com/pytorch/pytorch/pull/155695))
- convert `jit_language_reference_v2.rst` to `jit_language_reference_v2.md` ([#155781](https://github.com/pytorch/pytorch/pull/155781))
- convert: rst to myst pr 1/2 ([#155840](https://github.com/pytorch/pytorch/pull/155840))
- Optimize Tensor.backward type hints ([#155656](https://github.com/pytorch/pytorch/pull/155656))
- [Docs] Convert to markdown cond.rst, config_mod.rst ([#155653](https://github.com/pytorch/pytorch/pull/155653))
- Convert rst to markdown - optim.rst #155031 ([#155813](https://github.com/pytorch/pytorch/pull/155813))
- fix code chunk indentation for `jit_language_reference_v2.md` ([#155937](https://github.com/pytorch/pytorch/pull/155937))
- [CI] Keep going display on HUD: upload log when test fails ([#155371](https://github.com/pytorch/pytorch/pull/155371))
- Fix test after revert  ([#155946](https://github.com/pytorch/pytorch/pull/155946))
- Converting .rst files to .md files  ([#155377](https://github.com/pytorch/pytorch/pull/155377))
- [BE] Move optional submodules checkout to its own module ([#155947](https://github.com/pytorch/pytorch/pull/155947))
- [BE] Refactor clamp dtypes check ([#155930](https://github.com/pytorch/pytorch/pull/155930))
- [BE]: Sync cusparselt 12.9 with static build and other cuda 12 ([#155709](https://github.com/pytorch/pytorch/pull/155709))
- Enable manywheel build and smoke test on main branch for ROCm ([#153287](https://github.com/pytorch/pytorch/pull/153287))
- [BE][CI] Remove hardshrink integer exclusions ([#155965](https://github.com/pytorch/pytorch/pull/155965))
- fix tensor print behavior for MAIA ([#155609](https://github.com/pytorch/pytorch/pull/155609))
- [MTIA Aten Backend] Migrate bitwise_not.out ([#154632](https://github.com/pytorch/pytorch/pull/154632))
- [MTIA Aten Backend] Migrate bitwise_or.Tensor_out ([#154659](https://github.com/pytorch/pytorch/pull/154659))
- [MTIA Aten Backend] Migrate tanh.out and tanh_backward.grad_input ([#155925](https://github.com/pytorch/pytorch/pull/155925))
- [MTIA Aten Backend] Migrate sqrt.out / rsqrt.out / sin.out / silu.out ([#155926](https://github.com/pytorch/pytorch/pull/155926))
- [MTIA Aten Backend] Migrate relu / relu_ ([#155927](https://github.com/pytorch/pytorch/pull/155927))
- [BE] Better uv detection in `pip init` ([#155972](https://github.com/pytorch/pytorch/pull/155972))
- [nativert] move execution frame to torch ([#155830](https://github.com/pytorch/pytorch/pull/155830))
- Convert to markdown: named_tensor.rst, nested.rst, nn.attention.bias.rst, nn.attention.experimental.rst, nn.attention.flex_attention.rst #155028 ([#155696](https://github.com/pytorch/pytorch/pull/155696))
- [OpenReg] add manual_seed related capabilities ([#153947](https://github.com/pytorch/pytorch/pull/153947))
- [Openreg] Split TestOpenReg into two parts ([#154018](https://github.com/pytorch/pytorch/pull/154018))
- [OpenReg][1/N] Migrate cpp_extensions_open_device_registration to OpenReg ([#154019](https://github.com/pytorch/pytorch/pull/154019))
- [OpenReg][2/N] Migrate cpp_extensions_open_device_registration to OpenReg ([#154106](https://github.com/pytorch/pytorch/pull/154106))
- [OpenReg][3/N] Migrate cpp_extensions_open_device_registration to OpenReg ([#154181](https://github.com/pytorch/pytorch/pull/154181))
- [OpenReg][4/N] Migrate cpp_extensions_open_device_registration to OpenReg ([#155101](https://github.com/pytorch/pytorch/pull/155101))
- [OpenReg][5/N] add set_.source_Storage for openreg ([#155191](https://github.com/pytorch/pytorch/pull/155191))
- Add Helion softmax test ([#155976](https://github.com/pytorch/pytorch/pull/155976))
- [Easy][BE] update recommanded VS Code settings ([#152760](https://github.com/pytorch/pytorch/pull/152760))
- assume sparse tensor not coalesced_ gsv -> guard_or_false. ([#155869](https://github.com/pytorch/pytorch/pull/155869))
- use guard_or_false in checkInBoundsForStorage ([#155874](https://github.com/pytorch/pytorch/pull/155874))
- Add api info for torch._C._nn.pyi ([#148405](https://github.com/pytorch/pytorch/pull/148405))
- [torchgen] Fix `ruff format` for `# fmt: skip` comment for function signature ([#155909](https://github.com/pytorch/pytorch/pull/155909))
- [BE][Easy] bump `ruff` to 0.11.13 ([#155914](https://github.com/pytorch/pytorch/pull/155914))
- [BE][Easy] bump `isort` to 6.0.1 ([#155919](https://github.com/pytorch/pytorch/pull/155919))
- [executorch hash update] update the pinned executorch hash ([#153436](https://github.com/pytorch/pytorch/pull/153436))
- avoid gso in has_internal_overlap ([#155870](https://github.com/pytorch/pytorch/pull/155870))
- Unify dynamic shapes APIs naming 2 (expect_true and check) ([#155776](https://github.com/pytorch/pytorch/pull/155776))
- remove duplicated pybind flag in mps code ([#155936](https://github.com/pytorch/pytorch/pull/155936))
- Enable manywheel build and smoke test on main branch for ROCm ([#153287](https://github.com/pytorch/pytorch/pull/153287))
- [cuBLASLt][cuBLAS] Support 2D bias and `beta != 1.0` in cuBLASLt ([#154170](https://github.com/pytorch/pytorch/pull/154170))
- Add__int__ and __float__ methods to _sympy.functions.Identity ([#155873](https://github.com/pytorch/pytorch/pull/155873))
- [test][triton pin] add device-side TMA tests (AOTI + test_triton_kernels) ([#155827](https://github.com/pytorch/pytorch/pull/155827))
- convert: rst to myst pr2/2 ([#155911](https://github.com/pytorch/pytorch/pull/155911))
- Add __main__ guards to jit tests ([#154725](https://github.com/pytorch/pytorch/pull/154725))
- [BE] Remove non-existing operator from unimplemented list ([#156025](https://github.com/pytorch/pytorch/pull/156025))
- [MPS][Testing][BE] Fix samples for full_like ([#156026](https://github.com/pytorch/pytorch/pull/156026))
- [BE] add `codespell` linter ([#156066](https://github.com/pytorch/pytorch/pull/156066))
- [BE] fix typos in top-level files ([#156067](https://github.com/pytorch/pytorch/pull/156067))
- Convert complex_numbers.rst to markdown ([#156039](https://github.com/pytorch/pytorch/pull/156039))
- Convert to markdown: checkpoint.rst ([#156009](https://github.com/pytorch/pytorch/pull/156009))
- [BE][setup] gracefully handle envvars representing a boolean in `setup.py` ([#156040](https://github.com/pytorch/pytorch/pull/156040))
- [doc] Add documentation for division by zero behavior in autograd ([#155987](https://github.com/pytorch/pytorch/pull/155987))
- [PT2][partitioners] Add aten.split to view_ops list [relanding #155424] ([#155943](https://github.com/pytorch/pytorch/pull/155943))
- Remove non-header-only dep from c10_headers target ([#155858](https://github.com/pytorch/pytorch/pull/155858))
- [nativert] Move OpKernel to PyTorch core ([#156011](https://github.com/pytorch/pytorch/pull/156011))
- [Accelerator] Fix Python typing in accelerator ([#152394](https://github.com/pytorch/pytorch/pull/152394))
- [MPS][Testing][BE] Fix samples for full_like ([#156026](https://github.com/pytorch/pytorch/pull/156026))
- [invoke_subgraph] Ignore redundantly nested invoke_subgraph ([#155828](https://github.com/pytorch/pytorch/pull/155828))
- Use CMAKE_COLOR_DIAGNOSTICS ([#154583](https://github.com/pytorch/pytorch/pull/154583))
- [Intel GPU] Enable GQA and different head_dim of value for SDPA ([#150992](https://github.com/pytorch/pytorch/pull/150992))
- [BE][setup] allow passing pytorch-specific `setup.py` options from envvars ([#156041](https://github.com/pytorch/pytorch/pull/156041))
- [BE] fix typos in benchmarks/ ([#156077](https://github.com/pytorch/pytorch/pull/156077))
- [BE] Refactor functions from optional_submodules ([#155954](https://github.com/pytorch/pytorch/pull/155954))
- [Docs] Convert to markdown to fix 155025 ([#155789](https://github.com/pytorch/pytorch/pull/155789))
- [CI] Do not constrain memory for ROCm testing in CI ([#156115](https://github.com/pytorch/pytorch/pull/156115))
- [inductor][cutlass] binary remote cache ([#156106](https://github.com/pytorch/pytorch/pull/156106))
- Update CODEOWNERS ([#156182](https://github.com/pytorch/pytorch/pull/156182))
- Replace all RAIIATH with Tensor in libtorch_agnostic test, test some APIs ([#155977](https://github.com/pytorch/pytorch/pull/155977))
- [CI][run_test] Fix rerun logic for failing at exit ([#155853](https://github.com/pytorch/pytorch/pull/155853))
- Fix issue with right-nav ([#156119](https://github.com/pytorch/pytorch/pull/156119))
- [ROCm] enable batched eigen decomposition (syevD_batched) on ROCm ([#154525](https://github.com/pytorch/pytorch/pull/154525))
- [BE] fix typos in cmake/ ([#156079](https://github.com/pytorch/pytorch/pull/156079))
- [BE] fix typos in torchgen/ ([#156083](https://github.com/pytorch/pytorch/pull/156083))
- [Docs] Convert to markdown to fix 155032 ([#155520](https://github.com/pytorch/pytorch/pull/155520))
- Pass by const ref instead of by value in StableIValue from ([#156126](https://github.com/pytorch/pytorch/pull/156126))
- [Break XPU] Fix XPU UT failures introduced by community. ([#156091](https://github.com/pytorch/pytorch/pull/156091))
- [Build] Allow metal shaders to include ATen headers ([#156256](https://github.com/pytorch/pytorch/pull/156256))
- Convert rst to md: rpc.rst,  signal.rst,  size.rst, special.rst ([#155430](https://github.com/pytorch/pytorch/pull/155430))
- Improve IPC for Expandable Segments to use fabric handle when possible ([#156074](https://github.com/pytorch/pytorch/pull/156074))
- [dynamo] control one_graph behavior additionally through config ([#154283](https://github.com/pytorch/pytorch/pull/154283))
- [dynamo] fix set_fullgraph for nested calls ([#154782](https://github.com/pytorch/pytorch/pull/154782))
- [dynamo] handle fullgraph toggle using nested torch.compile ([#155166](https://github.com/pytorch/pytorch/pull/155166))
- [dynamo] raise hard error if error is encountered while tracing resume function prologue ([#154564](https://github.com/pytorch/pytorch/pull/154564))
- [BE] fix typos in c10/ ([#156078](https://github.com/pytorch/pytorch/pull/156078))
- [Kineto][submodule] Update kineto pin for XPU toggle feature  ([#155488](https://github.com/pytorch/pytorch/pull/155488))
- [PT2][partitioners] raise getitems in partitioners to allow earlier release of buffers ([#155809](https://github.com/pytorch/pytorch/pull/155809))
- Refine alignment check along dynamic dimension for grouped MMs ([#155466](https://github.com/pytorch/pytorch/pull/155466))
- Remove legacy export testing path ([#156093](https://github.com/pytorch/pytorch/pull/156093))
- Convert to markdown: quantization-accuracy-debugging.rst, quantization-backend-configuration.rst, quantization-support.rst, random.rst ([#155520](https://github.com/pytorch/pytorch/pull/155520))
- fix: avoid flamegraph script setup conflicts ([#156310](https://github.com/pytorch/pytorch/pull/156310))
- [PT2]load dense delta by trimming prefixes ([#155872](https://github.com/pytorch/pytorch/pull/155872))
- [MTIA Aten Backend] Achieve CPU fallback by overriding registration ([#155634](https://github.com/pytorch/pytorch/pull/155634))
- [MTIA Aten Backend] Migrate nan_to_num.out ([#156046](https://github.com/pytorch/pytorch/pull/156046))
- [MTIA Aten Backend] Migrate remainder.Tensor_out / reciprocal.out / neg.out ([#156047](https://github.com/pytorch/pytorch/pull/156047))
- [MTIA Aten Backend] Migrate logical_or.out / log.out / log2.out ([#156283](https://github.com/pytorch/pytorch/pull/156283))
- [MTIA Aten Backend] Migrate logit ([#156284](https://github.com/pytorch/pytorch/pull/156284))
- [MTIA Aten Backend] Migrate lt.Tensor_out / lt.Scalar_out ([#156285](https://github.com/pytorch/pytorch/pull/156285))
- [nativert] move layoutplanneralgorithm to libtorch ([#156205](https://github.com/pytorch/pytorch/pull/156205))
- [MTIA Aten Backend] Migrate logical_and.out ([#156286](https://github.com/pytorch/pytorch/pull/156286))
- [nativert] session state ([#156190](https://github.com/pytorch/pytorch/pull/156190))
- add a corner test case of dynamic sizes for combo kernel ([#156035](https://github.com/pytorch/pytorch/pull/156035))
- [ROCm] support CUDA_KERNEL_ASSERT using abort() ([#155262](https://github.com/pytorch/pytorch/pull/155262))
- Context on torch.cuda.memory._record_memory_history max_entries ([#155889](https://github.com/pytorch/pytorch/pull/155889))
- [build] Create target for flash attention ([#156235](https://github.com/pytorch/pytorch/pull/156235))
- Add view_simple as meta function for view,  and avoid calling reshape_view_helper for unbacked ([#154757](https://github.com/pytorch/pytorch/pull/154757))
- [InductorBench] Fix accuracy validation logic for MPS ([#156385](https://github.com/pytorch/pytorch/pull/156385))
- [Makefile] lazily setup `lintrunner` on first `make lint` run ([#156058](https://github.com/pytorch/pytorch/pull/156058))
- [bugfix] [build] guard cuda version for ipc with fabric handle ([#156394](https://github.com/pytorch/pytorch/pull/156394))
- update codebase structure documentation to include mps ([#156297](https://github.com/pytorch/pytorch/pull/156297))
- [nativert] Move c10_kernel ([#156208](https://github.com/pytorch/pytorch/pull/156208))
- tools/nightly.py: use `uv pip install` instead of `pip install` ([#156408](https://github.com/pytorch/pytorch/pull/156408))
- tools/nightly.py: only download `torch` via pip and install dependenices via `uv` ([#156409](https://github.com/pytorch/pytorch/pull/156409))
- [build] Create target for flash attention ([#156235](https://github.com/pytorch/pytorch/pull/156235))
- [ROCm] Bump AOTriton to 0.10b ([#156290](https://github.com/pytorch/pytorch/pull/156290))
- [nativert] Move GraphExecutorBase to PyTorch core ([#156196](https://github.com/pytorch/pytorch/pull/156196))
- Forward fix inductor benchmark after #150287 ([#156455](https://github.com/pytorch/pytorch/pull/156455))
- [BE] fix `PYPROJECT` linting errors in `test/` and `tools/` ([#156021](https://github.com/pytorch/pytorch/pull/156021))
- [dynamo] control one_graph behavior additionally through config ([#154283](https://github.com/pytorch/pytorch/pull/154283))
- [dynamo] fix set_fullgraph for nested calls ([#154782](https://github.com/pytorch/pytorch/pull/154782))
- [dynamo] handle fullgraph toggle using nested torch.compile ([#155166](https://github.com/pytorch/pytorch/pull/155166))
- [dynamo] raise hard error if error is encountered while tracing resume function prologue ([#154564](https://github.com/pytorch/pytorch/pull/154564))
- [BE][Easy] do not install yanked `types-pkg-resources` in lint environment ([#156462](https://github.com/pytorch/pytorch/pull/156462))
- Update index.md ([#155143](https://github.com/pytorch/pytorch/pull/155143))
- Refine alignment check along dynamic dimension for grouped MMs ([#155466](https://github.com/pytorch/pytorch/pull/155466))
- [nativert] Move auto_functionalize_kernel ([#156454](https://github.com/pytorch/pytorch/pull/156454))
- skip flaky test in CPython 3.13 tests ([#155561](https://github.com/pytorch/pytorch/pull/155561))
- [invoke_subgraph] Make invoke_subgraph cacheable ([#156448](https://github.com/pytorch/pytorch/pull/156448))
- [nativert] Move SerialGraphExecutor to PyTorch core ([#156459](https://github.com/pytorch/pytorch/pull/156459))
- [BE] fix typos in functorch/ and scripts/ ([#156081](https://github.com/pytorch/pytorch/pull/156081))
- Split the exclude pattern for `CODESPELL` linter ([#156229](https://github.com/pytorch/pytorch/pull/156229))
- [MTIA Aten Backend][3/n] Migrate mm.out from out-of-tree to in-tree ([#154393](https://github.com/pytorch/pytorch/pull/154393))
- Add missing `inline namespace CPU_CAPABILITY` to Gelu/Elu.h ([#156512](https://github.com/pytorch/pytorch/pull/156512))
- [nativert] move layout planner algorithms to libtorch ([#156508](https://github.com/pytorch/pytorch/pull/156508))
- remove make_fast_binary_impl from make_fast_binary_impl ([#156528](https://github.com/pytorch/pytorch/pull/156528))
- [logging] dynamo_timed for CachingAutotuner.coordinate_descent_tuning ([#156517](https://github.com/pytorch/pytorch/pull/156517))
- [ca] skip on some PYTORCH_TEST_WITH_DYNAMO=1 autograd tests ([#156374](https://github.com/pytorch/pytorch/pull/156374))
- [PT2][partitioners] raise getitems in partitioners to allow earlier release of buffers ([#155809](https://github.com/pytorch/pytorch/pull/155809))
- [CUDA] Skip test on low vram machines ([#156548](https://github.com/pytorch/pytorch/pull/156548))
- Make build-deps drop builds into current venv again ([#156200](https://github.com/pytorch/pytorch/pull/156200))
- Use official CUDAToolkit module in CMake ([#154595](https://github.com/pytorch/pytorch/pull/154595))
- [BE][4/16] fix typos in torch/ (torch/_dynamo/) ([#156314](https://github.com/pytorch/pytorch/pull/156314))
- [BE][7/16] fix typos in torch/ (torch/csrc/) ([#156317](https://github.com/pytorch/pytorch/pull/156317))
- [dynamo] fixes to lru_cache message and adding user stack trace in debug mode ([#156463](https://github.com/pytorch/pytorch/pull/156463))
- [OpenReg][1/N] Migrate cpp_extensions_open_device_registration to OpenReg ([#156400](https://github.com/pytorch/pytorch/pull/156400))
- [OpenReg][2/N] Migrate cpp_extensions_open_device_registration to OpenReg ([#156401](https://github.com/pytorch/pytorch/pull/156401))
- [BE][4/16] fix typos in torch/ (torch/_dynamo/) ([#156314](https://github.com/pytorch/pytorch/pull/156314))
- [BE][7/16] fix typos in torch/ (torch/csrc/) ([#156317](https://github.com/pytorch/pytorch/pull/156317))
- Enable Leak Sanitizer ([#154584](https://github.com/pytorch/pytorch/pull/154584))
- Simplify nvtx3 CMake handling, always use nvtx3 ([#153784](https://github.com/pytorch/pytorch/pull/153784))
- Use CMake wholearchive group ([#156393](https://github.com/pytorch/pytorch/pull/156393))
- Add DeviceAllocator as the base device allocator ([#138222](https://github.com/pytorch/pytorch/pull/138222))
- [invoke_subgraph] make same subgraph share get_attr target ([#156260](https://github.com/pytorch/pytorch/pull/156260))
- [invoke_subgraph] make collect_meta_analysis fake prop cachable ([#156347](https://github.com/pytorch/pytorch/pull/156347))
- [torchbench] update environment setup script ([#156465](https://github.com/pytorch/pytorch/pull/156465))
- Move code out of individual token linters ([#152256](https://github.com/pytorch/pytorch/pull/152256))
- [nativert] Move HigherOrderKernel ([#156507](https://github.com/pytorch/pytorch/pull/156507))
- [MTIA Aten Backend] Migrate maximum.out / minimum.out / cos.out / erf.out / exp.out ([#156502](https://github.com/pytorch/pytorch/pull/156502))
- [MTIA Aten Backend] Migrate _log_softmax.out / _log_softmax_backward_data.out ([#156539](https://github.com/pytorch/pytorch/pull/156539))
- [MTIA Aten Backend] Migrate isnan ([#156554](https://github.com/pytorch/pytorch/pull/156554))
- [MTIA Aten Backend] Migrate max.dim_max / min.dim_min ([#156568](https://github.com/pytorch/pytorch/pull/156568))
- Fix native static dispatch kernels ([#156331](https://github.com/pytorch/pytorch/pull/156331))
- [partitioner] Fix _broadcast_on_rank0 to use deterministic hash function ([#153734](https://github.com/pytorch/pytorch/pull/153734))
- [nativert] reland D76832891 remove designated initializer cpp20 ([#156565](https://github.com/pytorch/pytorch/pull/156565))
- Add fx_graph_runnable tests boilerplate ([#156552](https://github.com/pytorch/pytorch/pull/156552))
- Deprecate c10::string ([#155084](https://github.com/pytorch/pytorch/pull/155084))
- [ROCm][Windows] Fix rocsolver undefined symbol error  ([#156591](https://github.com/pytorch/pytorch/pull/156591))
- remove deprecated numpy.typing.mypy_plugin in  mypy.ini  ([#156601](https://github.com/pytorch/pytorch/pull/156601))
- Replace deprecated `is_compiling` method ([#154476](https://github.com/pytorch/pytorch/pull/154476))
- Fix UT failure on non-cuda backend ([#156577](https://github.com/pytorch/pytorch/pull/156577))
- [Reland] [Intel GPU] Make SDPA output has the same stride as Query. ([#154340](https://github.com/pytorch/pytorch/pull/154340))
- Fix TORCH_CUDA_ARCH_LIST ([#156667](https://github.com/pytorch/pytorch/pull/156667))
- Validate custom op support for compile_kernel ([#156332](https://github.com/pytorch/pytorch/pull/156332))
- simplify nvrtc discovery login in compile_kernel ([#156674](https://github.com/pytorch/pytorch/pull/156674))
### security
