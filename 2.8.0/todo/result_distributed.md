
# Release Notes worksheet distributed

The main goal of this process is to rephrase all the commit messages below to make them **clear and easy to read** by the end user. You should follow the following instructions to do so:

* **Please clean up and format commit titles to be readable by the general PyTorch user.** Make sure you're [following the guidance here](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit)! Your resulting notes must be consistent and easy to read.
* Please sort commits into the following categories (you should not rename the categories!), I tried to pre-sort these to ease your work, feel free to move commits around if the current categorization is not good.
* Anything that is not public facing needs to be removed.
* If anything is miscategorized/belongs to another domain, move it to `miscategorized.md`.
* Please scan through `miscategorized.md` and handle any commits that belong within your domain according to these instructions.
* We place a lot of emphasis on the “BC-breaking” and “deprecation” sections. Those should be where the most effort goes in. The “improvements” and “bug fixes” for Python API should be nice as well.
* Once you are finished, move this very file from `todo/` to `done/` and submit a pull request.

The categories below are as follows:

* BC breaking: All commits that are BC-breaking. These are the most important commits. If any pre-sorted commit is actually BC-breaking, do move it to this section. Each commit should contain a paragraph explaining the rational behind the change as well as an example for how to update user code [BC-Guidelines](https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit#heading=h.a9htwgvvec1m).
* Deprecations: All commits introducing deprecation. Each commit should include a small example explaining what should be done to update user code.
* new_features: All commits introducing a new feature (new functions, new submodule, new supported platform etc)
* improvements: All commits providing improvements to existing feature should be here (new backend for a function, new argument, better numerical stability)
* bug fixes: All commits that fix bugs and behaviors that do not match the documentation
* performance: All commits that are added mainly for performance (we separate this from improvements above to make it easier for users to look for it)
* documentation: All commits that add/update documentation
* Developers: All commits that are not end-user facing but still impact people that compile from source, develop into pytorch, extend pytorch, etc
* not user facing: All commits that are not public end-user facing and hence should be dropped from the release notes

## distributed
### bc breaking
### deprecation
### new features
- c10d
  - Added a collective time estimator for NCCL comms ([#149343](https://github.com/pytorch/pytorch/pull/149343))
  - Added support ReduceOp::AVG in `ProcessGroupGloo` ([#149781](https://github.com/pytorch/pytorch/pull/149781))
  - Added support `reduce_scatter` + updated support chart in `ProcessGroupGloo` ([#149869](https://github.com/pytorch/pytorch/pull/149869))
  - Added `clone` feature for tcpstore (#150966) (#150966) ([#151045](https://github.com/pytorch/pytorch/pull/151045))
  - Added `queues` for tcpstore ([#150969](https://github.com/pytorch/pytorch/pull/150969))
  - Added `_allgather_base` , `reduce_scatter` , and `_reduce_scatter_base` into ProcessGroupMPI to enable FSDP with MPI backend ([#150162](https://github.com/pytorch/pytorch/pull/150162))
  - Added nonblocking mode to `queue_pop` for tcpstore ([#151485](https://github.com/pytorch/pytorch/pull/151485))
  - Added api to enable/disable NaN detector per-PG ([#151723](https://github.com/pytorch/pytorch/pull/151723))
  - Added FP8 support in `ProcessGroupNCCL` ([#152706](https://github.com/pytorch/pytorch/pull/152706))
  - Added `ibverbs` backend in gloo ([#153015](https://github.com/pytorch/pytorch/pull/153015), [#153425](https://github.com/pytorch/pytorch/pull/153425))
  - Enabled Gloo CUDA when used with a backend that supports GPUDirect ([#153406](https://github.com/pytorch/pytorch/pull/153406))
### improvements
- c10d
  - Made `getDefaultBackend` more fault tolerant without relying on exceptions ([#149152](https://github.com/pytorch/pytorch/pull/149152))
  - Update error message in `get_backend()` with more details ([#141796](https://github.com/pytorch/pytorch/pull/141796))
  - Specified the default PyTorch Distributed backend for MPS ([#149538](https://github.com/pytorch/pytorch/pull/149538))
  - Supported `masterListenFd` in `TCPStoreLibUvBackend` ([#150215](https://github.com/pytorch/pytorch/pull/150215))
  - Used shared Stores in gloo ([#150230](https://github.com/pytorch/pytorch/pull/150230))
  - Improved FR dump robustness with all watchdog broadcast wait and more frequent store check ([#150652](https://github.com/pytorch/pytorch/pull/150652))
  - Implemented safer book-keeping of NCCL communicators ([#150681](https://github.com/pytorch/pytorch/pull/150681))
  - Clarified behavior of `TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK` ([#150682](https://github.com/pytorch/pytorch/pull/150682))
  - Registered also future allocations in mempool with NCCL ([#150684](https://github.com/pytorch/pytorch/pull/150684))
  - Added the record of each individual collective being coalesced in FR ([#151238](https://github.com/pytorch/pytorch/pull/151238))
  - Added counters for FR dump and reduce its timeout to finish dump before watchdog timeout ([#151329](https://github.com/pytorch/pytorch/pull/151329))
  - Avoided computing `global_rank` when `group_rank` is used ([#151373](https://github.com/pytorch/pytorch/pull/151373))
  - Exposed NCCL communicator from `ProcessGroupNCCL` via an unsafe API ([#152496](https://github.com/pytorch/pytorch/pull/152496))
  - Added split sizes info dump for uneven all2all bw calculation ([#151438](https://github.com/pytorch/pytorch/pull/151438))
  - Made FR vendor neutral so that other backends can use it ([#152585](https://github.com/pytorch/pytorch/pull/152585), [#152563](https://github.com/pytorch/pytorch/pull/152563), [#154929](https://github.com/pytorch/pytorch/pull/154929))
  - Integrated vendor generic FR into gloo ([#152614](https://github.com/pytorch/pytorch/pull/152614))
  - Added `needs_contiguous_strides` tag in functional collective ([#153399](https://github.com/pytorch/pytorch/pull/153399), [#153523](https://github.com/pytorch/pytorch/pull/153523))
  - Allowed `split_group` to work with non-nccl backends ([#152175](https://github.com/pytorch/pytorch/pull/152175))
  - Simplified `new_subgroups()` by using `new_subgroups_by_enumeration()` ([#153843](https://github.com/pytorch/pytorch/pull/153843))
  - Made only current thread allocate to pool in `ProcessGroupNCCL` ([#153990](https://github.com/pytorch/pytorch/pull/153990))
  - Enabled using c10::Half for gloo ([#153862](https://github.com/pytorch/pytorch/pull/153862))
  - Released GIL in PG destructor ([#154976](https://github.com/pytorch/pytorch/pull/154976))
  - Enhanced `get_process_group_ranks()` to accept `group=None` ([#154902](https://github.com/pytorch/pytorch/pull/154902))
  - Skipped updating the default device distributed backend if already registered ([#155320](https://github.com/pytorch/pytorch/pull/155320))
  - Shrinked the range of mutex lock to avoid deadlock ([#155949](https://github.com/pytorch/pytorch/pull/155949))
  - Enabled querying the build and runtime NCCL versions ([#156305](https://github.com/pytorch/pytorch/pull/156305))
  - Disabled NCCL NVLS when using deterministic mode ([#156381](https://github.com/pytorch/pytorch/pull/156381))
  - Made `init_process_group` support index-only device id ([#156214](https://github.com/pytorch/pytorch/pull/156214))
- DeviceMesh
  - Improved device selection logic ([#150897](https://github.com/pytorch/pytorch/pull/150897))
- DistributedDataParallel (DDP)
  - Added one option to allow skipping all reduce unused parameters ([#151503](https://github.com/pytorch/pytorch/pull/151503))
  - Added check on received data to avoid segfault in the DDP reducer ([#152143](https://github.com/pytorch/pytorch/pull/152143))
  - Propagated `use_python_reducer` to C++ reducer ([#152735](https://github.com/pytorch/pytorch/pull/152735))
- DistributedStateDict (DSD)
  - Created and sent `full_tensor` on `ProcessGroup`-supported device in `_broadcast_tensors` ([#148865](https://github.com/pytorch/pytorch/pull/148865))
  - Supported non-tensor-data `write_size` in planner write items. ([#149699](https://github.com/pytorch/pytorch/pull/149699))
  - Switched to `_apply_to_tensors` for dataclass input ([#154897](https://github.com/pytorch/pytorch/pull/154897))
  - Not pop tensors if they are on Meta device ([#153185](https://github.com/pytorch/pytorch/pull/153185))
- DTensor
  - Added more generically support `CompositeImplicitAutograd` ops under inference mode ([#149514](https://github.com/pytorch/pytorch/pull/149514))
  - Made `StridedShard` support uneven sharding ([#150490](https://github.com/pytorch/pytorch/pull/150490))
  - Added op support for `torch.cumsum` ([#151071](https://github.com/pytorch/pytorch/pull/151071))
  - Added add op support for `torch._grouped_mm` ([#151072](https://github.com/pytorch/pytorch/pull/151072))
  - Added DTensor `redistribute` fwd/bwd datatype conversion to enable SimpleFSDP mixed precision training ([#150740](https://github.com/pytorch/pytorch/pull/150740))
  - Added errors on illegal view op during sharding prop ([#149764](https://github.com/pytorch/pytorch/pull/149764))
  - Added rich support to `torch.distributed.tensor.debug.visualize_sharding` ([#152027](https://github.com/pytorch/pytorch/pull/152027))
- FullyShardedDataParallel2 (FSDP2)
  - Added PrivateUse1 backend in fsdp collecitves ([#147260](https://github.com/pytorch/pytorch/pull/147260))
  - Added `set_reshard_after_forward` ([#149103](https://github.com/pytorch/pytorch/pull/149103))
  - Added `privateuse1` device type to pre forward hook of fsdp ([#149487](https://github.com/pytorch/pytorch/pull/149487))
  - Allowed different dtypes for no grad model params ([#154103](https://github.com/pytorch/pytorch/pull/154103))
  - Respected `reshard_after_forward=True` for root model ([#154704](https://github.com/pytorch/pytorch/pull/154704))
  - Kept root unsharded when not specifying `reshard_after_forward` ([#155319](https://github.com/pytorch/pytorch/pull/155319))
  - Allowed forcing FSDP2 to always use SUM reductions ([#155915](https://github.com/pytorch/pytorch/pull/155915))
  - Made assert on all_reduce_event only if it's not CPU device. ([#150316](https://github.com/pytorch/pytorch/pull/150316))
- Pipeline Parallelism
  - Added schedule visualizer ([#150347](https://github.com/pytorch/pytorch/pull/150347))
  - Allowed unused kwargs in ZB path ([#153498](https://github.com/pytorch/pytorch/pull/153498))
  - Added `get_pipeline_order()` for Gpipe and 1F1B ([#155935](https://github.com/pytorch/pytorch/pull/155935))
- ShardedTensor
  - Added support for 0 size shardedTensor and recalculated metadata from `all_gather` ([#152583](https://github.com/pytorch/pytorch/pull/152583))
- TensorParallel
  - Added `repr` methods for `ParallelStyle`s ([#149478](https://github.com/pytorch/pytorch/pull/149478))
  - Added a `ParallelStyle PrepareModuleInputOutput` ([#150372](https://github.com/pytorch/pytorch/pull/150372))
- torchelastic
  - No shutdown of rendezvous on leaving workers ([#152525](https://github.com/pytorch/pytorch/pull/152525))
### bug fixes
- c10d
  - Fixed extra CUDA context created by barrier ([#149144](https://github.com/pytorch/pytorch/pull/149144))
  - Fixed the logic to use group rank instead of global rank when possible ([#149488](https://github.com/pytorch/pytorch/pull/149488))
  - Fixed ET trace collection of all_to_all ([#149485](https://github.com/pytorch/pytorch/pull/149485))
  - Disabled start event recording for coalesced col and improved profile title ([#150863](https://github.com/pytorch/pytorch/pull/150863))
  - Fixed connection reset caused by wrong socket close in tcp store ([#150987](https://github.com/pytorch/pytorch/pull/150987))
  - Added back correct EOF case check in the libuv backend of TCPStore ([#151052](https://github.com/pytorch/pytorch/pull/151052))
  - Fixed unused `group` input argument in `new_subgroups()` ([#152765](https://github.com/pytorch/pytorch/pull/152765))
  - Fixed `new_subgroups(group=)` bug ([#153798](https://github.com/pytorch/pytorch/pull/153798))
  - Fixed tcp init when using port 0 ([#154156](https://github.com/pytorch/pytorch/pull/154156))
  - Adopted a vector to temporarily keep the reference to future object to avoid block ([#156653](https://github.com/pytorch/pytorch/pull/156653))
- DistributedDataParallel (DDP)
  - Fixed DDPOptimizer issue on static tensor index ([#155746](https://github.com/pytorch/pytorch/pull/155746))
- DTensor
  - Fixed `local_map` with multi-threading ([#149070](https://github.com/pytorch/pytorch/pull/149070))
  - Fixed `new_local_tensor` in `redistribute` be None case ([#152303](https://github.com/pytorch/pytorch/pull/152303))
  - Fixed bug visualizing 1D Tensor using rich ([#152871](https://github.com/pytorch/pytorch/pull/152871))
- Pipeline Parallelism
  - Optimized memory usage by releasing output memory earlier ([#153383](https://github.com/pytorch/pytorch/pull/153383))
- RPC
  - Made torch importable if compiled without TensorPipe ([#154382](https://github.com/pytorch/pytorch/pull/154382))
- ShardedTensor
  - Fixed sharded tensor `gather` when a local tensor on certain ranks has zero elements ([#150914](https://github.com/pytorch/pytorch/pull/150914))

### performance
- c10d
  - Added support of `lazy_init` in `ProcessGroupGloo` (#150801)" ([#151031](https://github.com/pytorch/pytorch/pull/151031))
### docs
- c10d
  - Documented object collectives limitations ([#150815](https://github.com/pytorch/pytorch/pull/150815))
  - Updated `NCCLConfig` with QOS variable ([#151821](https://github.com/pytorch/pytorch/pull/151821))
- FullyShardedDataParallel2 (FSDP2)
  - Updated `ignored_params` docstring and added unit tests ([#149074](https://github.com/pytorch/pytorch/pull/149074))
  - Added warning that `reshard_after_forward` = 1 and True are different ([#149750](https://github.com/pytorch/pytorch/pull/149750))
  - Added pointer to torchtitan ([#153079](https://github.com/pytorch/pytorch/pull/153079))
  - Add warning for incorrected grad results at world size 1 ([#154928](https://github.com/pytorch/pytorch/pull/154928))
### devs
- c10d
  - Added param recording for uniqueID broadcasting and allgather ([#149166](https://github.com/pytorch/pytorch/pull/149166))
  - Added logger config for flight record in PGNCCL ([#150356](https://github.com/pytorch/pytorch/pull/150356))
  - Added logging for desync debug report ([#150513](https://github.com/pytorch/pytorch/pull/150513))
  - Surfaced error type when we unlink and create named pipe for DumpPipe ([#150648](https://github.com/pytorch/pytorch/pull/150648))
  - Added logging of `nccl_version` into fr and its dump ([#151048](https://github.com/pytorch/pytorch/pull/151048))
  - Added logging after FR dump completed ([#152648](https://github.com/pytorch/pytorch/pull/152648))
  - Improved the logs on remote shutdown of tcpstore ([#153586](https://github.com/pytorch/pytorch/pull/153586))
  - Enhanced Error Logging in `new_subgroups()` for Non-Divisible World Sizes ([#154124](https://github.com/pytorch/pytorch/pull/154124))
  - Added the log of thread name and thread id into fr ([#155142](https://github.com/pytorch/pytorch/pull/155142))
  - Added log when fr dump triggered from pipe in `ProcessGroupNCCL` ([#155754](https://github.com/pytorch/pytorch/pull/155754))
  - Added a logger for all nccl collectives with its time duration when completed ([#156008](https://github.com/pytorch/pytorch/pull/156008))
- FullyShardedDataParallel (FSDP1)
  - Printed fqns when debug `FlatParamHandle` ([#151336](https://github.com/pytorch/pytorch/pull/151336))
- FullyShardedDataParallel2 (FSDP2)
  - Added FSDP2 logging ([#155826](https://github.com/pytorch/pytorch/pull/155826))
- RPC
  - Correctly passed exceptions raised from `rpc_init` to CPython ([#154325](https://github.com/pytorch/pytorch/pull/154325))
- torchelastic
  - Added the logging of start of torch elastic workers. ([#150849](https://github.com/pytorch/pytorch/pull/150849))
  - Passed event log handler to record function calls ([#155457](https://github.com/pytorch/pytorch/pull/155457))
  - Added torch.distributed.run option to provide destination for event logging (#154644) ([#155268](https://github.com/pytorch/pytorch/pull/155268))
### Untopiced
### not user facing
- [DTensor][tp] fix errors in FSDP+TP checkpointing test ([#150354](https://github.com/pytorch/pytorch/pull/150354))
- remove allow-untyped-defs from elastic_distributed_sampler.py ([#154620](https://github.com/pytorch/pytorch/pull/154620))
- remove allow-untyped-defs from torch/distributed/elastic/utils/logging.py ([#154625](https://github.com/pytorch/pytorch/pull/154625))
- Fix #155018 (convert distributed rst to markdown) ([#155528](https://github.com/pytorch/pytorch/pull/155528))
- [symm_mem] Add sym mem test into ptd h100 ci ([#156634](https://github.com/pytorch/pytorch/pull/156634))
- [PP] Fix disabled flaky tests ([#154856](https://github.com/pytorch/pytorch/pull/154856))
- Rename inductor cache ([#156128](https://github.com/pytorch/pytorch/pull/156128))
- [CI][CUDA] Re-enable the test-nan-assert on CUDA12 ([#154448](https://github.com/pytorch/pytorch/pull/154448))
- [dtensor] add op support for select_backward and slice_backward ([#150357](https://github.com/pytorch/pytorch/pull/150357))
- add device generalisation support for distributed tests ([#152471](https://github.com/pytorch/pytorch/pull/152471))
- [c10d] Disable stack trace call in logging ([#156362](https://github.com/pytorch/pytorch/pull/156362))
- Made watchdog thread a class ([#155831](https://github.com/pytorch/pytorch/pull/155831))
- [BE][c10d/Store]add check in pyi (#155855) ([#155865](https://github.com/pytorch/pytorch/pull/155865))
- [PT] expose FlightRecord API for building ([#154866](https://github.com/pytorch/pytorch/pull/154866))
- [c10d][fr] Add try catch to update entry due to cuda error ([#153414](https://github.com/pytorch/pytorch/pull/153414))
- [C10D] Move getNcclDataType into NCCLUtils ([#153113](https://github.com/pytorch/pytorch/pull/153113))
- [c10d][fr] Allow multiple writer registration with warnings ([#150232](https://github.com/pytorch/pytorch/pull/150232))
- [torch/c10d] change class variable from private to protected (#149579) ([#149645](https://github.com/pytorch/pytorch/pull/149645))
- [State_dict] Remove functools.cache and add unit test ([#149354](https://github.com/pytorch/pytorch/pull/149354))
- [Ez][BE]: Fix KeyError LOGNAME ([#153324](https://github.com/pytorch/pytorch/pull/153324))
- [BE]: Cleanup traceutils with fmtlib ([#152265](https://github.com/pytorch/pytorch/pull/152265))
- [ROCm] Skip *_stress_cuda and test_ddp_apply_optim_in_backward* ([#155724](https://github.com/pytorch/pytorch/pull/155724))
- Fix a small sphinx markup error ([#156061](https://github.com/pytorch/pytorch/pull/156061))
- Fix an incorrect link markup ([#152239](https://github.com/pytorch/pytorch/pull/152239))
- [1/N] Use std::filesystem ([#152288](https://github.com/pytorch/pytorch/pull/152288))
- Fix lint (c4688af254c)
- Code Clean: Using the new builtin function provides by python 3.8 later ([#150839](https://github.com/pytorch/pytorch/pull/150839))
- Fix space typo in warning message ([#143473](https://github.com/pytorch/pytorch/pull/143473))
- Fix typo ([#150363](https://github.com/pytorch/pytorch/pull/150363))
- [BE] Remove outdated RPC benchmark ([#146716](https://github.com/pytorch/pytorch/pull/146716))
- Migrate aten.split.Tensor from using Sharding Rule to Sharding Strategy ([#149106](https://github.com/pytorch/pytorch/pull/149106))
- Add batch dim sharding rule to sdpa ([#149253](https://github.com/pytorch/pytorch/pull/149253))
- Let pointwise sharding take arg with largest number of dims in case of ties ([#149721](https://github.com/pytorch/pytorch/pull/149721))
- [ca] fix accumulate grad polyfill when different strides between param and grad ([#149651](https://github.com/pytorch/pytorch/pull/149651))
- [CUDA]][SymmetricMemory] Interpret empty string as `std::nullopt` in `rendezvous` ([#149793](https://github.com/pytorch/pytorch/pull/149793))
- Implement aten.select.int sharding strategy ([#149842](https://github.com/pytorch/pytorch/pull/149842))
- Unify use of `enableCollectiveHashDebug_` and trivial updates ([#142865](https://github.com/pytorch/pytorch/pull/142865))
- [ROCm] Enable several fsdp related UTs ([#149369](https://github.com/pytorch/pytorch/pull/149369))
- [ROCm] update test buffer fudge factor for hipblaslt ([#150348](https://github.com/pytorch/pytorch/pull/150348))
- [CUDA]][SymmetricMemory] Interpret empty string as `std::nullopt` in `rendezvous` ([#149793](https://github.com/pytorch/pytorch/pull/149793))
- [torchrec] update local_shards_wrapper to latest version ([#150469](https://github.com/pytorch/pytorch/pull/150469))
- [DTensor] add _explicit_order_placements util ([#150493](https://github.com/pytorch/pytorch/pull/150493))
- [CI][CUDA][Distributed]Update test_composability.py ([#148578](https://github.com/pytorch/pytorch/pull/148578))
- [DTensor] clean up _local_shard_size_and_offset ([#150650](https://github.com/pytorch/pytorch/pull/150650))
- Document poison fork note for accelerator APIs ([#147507](https://github.com/pytorch/pytorch/pull/147507))
- [elastic][test] fix race condition in test_barrier_timeout_rank_tracing ([#150768](https://github.com/pytorch/pytorch/pull/150768))
- [async TP] Fix handling of case where scatter dim = 0 for 2D output tensor ([#150935](https://github.com/pytorch/pytorch/pull/150935))
- [c10d][fr] Fix the false positive in the dtype check in fr analysis script ([#151063](https://github.com/pytorch/pytorch/pull/151063))
- [DTensor] Fix empty shard global-offset calculation ([#150862](https://github.com/pytorch/pytorch/pull/150862))
- test_store: fix timeout for test_queues ([#151252](https://github.com/pytorch/pytorch/pull/151252))
- clang-format CUDASymmetricMemory.cu ([#151260](https://github.com/pytorch/pytorch/pull/151260))
- [c10d][fr] Enable FR analysis script for all fast-path coalesce op ([#151243](https://github.com/pytorch/pytorch/pull/151243))
- Add @requires_multicast_support to test_multimem_all_gather ([#151227](https://github.com/pytorch/pytorch/pull/151227))
- [c10d][fr] Enable FR analysis script for rest of all coalesce op ([#151247](https://github.com/pytorch/pytorch/pull/151247))
- improve noop elimination for view ([#151095](https://github.com/pytorch/pytorch/pull/151095))
- [c10d][fr] Fix script for uneven reduce scatter and update test cases ([#151475](https://github.com/pytorch/pytorch/pull/151475))
- [BE][Easy]: Normalize Dim typing in torch distributed ([#151566](https://github.com/pytorch/pytorch/pull/151566))
- [c10d][fr] Fix another bug when we should continue when the op list is empty ([#151798](https://github.com/pytorch/pytorch/pull/151798))
- Fix DTensorTestBase to barrier with device ids ([#150896](https://github.com/pytorch/pytorch/pull/150896))
- [BE] follow autoformating and linter ([#151507](https://github.com/pytorch/pytorch/pull/151507))
- Move verbose warning to warning_once ([#152044](https://github.com/pytorch/pytorch/pull/152044))
- Refactor to use torch.accelerator.device_index instead of torch.cuda.device for generic device context manager ([#148880](https://github.com/pytorch/pytorch/pull/148880))
- [CP] Use TorchFunctionMode to dispatch SDPA for CP ([#147902](https://github.com/pytorch/pytorch/pull/147902))
- [BE][Easy]: Change typing to DimsType in dim_reduction ([#151677](https://github.com/pytorch/pytorch/pull/151677))
- Fix common_distributed.py to NOT set root logger ([#152319](https://github.com/pytorch/pytorch/pull/152319))
- [DTensor] make test_dtensor_ops report dtensor_args ([#152045](https://github.com/pytorch/pytorch/pull/152045))
- [CUDA][CUTLASS] CUTLASS 3.9 submodule upgrade ([#151253](https://github.com/pytorch/pytorch/pull/151253))
- Respect checkpointed boundaries when using knapsack formulation in the partitioner ([#141684](https://github.com/pytorch/pytorch/pull/141684))
- [CP] Fix the offsets to KV in backward ([#152625](https://github.com/pytorch/pytorch/pull/152625))
- [ROCm][CI] Enabled fp8 distributed tests in test_micro_pipeline_tp.py for MI300 ([#151977](https://github.com/pytorch/pytorch/pull/151977))
- Implement util function compute_global_tensor_shape for 1D device mesh ([#152751](https://github.com/pytorch/pytorch/pull/152751))
- Add a test for AsyncCollectiveTensor handling for maybe-view ops ([#152688](https://github.com/pytorch/pytorch/pull/152688))
- [BE][lint] fix PYFMT for PT-D code under torch.testing._internal, add them to the lint list ([#153114](https://github.com/pytorch/pytorch/pull/153114))
- [c10d] Remove unordered PG destroy test ([#153110](https://github.com/pytorch/pytorch/pull/153110))
- [c10d] Test multiple CUDA Graph captures ([#150040](https://github.com/pytorch/pytorch/pull/150040))
- [c10d] Reduce test verbosity ([#153116](https://github.com/pytorch/pytorch/pull/153116))
- [Graph Partition] Maintain relative order within partition during reordering ([#153111](https://github.com/pytorch/pytorch/pull/153111))
- [BE][DTensor] move torch.distributed._tensor import to torch.distributed.tensor in test files ([#153225](https://github.com/pytorch/pytorch/pull/153225))
- [Pipelining] Fix _batch_p2p bug for non-NCCL backends (#132644) ([#152938](https://github.com/pytorch/pytorch/pull/152938))
- [BE] fix failing test_dp_state_dict_save_load on ROCm CI where world_size=7 ([#153283](https://github.com/pytorch/pytorch/pull/153283))
- Fix test_fused_scaled_matmul_reduce_scatter when scatter_dim is 0 ([#153286](https://github.com/pytorch/pytorch/pull/153286))
- [device_mesh] replace dim_group_info with group_name ([#150898](https://github.com/pytorch/pytorch/pull/150898))
- Fix negative dim issue in for parallel loss context manager ([#152785](https://github.com/pytorch/pytorch/pull/152785))
- Add TEST_HPU flag to set device type ([#153461](https://github.com/pytorch/pytorch/pull/153461))
- Fix typo ([#153561](https://github.com/pytorch/pytorch/pull/153561))
- [Async TP] Fix dim swapping before reduction in fused_scaled_matmul_reduce_scatter ([#153595](https://github.com/pytorch/pytorch/pull/153595))
- Refactoring FSDP2 (_composable/fsdp) test cases to be device agnostic ([#149848](https://github.com/pytorch/pytorch/pull/149848))
- [BE] Remove extra semicolons from SymmetricMemory.hpp ([#154034](https://github.com/pytorch/pytorch/pull/154034))
- [Graph Partition] support removed arguments, NoneLayout, and mutation ([#153899](https://github.com/pytorch/pytorch/pull/153899))
- [DTensor] enable SimpleFSDP's composability with Tensor Parallel ([#152286](https://github.com/pytorch/pytorch/pull/152286))
- [c10d] Add support for testing SIGABRT return ([#153167](https://github.com/pytorch/pytorch/pull/153167))
- [c10d] Add support for testing SIGABRT return ([#153167](https://github.com/pytorch/pytorch/pull/153167))
- PYFMT lint grandfathered files 1 ([#154261](https://github.com/pytorch/pytorch/pull/154261))
- [c10d] Add support for testing SIGABRT return ([#153167](https://github.com/pytorch/pytorch/pull/153167))
- [SymmMem] Speed up tests ([#153677](https://github.com/pytorch/pytorch/pull/153677))
- Add `torch.Tensor._make_wrapper_subclass` to `torch/_C/__init__.pyi` ([#154022](https://github.com/pytorch/pytorch/pull/154022))
- [c10d][CI] Change expected return code in Sandcastle for Nan tests ([#154441](https://github.com/pytorch/pytorch/pull/154441))
- [BE][Ez] Update deprecated pybind11 functions ([#154798](https://github.com/pytorch/pytorch/pull/154798))
- [BE]: Replace printf with fmtlib call ([#154814](https://github.com/pytorch/pytorch/pull/154814))
- [CI][CUDA][UCC] Update test_c10d_ucc.py - remove xfailIfLinux because it now succeeds ([#150979](https://github.com/pytorch/pytorch/pull/150979))
- Add __main__ guards to distributed tests ([#154628](https://github.com/pytorch/pytorch/pull/154628))
- Add NVSHMEM to PYTORCH_EXTRA_INSTALL_REQUIREMENTS ([#154568](https://github.com/pytorch/pytorch/pull/154568))
- [Graph Partition] add symints to get_graph_inputs ([#154679](https://github.com/pytorch/pytorch/pull/154679))
- DOC: Convert to markdown: ddp_comm_hooks.rst, debugging_environment_variables.rst, deploy.rst, deterministic.rst, distributed.algorithms.join.rst ([#155298](https://github.com/pytorch/pytorch/pull/155298))
- Adapt dtensor tests to be device agnostic ([#154840](https://github.com/pytorch/pytorch/pull/154840))
- [NCCL] Expose new `ncclConfig_t` flags in 2.27 ([#155379](https://github.com/pytorch/pytorch/pull/155379))
- Convert rst files to md ([#155369](https://github.com/pytorch/pytorch/pull/155369))
- [dtensor] refactor PlacementStrategy -> OpSpec, move utils to OpSchema ([#155592](https://github.com/pytorch/pytorch/pull/155592))
- [DTensor] Support in gradient placement for local_map() ([#155181](https://github.com/pytorch/pytorch/pull/155181))
- rename distributed.rst to md ([#155767](https://github.com/pytorch/pytorch/pull/155767))
- [a2av] Test must allocate tensors symmetrically ([#155835](https://github.com/pytorch/pytorch/pull/155835))
- Convert to markdown: distributed.tensor.parallel.rst, distributed.tensor.rst, distributions.rst, dlpack.rst ([#155297](https://github.com/pytorch/pytorch/pull/155297))
- Allow MultiProcContinuousTest to set world_size ([#155920](https://github.com/pytorch/pytorch/pull/155920))
- Update test_schedule_multiproc to use world_size=2 ([#155921](https://github.com/pytorch/pytorch/pull/155921))
- [FSDP2] explain user contract for fully_shard ([#156070](https://github.com/pytorch/pytorch/pull/156070))
- [fr] Fix one error in analysis script when subPG world size is smaller than global size ([#156156](https://github.com/pytorch/pytorch/pull/156156))
- remove skipifrocm from composability tests ([#156036](https://github.com/pytorch/pytorch/pull/156036))
- Make the NCCL PG Options and Config copyable and safe to init standalone ([#155700](https://github.com/pytorch/pytorch/pull/155700))
- Make dtensor tests device agnostic ([#155687](https://github.com/pytorch/pytorch/pull/155687))
- [BE][11/16] fix typos in torch/ (torch/csrc/distributed/) ([#156321](https://github.com/pytorch/pytorch/pull/156321))
- [BE][11/16] fix typos in torch/ (torch/csrc/distributed/) ([#156321](https://github.com/pytorch/pytorch/pull/156321))
- Register hpu device to fake backend ([#156076](https://github.com/pytorch/pytorch/pull/156076))
- [ROCm] update state check for test_trace_while_active* ([#153545](https://github.com/pytorch/pytorch/pull/153545))
- [BE][PYFMT] migrate PYFMT for `test/inductor/` to `ruff format` ([#148186](https://github.com/pytorch/pytorch/pull/148186))
### security

### Removed because not released yet
- [Async TP] More robust support for rowwise scales when fusing matmul reduce-scatter ([#149247](https://github.com/pytorch/pytorch/pull/149247))
- [Async TP] Fuse matmul-reduce-scatters when reduce scatters have multiple users, and save fused node for backward instead of reduce_scatter node ([#149946](https://github.com/pytorch/pytorch/pull/149946))
- Fix detection of GPU multicast ([#150563](https://github.com/pytorch/pytorch/pull/150563))
- [symmem] Add some code comments to rendezvous code ([#151716](https://github.com/pytorch/pytorch/pull/151716))
- [SymmMem] Experimental NVSHMEM integration ([#151261](https://github.com/pytorch/pytorch/pull/151261))
- [SymmMem] Add all-to-all ([#151498](https://github.com/pytorch/pytorch/pull/151498))
- [SymmMem] Add all_to_all_vdev ([#151819](https://github.com/pytorch/pytorch/pull/151819))
- [ROCm] Add support for SymmetricMemory ([#150580](https://github.com/pytorch/pytorch/pull/150580))
- [SymmMem] Use cub's BlockScan instead of in-house impl for offset calculation ([#151993](https://github.com/pytorch/pytorch/pull/151993))
- [SymmMem][a2av] Fix TODO: change stride unit ([#153483](https://github.com/pytorch/pytorch/pull/153483))
- [SymmMem][a2av] Use more CTAs for intra-node case ([#153509](https://github.com/pytorch/pytorch/pull/153509))
- [symm_mem] Move all symm mem code into a dedicated folder ([#155573](https://github.com/pytorch/pytorch/pull/155573))
- [SymmMem] Remove unused ptr_to_symm_mem_ ([#155968](https://github.com/pytorch/pytorch/pull/155968))
- [SymmMem] Remove wrappers around nvshmem APIs ([#155971](https://github.com/pytorch/pytorch/pull/155971))
- [SymmMem] Add nvshmem_free ([#155975](https://github.com/pytorch/pytorch/pull/155975))
- [SymmMem] Add NVSHMEM GET support to Triton ([#155890](https://github.com/pytorch/pytorch/pull/155890))
- [SymmMem] Cache rank_to_global_rank exchange ([#156116](https://github.com/pytorch/pytorch/pull/156116))
- [SymmMem] Make get_rank_to_global_rank return const ref ([#156117](https://github.com/pytorch/pytorch/pull/156117))
- [SymmMem] Add runtime detection of NVSHMEM ([#156291](https://github.com/pytorch/pytorch/pull/156291))
- [SymmMem] Add NVSHMEM PUT with Signal support to Triton ([#156211](https://github.com/pytorch/pytorch/pull/156211))
- [symm_mem] Add nccl as a backend for symmetric memory ([#155740](https://github.com/pytorch/pytorch/pull/155740))
- [SymmMem] Add NVSHMEM wait_until support to Triton ([#156472](https://github.com/pytorch/pytorch/pull/156472))
- [SymmMem] Add NVSHMEM signal_wait_until support to Triton  ([#156473](https://github.com/pytorch/pytorch/pull/156473))
- [symm_mem] Add one side put API for nvshvem ([#156443](https://github.com/pytorch/pytorch/pull/156443))
- [SymmMem] Add NVSHMEM Fence support to Triton  ([#156474](https://github.com/pytorch/pytorch/pull/156474))
- [SymmMem] Add NVSHMEM Quiet support to Triton  ([#156475](https://github.com/pytorch/pytorch/pull/156475))
- [SymmMem] Rename all_to_all_vdev ops ([#156582](https://github.com/pytorch/pytorch/pull/156582))
- [ROCm][SymmetricMemory] Avoid bf16 to float conversion during reduce ([#155587](https://github.com/pytorch/pytorch/pull/155587))
- Enables NCCL symmetric memory kernels through mempool registration ([#155134](https://github.com/pytorch/pytorch/pull/155134))
- add reduce_scatter to symm mem ops ([#150813](https://github.com/pytorch/pytorch/pull/150813))
- Turn off symm_mem when cuda version is <12.3 ([#151203](https://github.com/pytorch/pytorch/pull/151203))
- [SymmMem] Enable NVSHMEM for Triton ([#155506](https://github.com/pytorch/pytorch/pull/155506))
- Detect NVSHMEM location ([#153010](https://github.com/pytorch/pytorch/pull/153010))
- Turn on compile with NVSHMEM ([#154538](https://github.com/pytorch/pytorch/pull/154538))
- [BE][Ez]: Optimize nvshmem alloc with missing move ([#156000](https://github.com/pytorch/pytorch/pull/156000))
- [a2av] 2D all-to-all-vdev ([#155058](https://github.com/pytorch/pytorch/pull/155058))
- [a2av] Align length of major dimension in output of 2D a2av ([#155172](https://github.com/pytorch/pytorch/pull/155172))
- [a2av] Improve tuning for 4 GPUs ([#154580](https://github.com/pytorch/pytorch/pull/154580))
- Fixed simple-fsdp mixed-precision training bugs ([#154975](https://github.com/pytorch/pytorch/pull/154975))
- Improved All to All Perf for inter-node use-case (#156376) ([#156389](https://github.com/pytorch/pytorch/pull/156389))

### Not related to distributed
- [partitioner] always ban compiler-driven recompute of collectives by default ([#147561](https://github.com/pytorch/pytorch/pull/147561))
- Support subclass constructor capturing in export ([#147014](https://github.com/pytorch/pytorch/pull/147014))
- [AOTI][reland] Update test runner to use the new APIs ([#149412](https://github.com/pytorch/pytorch/pull/149412))
- [export] specialize for aten.to ([#149235](https://github.com/pytorch/pytorch/pull/149235))
- [1/N] Use internal linkage in torch/csrc C++ files. ([#150930](https://github.com/pytorch/pytorch/pull/150930))
- Reland fast gather and index implementation ([#151917](https://github.com/pytorch/pytorch/pull/151917))
- Support XPU in memory tracker ([#150703](https://github.com/pytorch/pytorch/pull/150703))
- Fix xrefs ([#151888](https://github.com/pytorch/pytorch/pull/151888))
- Add option to use mempool on OOM ([#151487](https://github.com/pytorch/pytorch/pull/151487))
- [ca] Functionalize AccumulateGrad ([#155521](https://github.com/pytorch/pytorch/pull/155521))
- [pt2d] Add reorder_comms_preserving_peak_memory pass ([#146562](https://github.com/pytorch/pytorch/pull/146562))
- fix numpy compatibility for 2d small list indices ([#154806](https://github.com/pytorch/pytorch/pull/154806))
- Resubmit Remove MemPoolContext  (#154042) ([#154746](https://github.com/pytorch/pytorch/pull/154746))
